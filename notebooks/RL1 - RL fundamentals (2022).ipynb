{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/reinforcement-learning/\">https://supaerodatascience.github.io/reinforcement-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 1: Reinforcement Learning fundamentals</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this course works (pedagogically):\n",
    "- one notebook to rule them all (them = the concepts)\n",
    "- no slides\n",
    "- short exercices along the way\n",
    "- a bit of live coding\n",
    "- two class breaks for you to breathe\n",
    "    \n",
    "What you should expect:\n",
    "- some plain words notions,\n",
    "- but avoidance of over-simplification,\n",
    "- and also a fair amount of (hopefully painless) rigorous notations and abstract concepts.\n",
    "- Also most things will be fully written down to increase your autonomy in replaying the notebook.\n",
    "\n",
    "Duration:  \n",
    "This notebook was played in the [introductory course](https://rlvs.aniti.fr/rl-fundamentals.html) of the 2021 [Reinforcement Learning Virtual School](https://rlvs.aniti.fr) (video class available). The class lasted 3.5 hours (with breaks).  \n",
    "For an in-class experience, it is safer to reserve 6 hours.\n",
    "\n",
    "Color code:\n",
    "<div class=\"alert alert-success\">Key results in green boxes</div>\n",
    "<div class=\"alert alert-warning\">Exercices in yellow boxes</div>\n",
    "<div class=\"alert alert-danger\">Solutions in red boxes</div>\n",
    "\n",
    "And a first yellow box:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic algebra.\n",
    "- Random variables, probability distributions.\n",
    "- Gradient descent.\n",
    "    \n",
    "**Useful but not compulsory:**\n",
    "- Random processes, Markov chains.\n",
    "- Notion of contraction mapping.\n",
    "- Dynamic Programming\n",
    "- Stochastic Gradient Descent.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Class-goals\" data-toc-modified-id=\"Class-goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Class goals</a></span></li><li><span><a href=\"#Ruining-the-suspense-with-a-general-abstract-definition-(5-minutes)\" data-toc-modified-id=\"Ruining-the-suspense-with-a-general-abstract-definition-(5-minutes)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ruining the suspense with a general abstract definition (5 minutes)</a></span></li><li><span><a href=\"#RL-within-Machine-Learning-(5-minutes)\" data-toc-modified-id=\"RL-within-Machine-Learning-(5-minutes)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RL within Machine Learning (5 minutes)</a></span></li><li><span><a href=\"#From-plain-words-to-first-variables-(5-minutes)\" data-toc-modified-id=\"From-plain-words-to-first-variables-(5-minutes)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>From plain words to first variables (5 minutes)</a></span></li><li><span><a href=\"#Modeling-sequential-decision-problems-with-Markov-Decision-Processes-(30-minutes)\" data-toc-modified-id=\"Modeling-sequential-decision-problems-with-Markov-Decision-Processes-(30-minutes)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling sequential decision problems with Markov Decision Processes (30 minutes)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Definition</a></span></li><li><span><a href=\"#Value-of-a-trajectory-/-of-a-policy\" data-toc-modified-id=\"Value-of-a-trajectory-/-of-a-policy-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Value of a trajectory / of a policy</a></span></li><li><span><a href=\"#Optimal-policies\" data-toc-modified-id=\"Optimal-policies-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Optimal policies</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Homework:-MDP-notions\" data-toc-modified-id=\"Homework:-MDP-notions-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Homework: MDP notions</a></span></li></ul></li><li><span><a href=\"#Characterizing-value-functions:-the-Bellman-equations-(40-minutes)\" data-toc-modified-id=\"Characterizing-value-functions:-the-Bellman-equations-(40-minutes)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Characterizing value functions: the Bellman equations (40 minutes)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Intuitions\" data-toc-modified-id=\"Intuitions-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Intuitions</a></span></li><li><span><a href=\"#The-evaluation-equation\" data-toc-modified-id=\"The-evaluation-equation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>The evaluation equation</a></span></li><li><span><a href=\"#The-optimality-equation\" data-toc-modified-id=\"The-optimality-equation-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>The optimality equation</a></span></li><li><span><a href=\"#Dynamic-Programming-for-the-optimality-equation\" data-toc-modified-id=\"Dynamic-Programming-for-the-optimality-equation-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Dynamic Programming for the optimality equation</a></span></li><li><span><a href=\"#Approximate-Dynamic-Programming\" data-toc-modified-id=\"Approximate-Dynamic-Programming-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Approximate Dynamic Programming</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Homework:-the-Bellman-equations\" data-toc-modified-id=\"Homework:-the-Bellman-equations-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Homework: the Bellman equations</a></span></li><li><span><a href=\"#Homework:-Policy-Iteration-and-Modified-Policy-Iteration\" data-toc-modified-id=\"Homework:-Policy-Iteration-and-Modified-Policy-Iteration-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Homework: Policy Iteration and Modified Policy Iteration</a></span></li><li><span><a href=\"#Homework:-solving-MDPs-with-Linear-Programming\" data-toc-modified-id=\"Homework:-solving-MDPs-with-Linear-Programming-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Homework: solving MDPs with Linear Programming</a></span></li><li><span><a href=\"#Homework:-Asynchronous-Dynamic-Programming\" data-toc-modified-id=\"Homework:-Asynchronous-Dynamic-Programming-6.10\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Homework: Asynchronous Dynamic Programming</a></span></li></ul></li><li><span><a href=\"#Learning-optimal-value-functions-(40-minutes)\" data-toc-modified-id=\"Learning-optimal-value-functions-(40-minutes)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Learning optimal value functions (40 minutes)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Policy-evaluation-as-Stochastic-Approximation:-Temporal-Differences\" data-toc-modified-id=\"Policy-evaluation-as-Stochastic-Approximation:-Temporal-Differences-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Policy evaluation as Stochastic Approximation: Temporal Differences</a></span></li><li><span><a href=\"#Approximate-Value-Iteration-as-Stochastic-Approximation:-Q-learning\" data-toc-modified-id=\"Approximate-Value-Iteration-as-Stochastic-Approximation:-Q-learning-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Approximate Value Iteration as Stochastic Approximation: Q-learning</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Homework:-variations-on-TD(0)\" data-toc-modified-id=\"Homework:-variations-on-TD(0)-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Homework: variations on TD(0)</a></span></li><li><span><a href=\"#Homework:-delayed-updates-and-experience-replay-for-TD(0)\" data-toc-modified-id=\"Homework:-delayed-updates-and-experience-replay-for-TD(0)-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Homework: delayed updates and experience replay for TD(0)</a></span></li><li><span><a href=\"#Homework:-the-importance-of-the-behavior-distribution\" data-toc-modified-id=\"Homework:-the-importance-of-the-behavior-distribution-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Homework: the importance of the behavior distribution</a></span></li><li><span><a href=\"#Homework:-Monte-Carlo-evaluation\" data-toc-modified-id=\"Homework:-Monte-Carlo-evaluation-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Homework: Monte Carlo evaluation</a></span></li><li><span><a href=\"#Homework:-TD($\\lambda$)\" data-toc-modified-id=\"Homework:-TD($\\lambda$)-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Homework: TD($\\lambda$)</a></span></li><li><span><a href=\"#Homework:-value-function-approximation\" data-toc-modified-id=\"Homework:-value-function-approximation-7.9\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>Homework: value function approximation</a></span></li><li><span><a href=\"#Homework:-Generalized-Policy-Iteration-and-Actor-Critic-architectures\" data-toc-modified-id=\"Homework:-Generalized-Policy-Iteration-and-Actor-Critic-architectures-7.10\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>Homework: Generalized Policy Iteration and Actor-Critic architectures</a></span></li><li><span><a href=\"#Homework:-SARSA\" data-toc-modified-id=\"Homework:-SARSA-7.11\"><span class=\"toc-item-num\">7.11&nbsp;&nbsp;</span>Homework: SARSA</a></span></li></ul></li><li><span><a href=\"#Challenges-in-RL-and-RLVS-classes-(10-minutes)\" data-toc-modified-id=\"Challenges-in-RL-and-RLVS-classes-(10-minutes)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Challenges in RL and RLVS classes (10 minutes)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-summary\" data-toc-modified-id=\"General-summary-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>General summary</a></span></li><li><span><a href=\"#Three-intrinsic-challenges-in-Reinforcement-Learning\" data-toc-modified-id=\"Three-intrinsic-challenges-in-Reinforcement-Learning-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Three intrinsic challenges in Reinforcement Learning</a></span></li><li><span><a href=\"#Specific-questions-and-challenges-in-RL\" data-toc-modified-id=\"Specific-questions-and-challenges-in-RL-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Specific questions and challenges in RL</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- acquire the fundamental building blocks of RL:\n",
    "    - plain word notions\n",
    "    - MDPs, policies, optimality equations, etc.\n",
    "    - common notations\n",
    "    - key algorithms\n",
    "    - common misconceptions\n",
    "- key challenges in RL and their connection to future lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ruining the suspense with a general abstract definition (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about learning to control dynamic systems.\n",
    "<img src=\"img/dynamic.png\" style=\"width: 400px;\"></img>\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$ over a certain time horizon.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of RL problems <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Exiting a spiral</td>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Cart-pole balancing</td>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Portfolio management</td>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- Bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Ecosystem regulation and preservation\n",
    "- Robocup soccer\n",
    "- Video game playing (Atari, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, learning to play a board game, learning to juggle, learning to take good strategic decisions, learning to drive... all fall into the same category of **control problems** and Reinforcement Learning studies the process of **elaborating a good control strategy through interaction samples**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Reinforcement Learning is about learning an optimal sequential behavior in a given environment.\n",
    "</div>\n",
    "\n",
    "Let's break this down.\n",
    "- sequential behavior in a given environment  \n",
    "$\\rightarrow$ discrete time steps, sequence of actions\n",
    "- optimal  \n",
    "$\\rightarrow$ a reward signal informs us of the quality of the last action\n",
    "- learning  \n",
    "$\\rightarrow$ no known model, just interaction samples, behavior adaptation.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"width: 400px;\"></img></center>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Keywords:**\n",
    "- system to control / environment\n",
    "- control policy\n",
    "- optimality\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warm-up poll:** \n",
    "How do you do today?  \n",
    "[https://linkto.run/p/BOOR15YA](https://linkto.run/p/BOOR15YA)\n",
    "- Great, I'm learning RL!\n",
    "- Great, but I'm scared the RL unicorn will turn into a difficult to tame rhino.\n",
    "- Great, bring the math on (as long as you do it step by step).\n",
    "- Why do you ask the question if the only answer is \"Great\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standing on the shoulders of giants**\n",
    "\n",
    "> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Sutton & Barto, 2018, [Reinforcement Learning: an Introduction](http://incompleteideas.net/book/the-book-2nd.html))\n",
    "\n",
    "Caveat: this is a definition of *learning*, not specifically of *reinforcement learning* (although it applies to RL), so it is worth giving some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL within Machine Learning (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have had classes on Machine Learning before. There are three strongly distinct categories of problems in ML:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "Let's try to answer the following questions for each category.\n",
    "- What's the abstract problem we are trying to solve?\n",
    "- What's the data provided to the algorithms?\n",
    "- Give examples of algorithms in SL/UL/RL.  \n",
    "\n",
    "<center>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td> <b>Question</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Supervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Unsupervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Reinforcement</b> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $f(x)=y$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $x\\in X$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\pi(s)=a$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target (rephrased) </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Predict outputs given inputs</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Discover structure in data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Find an optimal behavior </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(x,y\\right)\\right\\}$ supervisor's labels </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{x\\right\\}$ unlabelled data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(s,a,r,s'\\right)\\right\\}$ experience samples </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Output </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Classifier or regressor</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Clusters or dimension reduction </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Policies, value functions </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Key algorithms </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Neural networks, SVMs, etc.</td>\n",
    "    <td style=\"border-left: 1px solid black\"> k-means, PCA, etc. </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Q-learning, Policy Gradients, etc. </td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "This table helps distinguish the different natures of the problems tackled. The RL problem is about finding the optimal policy for a given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from Supervised Learning?\n",
    "- no correct $(s,a)$ example, rather $(s,a,r,s')$ samples\n",
    "- Delayed rewards, credit assignment, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:** Pick the true statement(s).  \n",
    "[https://linkto.run/p/3OG3IJO3](https://linkto.run/p/3OG3IJO3)\n",
    "- Sorting new emails as spam (or not) given a million labelled emails is a reinforcement learning task.\n",
    "- Deciding what move to play at chess, based on thousands of previous games is a reinforcement learning task.\n",
    "- Incrementally improving the accuracy of a radar detection software from online collected data is a reinforcement learning task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspirations for RL:\n",
    "- Control theory and Stochastic processes for the **modeling** part\n",
    "- Statistics, Optimization and Cognitive Psychology for the **learning** part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From plain words to first variables (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A medical prescription example <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient-doctor.png\" style=\"height: 200px;\">\n",
    "    \n",
    "A patient walks into a clinic with her medical file (medical history, x-rays, blood work, etc.). You, as her doctor, need to write a prescription. Let us use this example to formalize the process of deciding what to write on the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Patient variables <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states: $S_t$\n",
    "</center>\n",
    "\n",
    "The medical file of the patient allows us to define a number of variables that characterize the patient now. We will write $S_0$ the vector of these variables. Future measurements will be noted $S_t$.\n",
    "\n",
    "$S_t$ is a random vector, taking different values in a *patient description space* $S$ at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescription <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>\n",
    "\n",
    "The prescription is a series of recommendations we give to the patient over the course of treatment. It is thus a sequence $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$ of variables $A_t$.\n",
    "\n",
    "These treatments $A_t$ are random variables too, taking their value in some space $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient evolution <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</center>\n",
    "\n",
    "The patient evolves over time steps. Her evolution follows a certain probability distribution $\\mathbb{P}(S_t)$ over descriptive states.\n",
    "\n",
    "So $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ defines a *random process* that describes the patient's evolution under the influence of past $S_t$ and $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physician's goal <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$\n",
    "\n",
    "The physician's goal is to bring the patient from an unhealthy state $S_0$ to a healthy situation.  \n",
    "\n",
    "This goal is not only defined by a final state of the patient but by the full trajectory followed by the variables $S_t$ and $A_t$. For example, prescribing a drug that damages the patient's liver, or letting the patient experience too much pain over the course of treatment is discouraged.\n",
    "\n",
    "We define a criterion $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ that allows to quantify how good a trajectory in the joint $S\\times A$ space is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up <a class=\"tocSkip\">\n",
    "\n",
    "- Patient state $S_t$  (random variable)\n",
    "- Physician instruction $A_t$ (random variable)\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$   \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$  \n",
    "- Patient's trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common misconception <a class=\"tocSkip\">\n",
    "\n",
    "You will often see the following type of drawing, along with a sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<img src=\"img/misconception.png\" style=\"height: 300px;\"></img>\n",
    "\n",
    "Although this sentence is not false *per se*, it conveys an important misconception that may be grounded in too simple anthropomorphic analogies. One often talks about the *state of the agent* or the *state of the environment*. The distinction here is confusing at best: there is no separation between agent and environment. A better vocabulary is to talk about a *system to control*, that is described through its observed *state*. This system is controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with.\n",
    "\n",
    "Although less shiny, the drawing below may be less misleading.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 300px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three key notions <a class=\"tocSkip\">\n",
    "\n",
    "Understanding RL is a three-stage rocket, answering the questions:  \n",
    "1. What is the system to control?  \n",
    "2. What is an optimal strategy?  \n",
    "3. How do we learn such a strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (no poll, 1 minute):**  \n",
    "Suppose that, instead of treating a patient, we want to learn to swing the pole up in the cart-pole example.  \n",
    "What are the state description variables?  \n",
    "What are the action variables?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pend.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "State: cart position and velocity $x, \\dot{x}$, pole angle and velocity $\\theta, \\dot{\\theta}$.\n",
    "    \n",
    "Action: force $F$ applied on the cart.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Modeling sequential decision problems with Markov Decision Processes (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Definition\n",
    "\n",
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for our patient.\n",
    "\n",
    "Let us assume we have:\n",
    "- a set of states $S$ describing the system to control,\n",
    "- a set of actions $A$ we can apply.\n",
    "\n",
    "Curing patients is a conceptually difficult task. \n",
    "To keep things grounded, we shall use a toy example called [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) and work our way to more general concepts. It's also the occasion to familiarize with [OpenAI Gym](https://gym.openai.com/).\n",
    "\n",
    "<img src=\"img/frisbee.jpg\" style=\"height: 300px;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The game's goal is to navigate across this lake, from position S to position G, in order to retrieve a frisbee, while avoiding falling into the holes H. Frozen positions are slippery so you don't always move in the intended direction. Reaching the goal provides a reward of 1, and zero otherwise. Falling into a hole or reaching the goal ends an episode.\n",
    "\n",
    "Take a look at the funny description in `help(fl.FrozenLakeEnv)` if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:**  \n",
    "[https://linkto.run/p/65E9EO4Q](https://linkto.run/p/65E9EO4Q)  \n",
    "How many states are there in this game?  \n",
    "How many actions?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "States set: the 16 positions on the map.  \n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At every time step, the system state is $S_t$ and we decide to apply action $A_t$. This results in observing a new state $S_{t+1}$ and receiving a scalar reward signal $R_t$ for this transition.\n",
    "\n",
    "$R_t$ tells us how happy we are with the last transition.\n",
    "\n",
    "For example, in FrozenLake, all transitions have reward 0 except for the one that reaches the goal, which yields reward 1. Let's verify this and introduce a few utility functions on the way.\n",
    "\n",
    "Note that $S_t$, $A_t$, $S_{t+1}$ and $R_t$ are random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "print(actions)\n",
    "row=3\n",
    "col=2\n",
    "a=2\n",
    "print(\"Apply \", actions[2], \" from (\", row, \", \", col, \"):\", sep='')\n",
    "for tr in env.unwrapped.P[to_s(row,col)][a]:\n",
    "    print(\"  Reach (\", to_row_col(tr[1]), \") and get reward \", tr[2], \" with proba \", tr[0], \".\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now make our main assumption about the systems we want to control.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Fundamental assumption (Markov property)**\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t, S_{t-1}, A_{t-1}, \\ldots, S_0, A_0) = \\mathbb{P}(S_{t+1},R_t|S_t, A_t)$$\n",
    "</div>\n",
    "    \n",
    "Such a system will be called a Markov Decision Process (MDP).\n",
    "\n",
    "One generally separates the state dynamics and the rewards by:\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t) = \\mathbb{P}(S_{t+1}|S_t, A_t)\\cdot \\mathbb{P}(R_t|S_t, A_t, S_{t+1})$$\n",
    "\n",
    "Which leads in turn to the general definition of an MDP:\n",
    "<div class=\"alert alert-success\"><b>Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A set of states $S$\n",
    "<li> A set of actions $A$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Most of the results presented here can be found in M. L. Puterman's classic book, [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873).\n",
    "\n",
    "If $H\\rightarrow\\infty$ we have an infinite horizon control problem.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Since we will only work with infinite horizon problems, we shall identify the MDP with the 4-tuple $\\langle S,A,p,r\\rangle$.\n",
    "</div>\n",
    "    \n",
    "So, in RL, we wish to control the trajectory of a system that, we suppose, behaves as a Markov Decision Process.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 240px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Value of a trajectory / of a policy\n",
    "\n",
    "An oracle decides on how to choose actions at each time step:\n",
    "$$A_t \\sim \\pi_t.$$\n",
    "\n",
    "The collection of $\\pi_t$ is the oracle's **policy**. (politique de d√©cision)\n",
    "$\\pi_t$ is called the **decision rule** at step $t$, it is a distribution over the action space $A$.  \n",
    "The collection $\\pi = \\left(\\pi_t \\right)_{t\\in\\mathbb{N}}$ is the oracle's **policy**.\n",
    "\n",
    "<img src=\"img/frisbee.jpg\" style=\"height: 100px;\"></img>\n",
    "\n",
    "One policy implies one specific distribution over trajectories over the frozen lake. More generally, the policy and $S_0$ condition the sequence $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$\n",
    "\n",
    "In FrozenLake as in the patient's example, some trajectories are better than others. We need a criterion to compare trajectories. Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
    "\n",
    "Let's compare the policy that always moves to the right and the policy that always moves left by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nb_episodes = 50000\n",
    "horizon = 200\n",
    "\n",
    "Vright = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, _ = env.step(fl.RIGHT)\n",
    "        Vright[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "Vleft  = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, _ = env.step(fl.LEFT)\n",
    "        Vleft[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"est. value of 'right' policy:\", np.mean(Vright), \"variance:\", np.std(Vright))\n",
    "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"variance:\", np.std(Vleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "$G^\\pi(s)$ represents what we can gain in the long-term by applying the actions from $\\pi$.\n",
    "\n",
    "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
    "$$V^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
    "\n",
    "This defines the value function $V^\\pi$ of policy $\\pi$:\n",
    "<div class=\"alert alert-success\"><b>Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "S & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "</div>\n",
    "\n",
    "\n",
    "And, given a distribution $\\rho_0$ on starting states, we can map $\\pi$ to the scalar value:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} \\left[ V^\\pi(s) \\right]$$\n",
    "\n",
    "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies.\n",
    "\n",
    "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some uses the average reward criterion, and few works venture into more exotic criteria. Today, we will limit ourselves to the discounted criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state (or initial state distribution).  \n",
    "\n",
    "Thus, an **optimal** policy is one that is better than any other.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy $\\pi^*$</b><br>\n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} V^\\pi$.<br>\n",
    "<br>\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$$\n",
    "</div>\n",
    "\n",
    "Note that although there may be several optimal policies, they all share the same value function $V^* = V^{\\pi^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now get to our first fundamental result. Fortunately for us...  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy theorem</b><br>\n",
    "For $\\left\\{\\begin{array}{l}\n",
    "\\gamma\\textrm{-discounted criterion}\\\\\n",
    "\\textrm{infinite horizon}\n",
    "\\end{array}\\right.$, \n",
    "there always exists at least one optimal stationary, deterministic, Markovian policy.\n",
    "</div>\n",
    "\n",
    "Let's explain a little:\n",
    "- Markovian: all decision rules are only conditioned by the last seen state. Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right., \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right) = \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)$.  \n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.\n",
    "- Stationary (and Markovian): all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "- Deterministic: all decision rules put all probability mass on a single item of the action space $A$.  \n",
    "$\\pi_t(A_t|history) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal ways of picking $A_t$, at least one is a function $\\pi:S\\rightarrow A$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in a complex family of history-dependent, stochastic, non-stationary policies; instead we can simply search for a function $\\pi(s)=a$ that maps states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summary\n",
    "\n",
    "Let's wrap this whole section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$.\n",
    "\n",
    "So we have built the first stage of our three-stage rocket:  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**What is the system to control?**  \n",
    "The system to control is a Markov Decision Process $\\langle S, A, p, r \\rangle$ and we will control it with a policy $\\pi:s\\mapsto a$ in order to optimize $\\mathbb{E} \\left( \\sum_t \\gamma^t R_t\\right)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Poll** The limits of MDP modeling  \n",
    "[https://linkto.run/p/0WG7WNER](https://linkto.run/p/0WG7WNER)  \n",
    "Can these systems be modeled as MDPs?   \n",
    "- Playing a tennis video game based on a single video frame\n",
    "- Playing a tennis video game based on a full physical description of the ball and the players\n",
    "- The game of Poker\n",
    "- The collaborative game of [Hanabi](https://en.wikipedia.org/wiki/Hanabi_(card_game))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A single video frame does not contain enough information to accurately represent the current state of the game. The velocities are absent for instance. Hence the dynamics might not be Markovian.\n",
    "    \n",
    "A full physical description, however, may contain enough information so that $\\mathbb{P}(S_{t+1})$ is only conditioned by $S_t$ and $A_t$.\n",
    "    \n",
    "Poker is a two-player, adversarial, stochastic game. MDPs only model one-player games.\n",
    "\n",
    "Beyond the fact that it is a multi-player game. Hanabi is a game based mainly on epistemic reasoning. That is, reasoning on beliefs about the state of the world (specifically, the state of the other players' hand). This type of state description is difficult to encode within a Markovian dynamics model.\n",
    "\n",
    "**A bit of additional discussion to generalize these notions:**\n",
    "\n",
    "What if the system is an MDP but its state is not fully observable?  \n",
    "$\\rightarrow$ This is the (exciting) field of Partially Observable MDPs. Our key result of having a Markovian optimal policy does not hold anymore. There are ways to still obtain optimal policies (but it is often very computationaly costly) or approximate them with Markovian policies.\n",
    "\n",
    "What happens if there are multiple actions taken at the same time by different agents?  \n",
    "$\\rightarrow$ This falls into the category of multi-player stochastic games. Such games can be adversarial, cooperative, or a mix of the two. Of course they can also have partial observability.\n",
    "\n",
    "What if the transition model is not Markovian?  \n",
    "$\\rightarrow$ Beware, here be dragons! All the beautiful framework above crumbles down if its hypothesis are violated. So great care should be taken when choosing the state variables for a given problem. In a sense, an MDP is a discrete time version of a first-order differential equation. Writing a system as $\\dot{X} = f(X,U, noise)$ as is common in Control Theory is a good practice to ensure the Markov property.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Let's take a short break.**  \n",
    "**If there is time, I can take questions.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: MDP notions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "In the text above, we wrote that $\\pi_t$ is the distribution over the action space $A$ for the action $A_t$ taken at time step $t$.  \n",
    "- Write this probability $\\mathbb{P}(A_t)$ as a conditional probability $\\pi_t(A_t|\\ldots)$ (the real question is: what are the $\\ldots$?).\n",
    "- Rephrase, with your own words, what this $\\pi_t(A_t|\\ldots)$ indicates.  \n",
    "Then we defined a policy $\\pi$ as the collection of decision rules $\\left( \\pi_t \\right)_{t\\in\\mathbb{N}}$.\n",
    "- Using the answer to the previous questions, write the definition of a Markovian policy, then a stationary Markovian policy (the answer is actually in the text just after the Optimal policy theorem, the exercise is about being able to recall and explain the definitions and what they imply). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $\\pi_t$ describes the distribution over actions at time step $t$. Because of causality (future events don't affect current events), it can only depend on the realization of the state and actions random variables in previous time steps:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t)$$\n",
    "We define the *history* $H_t = S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t$ at time step $t$ as this random sequence. So:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | H_t)$$\n",
    "\n",
    "- In plain words, for an action $a$ and a history $h$ at step $t$, $\\pi_t(a|h)$ indicates  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the probability to pick action $a$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; at time $t$,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; given the history of states/actions $h$.  \n",
    "This is called a *history-dependent, non-stationary, stochastic* policy and is the most generic class of policies.\n",
    "\n",
    "- In a Markovian policy, all decision rules are only conditioned by the last encountered state.\n",
    "$$\\pi_t(A_t|H_t) = \\pi_t(A_t | S_t)$$\n",
    "In other words, given two (possibly different) sequences of state-action random variables realizations up to time $t-1$ and a single realization of S_t the distribution of $A_t$ is the same.\n",
    "Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right.,$\n",
    "\\begin{align*}\n",
    "    \\pi_t(A_t|H_t) &= \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t(A_t | S_t)\n",
    "\\end{align*}\n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.  \n",
    "In a stationary Markovian policy, all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "In the patient example, suppose the physician tells the patient to take drug A every day for 5 days, then drug B every two days for 9 days, then come back for a check-up. The physician adds to take drug C once a day if the patient feels pain over two consecutive days. Can you write the sequence of corresponding decision rules?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "This prescription is made over a finite horizon $H=14$ days. The actions are the combinations of drugs $A=\\left\\{ \\emptyset, (A), (B), (C), (A,B), (A,C), (B,C), (A,B,C) \\right\\}$.   \n",
    "    \n",
    "The prescription is deterministic: the distribution over actions is a Dirac. We will write it $a_t = \\pi_t(h)$.\n",
    "    \n",
    "The prescription depends on the two last states of the patient. So it's not Markovian, it is history-dependent. Precisely, it depends on the boolean state variable \"is there pain?\". So we can write $\\pi_t(h) = \\pi_t(s_t,s_{t-1})$.  \n",
    "  \n",
    "It also is not stationary, since the prescription changes after day 5.  \n",
    "    \n",
    "Consequently, the policy is:  \n",
    "For $t \\in [1, 5]$:   \n",
    "if $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (A,C)$,  \n",
    "if $pain(s_t,s_{t-1})=False$, $\\pi_t(s_t,s_{t-1}) = (A)$.  \n",
    "For $t \\in [6, 14]$:   \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (B,C)$,  \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = (B)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (C)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = \\emptyset$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Use the FrozenLake environment we've introduced earlier to obtain a Monte-Carlo estimate of $V^\\pi(s_0)$ over 100000 trials, with $s_0$ being the initial state and $\\pi$ being a simple policy that always goes right. Take $\\gamma = 0.9$. Yes, the code is almost the same as the example provided earlier.  \n",
    "Note that $\\gamma^{200} \\sim 10^{-9}$ so any reward obtained after 200 time steps will have a negligible contribution to $V^\\pi(s_0)$, thus rolling an episode out for 200 time steps should be sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise and note on the stationary distribution under policy $\\pi$**  \n",
    "\n",
    "Let's consider an MDP and a certain policy $\\pi$. Let's initialize the MDP to a starting state $s_0$ drawn from a distribution $\\rho_0(s)$ and let's look at how the state evolves across time steps.\n",
    "\n",
    "Because the stochastic process of $S_t$ is a Markov chain (since $\\pi$ is fixed, the probability of reaching $S_{t+1}$ is only conditionned by $S_t$), in the long run, the distribution of states follows a stationary distribution $\\rho^\\pi(s|s_0)$.\n",
    "\n",
    "This distribution is not necessarily unique: it depends on $s_0$. When all states are represented with non-zero probability in this distribution, the corresponding Markov chain is said to be *ergodic*. This is an assumption that will often be made to simply future reasoning, even if it is false most of the time.\n",
    "\n",
    "What can we say about the stationary distribution of the Markov chain corresponding to:\n",
    "- the patient with a chronic disease under a policy that fights off the disease?\n",
    "- the patient with a deadly disease under a policy that doesn't cure her?\n",
    "- the FrozenLake example with a fixed random policy?\n",
    "- the Mad Hatter's casino (from the previous class) under a fixed random policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The patient with a chronic disease under a policy that fights off the disease will most likely live a rather long life (let's say infinite, for the sake of this example) and will explore states that are linked to the evolution of the disease. The states corresponding to non-recoverable situations however will not be visited.\n",
    "    \n",
    "The patient with a deadly disease and a bad treatment policy will likely die, sadly. On an infinite horizon, the stationary distribution only has probability mass on the states corresponding to death.\n",
    "    \n",
    "Similarly, the FrozenLake example has several terminal states, either by reaching the goal or by falling into a hole. It should be noted however that for such episodic environments, it is possible to define an alternate distribution $\\rho^\\pi(s|s_0)$ that describes the distribution of states before termination.\n",
    "    \n",
    "Finally, the Mad Hatter's casino under a fixed random policy is a very nice ergodic Markov chain: from any starting state there is a non-zero probability of reaching any state in a finite number of steps. No terminal states in wonderland!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Characterizing value functions: the Bellman equations (40 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the maze below, where an agent can move North, South, East or West. The resulting transition is deterministic and a reward of $+1$ is gained when exiting the maze (which terminates the game). Otherwise all rewards are zero. Bumping into a wall terminates the game with a reward of zero.\n",
    "\n",
    "<img src=\"img/grid_raw.png\" width=\"200px\"></img>\n",
    "\n",
    "Let's consider the policy $\\pi$ that always moves East.\n",
    "\n",
    "<img src=\"img/grid_policy.png\" width=\"200px\"></img>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll**  \n",
    "[https://linkto.run/p/NO9LB7NP](https://linkto.run/p/NO9LB7NP)  \n",
    "Without writing any equation, what is the value of the top-right cell under this policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$V^\\pi((3,3)) = 1$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's take $\\gamma=0.9$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll**  \n",
    "[https://linkto.run/p/03LL2V5H](https://linkto.run/p/03LL2V5H)  \n",
    "Without writing any equation, what is the value of the top-middle cell under this policy? What is the value of the bottom-right cell?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The value of $(2,3)$ is the expected discounted sum of what one gets from applying $\\pi$ from $(2,3)$. Since the $\\pi$ is deterministic and the transitions are deterministic too, $\\pi(2,3)$ always take us to state $(3,3)$. So $V^\\pi((2,3)) = 0 + \\gamma \\times V^\\pi((3,3)) = 0.9$.\n",
    "    \n",
    "The value of $(3,1)$ is the expected infinite sum of discounted rewards from $(3,1)$. Since the agent keeps bumping into the wall when applying $\\pi$, it never exits the maze and this is an infinite sum of zero terms. Hence $V^\\pi((2,3)) = 0$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's draw the value function.\n",
    "\n",
    "<img src=\"img/grid_vpi.png\" width=\"200px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose you are currently in cell $(1,2)$ and would like to choose what action to take. Suppose also that you know the value function above. You need to put a scalar value on all four actions. To evaluate each action, let's estimate what we can get by applying the action and then using $\\gamma \\times V^\\pi(s)$ to estimate what can obtain in the long run after this first action. Define $Q^\\pi((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question / Poll**  \n",
    "What is $Q^\\pi((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we follow $\\pi$ after?  \n",
    "[https://linkto.run/p/5WV5GU62](https://linkto.run/p/5WV5GU62)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^\\pi((1,2),N) = 0 + \\gamma \\cdot \\gamma^2 = 0.729$  \n",
    "$Q^\\pi((1,2),S) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),E) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),W) = 0$  \n",
    "The best action seems to be $N$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An optimal policy is quite easy to guess. Let's draw the optimal value function (the value function of any optimal policy).\n",
    "\n",
    "<img src=\"img/grid_vopt.png\" width=\"200px\"></img>\n",
    "\n",
    "Define $Q^*((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$ if it is followed by an optimal policy.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question**   \n",
    "What is $Q^*((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we act optimally after? Rank the actions by utility.  \n",
    "https://linkto.run/p/3OG6IJO3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^*$ is what we gain immediately, plus $\\gamma$ times what we expect to receive from applying an optimal policy in the state we reach by applying $a$.  \n",
    "$Q^*((1,2),N) = 0 + \\gamma\\times\\gamma^2=\\gamma^3$  \n",
    "$Q^*((1,2),S) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),E) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),W) = 0 + \\gamma\\times\\gamma^3=\\gamma^4$  \n",
    "The best action seems to be $N$, followed by $W$, after that $S$ and $E$ are tied.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Question (no poll)</b><br>\n",
    "What property has the policy that always picks greedily the $Q^*$ maximizing action in each state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "It is an optimal policy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now suppose $(1,2)$ is a special slippery cell. Going North has a $0.7$ probability of actually reaching $(1,3)$, but also a $0.2$ probability of staying in $(1,2)$ and a $0.1$ probability of ending in $(2,2)$. Note that this changes the problem and the optimal expected return function $V^*$.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question (no poll)</b><br>\n",
    "Given this new problem, can you write $Q^*((1,2),N)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "When we take action $N$ in $(1,2)$, there are 3 possible outcomes:\n",
    "- with probability $0.7$, reach $(1,3)$ and get reward $0$,\n",
    "- with probability $0.2$, reach $(1,2)$ and get reward $0$,\n",
    "- with probability $0.1$, reach $(2,2)$ and get reward $0$.\n",
    "\n",
    "So what we can expect to get from applying $N$ in $(1,2)$ is:  \n",
    "\\begin{align*}\n",
    "    Q^*((1,2), N) &= 0.7 \\times (0+\\gamma V^*(1,3)) + 0.2\\times(0+\\gamma V^*(1,2)) + 0.1\\times(0+\\gamma V^*(2,2))\\\\\n",
    "    &= \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you can remark that if we knew the action $\\pi^*((1,2))$ taken by an optimal policy in $(1,2)$, then $Q^*((1,2), \\pi^*(1,2))$ would actually be precisely the optimal long-term return $V^*$ (since it would be the expected return of a policy that acts optimally at every time step, including the first one).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Suppose an oracle tells us that $\\pi^*((1,2))=N$. Using the previous exercice, write $V^*(1,2)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^*((1,2)) = Q^*((1,2),N)$, so\n",
    "$$V^*((1,2)) = \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have introduced the key concepts upon which this secton is built: $V$ and $Q$ functions, and the relation between $V(s)$ and $V(s')$ when $s'$ can be reached from $s$ in one action. The next steps are now to write all this formally, prove strong properties and derive algorithms for computing value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The evaluation equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Drawing inspiration from the exercises above, we can define the very important state-action value function $Q^\\pi$.\n",
    "<div class=\"alert alert-success\"><b>State-action value function</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t, S_{t+1}\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>\n",
    "\n",
    "To be precise and reuse the full notations from the MDP definition:\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) &=\\mathbb{E}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t=\\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right],\\\\\n",
    " &= \\mathbb{E}_{s'} \\left[ r(s,a,s') + \\gamma V^\\pi(s') \\right], \\\\\n",
    " &= r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/Qfunctions.png\" style=\"height: 200px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's remark that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. Let's replace $a$ by $\\pi(s)$ above and we obtain an important equation to characterize $V^\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/V-DP.png\" style=\"height: 200px;\"></img>\n",
    "$$V^\\pi(s) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]$$\n",
    "\n",
    "This equation uses $V^\\pi(s')$ in all $s'$ reachable from $s$ to define $V^\\pi(s)$.  \n",
    "Since this equation is true in all $s$, this provides as many equations as we have states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Evaluation equation</b><br>\n",
    "$V^\\pi$ obeys the linear system of equations:\n",
    "$$\n",
    "V^\\pi\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]\\\\\n",
    "$$\n",
    "Similarly:\n",
    "$$\n",
    "Q^\\pi\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\n",
    "$$\n",
    "</div>\n",
    "\n",
    "This leads to the introduction of the **Bellman evaluation operator**:\n",
    "<div class=\"alert alert-success\"><b>Evaluation operator $T^\\pi$</b><br>\n",
    "$T^\\pi$ is an operator on value functions, that transforms a function $V:S\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi V\\left(s\\right) &= r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V(s') \\right]\\\\\n",
    " &= r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)\n",
    "\\end{align*}\n",
    "    \n",
    "Similarly we can introduce an evaluation operator (with the same name $T^\\pi$) over state-action value functions. <br> \n",
    "$T^\\pi$ is an operator on state-action value functions, that transforms a function $Q:S\\times A\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi Q\\left(s,a\\right) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\\\\\n",
    " &= r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q^\\pi\\left(s', \\pi\\left(s'\\right)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that, fundamentally, we have written 4 times the same thing in the block above.  \n",
    "So finding $V^\\pi$ (resp. $Q^\\pi$) boils down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have gone far from our original FrozenLake problem. Let's make all this very concrete:\n",
    "- A policy $\\pi$ is an agent's behaviour\n",
    "- In every state $s$, one can expect to gain $V^\\pi(s)$ in the long run by applying $\\pi$\n",
    "- $V^\\pi(s)$ is the sum of the reward on the first step $r(s,\\pi(s))$ and the expected long-term return from the next state $\\gamma \\mathbb{E}_{s'} \\left[V^\\pi(s')\\right]$ \n",
    "- The function $V^\\pi$ actually obeys the linear system of equations above that simply link the value of a state with the values of its successors in an episode.\n",
    "\n",
    "We can stop for a minute on the $T^\\pi$ evaluation operator (that maps a function $S\\rightarrow\\mathbb{R}$ to a function $S\\rightarrow\\mathbb{R}$) and the search for $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^\\pi$</b><br>\n",
    "<ol>\n",
    "<li> $T^\\pi$ is an affine operator, it defines a linear system of equations.<br>\n",
    "<li> $T^\\pi$ is a contraction mapping<br>\n",
    "    Specifically, with $\\gamma<1$, $T^\\pi$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^\\pi$ (resp. $Q^\\pi$) is the unique solution to the (linear) fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^\\pi V$ (resp. $Q=T^\\pi Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's use this second property to compute $Q^\\pi$ for the policy that always moves right on FrozenLake.\n",
    "\n",
    "Suppose we start with $Q_0(s,a) = 0$ for all $(s,a)$.\n",
    "\n",
    "Recall that, in FrozenLake, rewards are provided under the $r(s,a,s')$ form.\n",
    "\n",
    "Applying $T^\\pi$ once results in:\n",
    "$$Q_1(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_0(s',\\pi(s')) \\right]$$\n",
    "\n",
    "In plain words, $Q_1$ is the one-step expected return under policy $\\pi$.\n",
    "\n",
    "Applying $T^\\pi$ twice results in:\n",
    "$$Q_2(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_1(s',\\pi(s')) \\right]$$\n",
    "\n",
    "This is the two-step expected return.\n",
    "\n",
    "And so on.\n",
    "\n",
    "If we apply $T^\\pi$ enough times, $Q_n$ should become closer to $Q^\\pi$, whatever the chosen value for $Q_0$.\n",
    "\n",
    "In more formal words, because $T^\\pi$ is a contraction mapping, the sequence $Q_{n+1} = T^\\pi Q_n$ converges to $T^\\pi$'s fixed point.\n",
    "\n",
    "Let us live-code this.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's compute the sequence $Q_{n+1} = T^\\pi Q_n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pi = fl.RIGHT*np.ones((env.observation_space.n), dtype=np.uint8)\n",
    "nb_iter = 20\n",
    "gamma = 0.9\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qpi_sequence = [Q]\n",
    "for i in range(nb_iter):\n",
    "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                Qnew[x,a] += p * (r + gamma * Q[y,pi[y]])\n",
    "    Q = Qnew\n",
    "    Qpi_sequence.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "residuals = []\n",
    "for i in range(1, len(Qpi_sequence)):\n",
    "    residuals.append(np.max(np.abs(Qpi_sequence[i]-Qpi_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can unfold the same kind of reasoning on the value of an optimal policy. We write:\n",
    "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal greedy policy</b><br>\n",
    "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And $Q^*$ obeys the same type of recurrence relation:\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality equation</b><br>\n",
    "The optimal value function obeys:\n",
    "\\begin{align*}\n",
    "    V^*(s) &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V^*(s') \\right]\\\\\n",
    "        &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]\n",
    "\\end{align*}\n",
    "or in terms of $Q$-functions:\n",
    "\\begin{align*}\n",
    "    Q^*(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q^*(s',a') \\right]\\\\\n",
    "        &= r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')\n",
    "\\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As for the evaluation equation, we have actually written 4 times the same thing in the block above.  \n",
    "We have also defined the **Bellman optimality operator $T^*$** (on $V$ and $Q$ functions) as:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality operator</b><br>\n",
    "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V(s') \\right]$$\n",
    "$$\\left(T^*Q\\right)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q(s',a') \\right]$$\n",
    "</div>\n",
    "\n",
    "So finding $V^*$ (resp. $Q^*$) boils down to solving $V= T^* V$ (resp. $Q = T^* Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^*$</b><br>\n",
    "<ol>\n",
    "<li> $T^*$ is non-linear.<br>\n",
    "<li> $T^*$ is a contraction mapping<br>\n",
    "With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dynamic Programming for the optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Repeatedly applying $T^*$ to an initial function $Q_0$ yields the sequence $Q_{n+1} = T^* Q_n$ that converges to $Q^*$.\n",
    "\n",
    "The implementation of this sequence's computation is the algorithm called **Value Iteration**.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's compute the sequence $Q_{n+1} = T^* Q_n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qopt_sequence = [Q]\n",
    "for i in range(nb_iter):\n",
    "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                Qnew[x,a] += p * (r + np.max(Q[y,:]) )\n",
    "    Q = Qnew\n",
    "    Qopt_sequence.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "residuals = []\n",
    "for i in range(1, len(Qopt_sequence)):\n",
    "    residuals.append(np.max(np.abs(Qopt_sequence[i]-Qopt_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are alternatives to Value Iteration with algorithms such as Gauss-Seidel Value Iteration, Asynchronous Value Iteration, Policy Iteration or Modified Policy Iteration for instance. We unfortunately won't have time to cover them today, check the exercises below to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Approximate Dynamic Programming\n",
    "\n",
    "Let's take a step back.\n",
    "\n",
    "With the Bellman equation, we have a way to **characterize** $Q^*$. This characterization directly translates to the **Value Iteration** algorithm. In turn, once we know $Q^*$, we can deduce $\\pi^*$.\n",
    "\n",
    "That's all very nice, but is it applicable in practice, on real world examples? In particular, how does the computation of $Q^*$ scale with large state and action spaces?\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll**  \n",
    "What is the time complexity of the Value Iteration algorithm implemented above, in terms of $|S|$ and $|A|$?  \n",
    "[https://linkto.run/p/6DVFBR0J](https://linkto.run/p/6DVFBR0J)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$O(S^2 A)$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The curse of dimensionality makes the number of states and actions scale exponentially with the dimension of the state and action spaces. So exact computation of $Q^*$ quickly becomes intractable.\n",
    "\n",
    "Instead, one can try to *approximate* the resolution of $T^* Q_n$ at each step of Value Iteration. This yields the Approximate Value Iteration algorithm.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Approximate Value Iteration** is the algorithm that computes the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$, where $\\mathcal{A}$ is an approximation procedure.\n",
    "</div>\n",
    "\n",
    "Note in particular that when dealing with parametric functions $Q_\\theta$, finding a minimizer of the loss\n",
    "$L_n(\\theta) = \\| Q_\\theta - T^* Q_n \\|$\n",
    "is such an approximation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us suppose that $\\mathcal{A}$ is not a bad approximation procedure and that its approximation error is uniformly bounded, that is, \n",
    "$$\\forall f \\in \\mathcal{F}(S\\times A,\\mathbb{R}), \\ \\| f-\\mathcal{A}f \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "The first important result is that Approximate Value Iteration does not converge. However, one can prove that $Q_n$ reaches a neighborhood of $Q^*$. Specifically:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\| Q^* - Q_n \\|_\\infty \\leq \\frac{\\epsilon}{1-\\gamma}.$$\n",
    "\n",
    "More importantly:  \n",
    "Let $\\pi_n$ be the greedy policy with respect to $Q_n$. Then:\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma}{1-\\gamma} \\|Q^*-Q_n\\|_\\infty.$$\n",
    "\n",
    "And consequently:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "So,\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate Value Iteration does not necessarily converge but reaches policies whose values are close to optimal.\n",
    "</div>\n",
    "\n",
    "These results are proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "Most supervised learning algorithms minimize a loss that is expressed as a weighted $L_2$ norm. Thus, they don't explicitly provide guarantees in $L_\\infty$ norm. R. Munos provided **[error bounds for approximate value iteration](https://www.aaai.org/Papers/AAAI/2005/AAAI05-159.pdf)** in the general case of weighted $L_p$ norms. Those bounds are similar to the one in $L_\\infty$ norm, thus justifying the use of supervised learning techniques (such as neural networks learning from samples for instance) in Approximate Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we have built upon the MDP properties of the environment we wish to control, in order to characterize policies through their value functions.\n",
    "\n",
    "We have learned that:\n",
    "- $Q^\\pi$ is a solution to the Bellman evaluation equation\n",
    "$$Q = T^\\pi Q$$\n",
    "- $Q^*$ is a solution to the Bellman optimality equation\n",
    "$$Q = T^* Q$$\n",
    "- Value Iteration constructs the sequence of $Q_{n+1} = T^* Q_n$ value functions\n",
    "- Approximate Value Iteration trades exact construction of this sequence for scalability by constructing an approximate sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ that still provides good policies if $\\mathcal{A}$ is a good approximation procedure.\n",
    "\n",
    "So we have built the second stage of our three-stage rocket:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What is an optimal strategy?**  \n",
    "An optimal policy is one that yields optimal cumulated rewards. It is a policy that is *greedy* with respect to an optimal value function $Q^*$. Such a value function obeys Bellman's optimality equation and can be (approximately) computed via (approximate) dynamic programming.\n",
    "</div>\n",
    "\n",
    "Still, we are dependent on a characterization of $\\pi^*$ that relies on the knowledge of the MDP.\n",
    "\n",
    "But we could imagine that $\\mathcal{A}$ is a procedure that *learns* $Q_{n+1}$ from samples of $T^* Q_n$. If such samples can be obtained from interaction with the system to control, it might be possible to learn $Q^*$ without knowing the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Let's take a short break.**  \n",
    "**If there is time, I can take questions.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: the Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Write a function that computes $Q^\\pi$ given $V^\\pi$ for FrozenLake. Use $\\gamma=0.9$.<br>\n",
    "Suppose $V^\\pi(s)=0$ in all $s$. Use your function to compute $Q^\\pi(s,a)$ given $V^\\pi$. Comment.\n",
    "</div>\n",
    "\n",
    "To help you, recall that in the previous class, we introduced a few utility functions and accessed the transition probabilities and rewards of FrozenLake using the `env.unwrapped.P` attribute as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "state = to_s(0,1)\n",
    "action = 0\n",
    "outcomes = env.unwrapped.P[state][action]\n",
    "print(outcomes)\n",
    "print()\n",
    "print(\"outcomes for the transition from state \", to_row_col(state), \" and action \", actions[action], \":\", sep='')\n",
    "for o in outcomes:\n",
    "    proba      = o[0]\n",
    "    next_state = o[1]\n",
    "    reward     = o[2]\n",
    "    isTerminal = o[3]\n",
    "    print(\" reach state \", to_row_col(next_state), \\\n",
    "          \" and get reward \", reward, \\\n",
    "          \" with proba \", proba, \". \", sep='', end=\"\")\n",
    "    if isTerminal:\n",
    "        print(\"Transition is terminal.\")\n",
    "    else:\n",
    "        print(\"Transition is not terminal.\")\n",
    "        \n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Write a function that takes a $Q$ function for FrozenLake and returns the greedy policy.<br>\n",
    "Use the function below to print the greedy policy for the $Q$ function of the previous exercise in a human-friendly format.<br>\n",
    "Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (variations on the theme of the evaluation equation)**  \n",
    "\n",
    "When we introduced the evaluation operator we wrote that finding $V^\\pi$ (resp. $Q^\\pi$) boiled down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$).  \n",
    "- Suppose the state space is finite. Write $V= T^\\pi V$ as matrix-vector equations. Explain the dimension of all variables.\n",
    "- How does this extend to the case of continuous state spaces?\n",
    "- Write the evaluation equation for a stochastic policy.\n",
    "- Repeat the previous questions for $Q = T^\\pi Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- When the state space is discrete:  \n",
    "$V$ is a vector of size $|S|$,  \n",
    "$P^\\pi$ is a matrix containing the values $P^\\pi_{ij} = p\\left(s_j|s_i,\\pi(s_i)\\right)$   \n",
    "and, similarly, $r^\\pi$ is a vector containing the values $r^\\pi_i = r(s_i,\\pi(s_i))$.  \n",
    "In better words, $P^\\pi$ is the <i>transition kernel</i> of the Markov chain describing the state dynamics under policy $\\pi$ and $r^\\pi$ is the associated reward model.  \n",
    "Then $T^\\pi$ is the linear operator in $\\mathbb{R}^{|S|}$ that maps $V$ to $r^\\pi + \\gamma P^\\pi V$.\n",
    "\n",
    "- This generalizes straightforwardly to the continuous states case:   \n",
    "$V$ is a function in the $\\mathcal{F}(S,\\mathbb{R})$ function space (the generalization of the vector in the previous sentence),    \n",
    "$r^\\pi$ becomes the function $s\\mapsto r(s,\\pi(s))$    \n",
    "and  $P^\\pi$ becomes the operator over $\\mathcal{F}(S,\\mathbb{R})$ that maps function $V$ to function $s\\mapsto \\int\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)ds'$.  \n",
    "Then $T^\\pi$ is the linear operator in $\\mathcal{F}(S,\\mathbb{R})$ that maps $V$ to the function $s\\mapsto r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)$.\n",
    "\n",
    "- For stochastic policies:\n",
    "\\begin{align*}\n",
    "    \\quad V^\\pi(s) &= \\mathbb{E}_{a\\sim\\pi(a|s)} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ V^\\pi\\left(s'\\right) \\right] \\right] \\\\\n",
    "        &= \\sum\\limits_{a\\in A} \\pi(a|s) \\left(r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V^\\pi(s') \\right)\n",
    "\\end{align*}<br>\n",
    "<br>\n",
    "If you prefer using $r(s,a,s')$ instead of $r(s,a)$:<br>\n",
    "\\begin{align*}\n",
    "    V^\\pi\\left(s\\right) &= \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ r(s,\\pi(s),s') + \\gamma V^\\pi\\left(s'\\right) \\right]\\\\\n",
    "        &= \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) \\left[ r\\left(s,\\pi\\left(s\\right),s'\\right) + \\gamma V^\\pi\\left(s'\\right) \\right]\n",
    "    \\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "We have seen that $T^\\pi$ is an affine operator. \n",
    "For a discrete state and action space, write $V^\\pi$ the vector of values functions. What is the size of this vector? Write the evaluation equation as a set of operations on such vectors. Deduce a closed-form solution to the evaluation equation on $V$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$V^\\pi$ is a column vector that lives in $\\mathbb{R}^{|S|}$.  \n",
    "\n",
    "Let $V$ be a column vector of $\\mathbb{R}^{|S|}$. The evaluation operator is written:\n",
    "$$(T^\\pi V) (s) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right).$$\n",
    "So this is a row of the $V=T^\\pi V$ equation.\n",
    "\n",
    "Let $p^\\pi(s)$ be the row vector composed of:\n",
    "$$p^\\pi(s) = \\left(p(s'|s,\\pi(s))\\right)_{s'\\in S}.$$\n",
    "And let $r^\\pi(s)$ be the scalar $r(s,\\pi(s))$. Then the evaluation operator can be written:\n",
    "$$(T^\\pi V)(s) = r^\\pi(s) + \\gamma p^\\pi(s)\\cdot V(s).$$\n",
    "\n",
    "Let $P^\\pi$ be the matrix obtained by stacking all row vectors $p^\\pi(s)$ on top of each other.  \n",
    "    $P^\\pi$ is the transition kernel of the Markov chain defined by the MDP and the policy $\\pi$.  \n",
    "Let also $r^\\pi$ be the column vector of all $r^\\pi(s)$.  \n",
    "Then the evaluation operator can be written in matrix-vector form as:\n",
    "$$T^\\pi V = r^\\pi + \\gamma P^\\pi V.$$\n",
    "\n",
    "Since $V^\\pi$ is a solution to $V = T^\\pi V$, one has $V = r^\\pi + \\gamma P^\\pi V$.  \n",
    "So $(I-\\gamma P^\\pi) V = r^\\pi$.  \n",
    "Since $P^\\pi$ is a probability matrix, all its eigenvalues are smaller or equal to one.  \n",
    "So the eigenvalues of $\\gamma P^\\pi$ are all strictly smaller than one (because $\\gamma <1$).  \n",
    "Consequently $I-\\gamma P^\\pi$ has only strictly positive eigenvalues and is thus invertible. Finally, the unique solution to $V = T^\\pi V$ is:\n",
    "$$V^\\pi = \\left( I - \\gamma P^\\pi \\right)^{-1} r^\\pi.$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Use the previous exercise to compute $V^\\pi$ by matrix inversion, for the policy that always moves right. To do this, you'll need to compute $r^\\pi$ and $P^\\pi$. Again, $\\gamma = 0.9$.\n",
    "Check if your result for $V^\\pi(s_0)$ is consistent with the Monte Carlo estimate of the previous section exercises.\n",
    "</div>\n",
    "\n",
    "Use the cell below to recall the solution to the Monte Carlo estimation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/RL1_exercise1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Generalize the code above to a function that takes any policy as input. We'll suppose in this case that the policy is an array of actions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "    \n",
    "Generalize the code provided in the section about the evaluation equation to write a function that computes the value function of any policy, using the contraction mapping property.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise and a word on residuals.**  \n",
    "    \n",
    "Between two applications of $T^\\pi$, the distance between $V_{n+1}$ and $V_n$ decreases as $\\|V_{n+1}-V_n\\| = \\|r^\\pi + \\gamma P^\\pi V_n - V_n\\|$. Since $T^\\pi$ is a contraction mapping, we have $\\|V_{n+1}-V_n\\| < \\|V_{n}-V_{n-1}\\|$. Let's call this distance at time step $n$ the **residual**. Then the successive residuals monotonically tend to zero.  \n",
    "   \n",
    "Use the property on the residuals to replace the maximum numbler of iterations parameter in the previous functions, by a precision parameter `epsilon` that specifies the maximum error allowed on $V^\\pi$.  \n",
    "Advice: still keep a maximum number of iterations in your function to stop the computation in case you specify an `epsilon` that is too small. Return both $V^\\pi$ and the sequence of residuals.  \n",
    "Plot the sequence of residuals and display the number of iterations necessary to reach the chose precision `epsilon`. Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "    \n",
    "\n",
    "Use the contraction mapping property of $T^*$ to compute $V^*$. To do this, you'll need to remember that since $T^*$ is a contraction mapping, the sequence $V_{n+1}=T^* V_n$ converges to $T^*$'s fixed point (which happens to be $V^*$ according to the property). Again, $\\gamma = 0.9$.  \n",
    "Plot the residuals and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Value Iteration for $V$ functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $V$, $\\epsilon$<br>\n",
    "Init: $\\Delta = \\epsilon+1$<br>\n",
    "While $\\Delta \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$:  <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a\\in A$:  <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{new}(s) = \\max_a Q(s,a)$  <br>\n",
    "&nbsp;&nbsp;&nbsp; $\\Delta = \\| V_{new}-V \\|_\\infty$  <br>\n",
    "&nbsp;&nbsp;&nbsp; $V = V_{new}$  <br>\n",
    "Return $V$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "    \n",
    "\n",
    "Compute and display an optimal policy for the FrozenLake game, using Value Iteration. If you copy-paste the results of exercises 1, 2 and 7, you almost don't need to write any new code.  \n",
    "Comment the obtained policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise8.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is often viewed as an algorithm that maintains a memory of a $V$ function and iterates until this function is close enough to $V^*$. Still, it is interesting (and will be useful later in the class) to rephrase the algorithm in alternate terms:  \n",
    "\n",
    "Start with a state-action value function $Q$\n",
    "- Define $\\pi(s) = \\arg\\max_a Q(s,a)$ in all states and actions.  \n",
    "  In the algorithm above, this operation is performed when the $\\max_a$ is solved,  \n",
    "  just the action is not explicitly stored.\n",
    "- $Q \\leftarrow T^\\pi Q$,  \n",
    "  that is, for all $s$ and $a$, $Q(s,a) \\leftarrow r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ Q(s',\\pi(s')) \\right]$.  \n",
    "- repeat\n",
    "\n",
    "So Value Iteration can be seen as picking the greedy action with respect to $Q$ and then updating $Q$ with just one application of $T^\\pi$. This application of $T^\\pi$ alone is not sufficient for $Q$ to reach $Q^\\pi$ but it changes $Q$ and so it changes what the greedy action will be at the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider an MDP whose rewards are all in the $[0,1]$ interval and take $\\gamma = 0.9$.  \n",
    "What is the maximum range of values for any $V^\\pi(s)$?  \n",
    "Suppose we have a function approximator whose error is uniformly upper-bounded by $\\epsilon=0.1$.  \n",
    "Compare the neighborhood size from the equation above to the largest value $V^\\pi(s)$ can take and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The smallest cumulated return we can get of obviously zero.  \n",
    "The largest cumulated return is reached if we receive a reward of 1 at each time step.  \n",
    "In this case, $V^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t = \\frac{1}{1-\\gamma}$.  \n",
    "With $\\gamma = 0.9$ this upper bound is $\\frac{1}{1-\\gamma}=10$.\n",
    "\n",
    "Since we suppose $\\epsilon=0.1$, that is roughly a 1% error in function approximation.\n",
    "    \n",
    "With $\\epsilon=0.1$ we have a neighborhood radius of $\\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}=18$.  \n",
    "If we had taken $\\gamma=0.99$, the upper bound on $V^\\pi$ would have been 100 and the neighborhood radious would have been 1980.\n",
    "\n",
    "So the bound above does not really provide strong guarantees. Fortunately, in practice, most Approximate Value Iteration application behave better than this worst-case example.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Policy Iteration and Modified Policy Iteration\n",
    "\n",
    "The Policy Iteration algorithm stems from the following remark. Suppose we have a policy $\\pi$ and know its value function $V^\\pi$ and state-action value function $Q^\\pi$. Then, the non-stationary policy $\\pi'$ that acts greedily with respect to $Q^\\pi$ for the first time step and then follows $\\pi$ has a value function $V^{\\pi'}$ that is greater or equal to $V^\\pi$ (equal if $\\pi$ is optimal, strictly greater otherwise). Actually, the contraction property of $T^*$ insures that the stationary policy $\\pi'$ that is greedy with respect to $Q^\\pi$ is at least as good as $\\pi$, that is $V^{\\pi'}\\geq V^\\pi$. Consequently, the sequence of policies defined by $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ has a monotonically improving corresponding sequence of value functions $V^{\\pi_n}$ and converges to $\\pi^*$.<br>\n",
    "<br>\n",
    "Let's make this more simple with a drawing. Policy iteration alternates two phases:\n",
    "1. Evaluate $\\pi_n$ $\\rightarrow Q^{\\pi_n}$\n",
    "2. Compute $\\pi_{n+1}$ as the $Q^{\\pi_n}$-greedy policy\n",
    "\n",
    "<img src=\"img/policyiteration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above defines a sequence of policies **and** value functions. Since, for finite state and action spaces, the number of policies is finite, Policy Iteration is guaranteed to converge in a finite number of iterations.\n",
    "\n",
    "Policy Iteration was introduced in R. A. Howard's book **[Dynamic Programming and Markov Processes](https://psycnet.apa.org/record/1961-01474-000)** (1960).\n",
    "\n",
    "Before we start implementing, let's first note that $\\pi_{n+1} = \\pi_n$ is not a valid termination criterion for Policy Iteration!\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Can you explain why? What would be a sound termination criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Policy Iteration using the contraction property of $T^\\pi$.  \n",
    "For the sake if simplicity and despite the warning of the previous exercise, use $\\pi_n=\\pi_{n-1}$ as the termination condition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $\\pi$, $V$, $\\epsilon$<br>\n",
    "Init: $\\Delta = \\epsilon+1$<br>\n",
    "Do:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation: apply $T^\\pi$ to $V$ until precision $\\epsilon$<br>\n",
    "&nbsp;&nbsp;&nbsp; While $\\Delta \\geq \\epsilon$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) = r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V_{old}(s')$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta = \\| V-V_{old} \\|_\\infty$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; $\\pi_{old} = \\pi$ <br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) = \\max_a Q(s,a)$<br>\n",
    "While $\\pi_{old} \\neq \\pi$ <br>\n",
    "Return $\\pi$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise was a little convoluted. We can make things simpler.\n",
    "\n",
    "In all rigor, Policy Iteration is the algorithm that applies the Bellman evaluation operator an infinite number of times to $V$ so that it reaches $V^\\pi$, then defines $\\pi$ as the greedy policy with respect to $V$.\n",
    "\n",
    "Obviously, an infinite number of applications of $T^\\pi$ is not very practical. In the previous exercises, we controlled the error made in the convergence to $V^\\pi$ with a parameter $\\epsilon$, that allowed to perform a finite number of $T^\\pi$ applications.\n",
    "\n",
    "A simpler way is to define a certain number $m$ of applications of $T^\\pi$. This provides the **Modified Policy Iteration** algorithm, that applies $T^\\pi$ $m$ times to update $V$, then defines $\\pi$ as the greedy policy with respect to $V$.\n",
    "\n",
    "Modified Policy Iteration was introduced by M. L. Puterman and M. C. Shin in **[Modified Policy Iteration Algorithms for Discounted Markov Decision Problems](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.24.11.1127)** (1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $\\pi$, $m$<br>\n",
    "Init: $\\Delta = \\Delta_\\pi = \\epsilon+1$<br>\n",
    "While $\\Delta_\\pi \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation, solve $V=T^\\pi V$<br>\n",
    "&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp; For $i \\in [1,m]$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) \\leftarrow (T^\\pi V)(s)$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) = \\max_a Q(s,a)$<br>\n",
    "Return $\\pi$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Modified Policy Iteration benefits from the same convergence properties as Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is Modified Policy Iteration with $m=1$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "According to the last remark about Value Iteration, that's exactly Value Iteration!  \n",
    "So Modified Policy Iteration is actually a continuum of algorithms between Value Iteration and Policy Iteration.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Compute and display an optimal policy for the FrozenLake game, using Modified Policy Iteration, with $m=500$ and $\\gamma=0.9$.  \n",
    "Since you will use a fixed number of iterations for the resolution of $V=T^\\pi V$, you could reuse function `policy_eval_iter_mat` from the correction of exercise 5 for instance. Recall also that exercise 1 transformed a $V$ function into a $Q$ function, and that exercise 2 picked the greedy policy from a $Q$ function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise9.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is the time complexity of Policy Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$O(|S|^2 |A|)$\n",
    "    \n",
    "Policy Iteration has the same time complexity as Value Iteration. In practice, for finite state and action spaces, Policy Iteration converges in a finite number of steps (contrarily to Value Iteration). But each of these steps requires the resolution of $V = T^\\pi V$ which is where the real computational cost is.  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Approximate Policy Iteration consists in solving the evaluation equation up to a certain precision $\\epsilon$ and then taking a greedy improvement step.\n",
    "\n",
    "We write $V_n$ the approximation of $V^{\\pi_n}$ at the end of an evaluation phase in Policy Iteration and suppose that the approximation error is uniformly bounded:\n",
    "$$\\| V^{\\pi_n} - V_n \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "Then it is known that, even though the sequence of greedy policies $\\pi_n$ does not converge, it oscillates among a set of policies such that:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "This result is proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "So the neighborhood reached has the same size as that of Approximate Value Iteration.\n",
    "\n",
    "Similar bounds **[error bounds for approximate policy iteration](https://www.aaai.org/Papers/ICML/2003/ICML03-074.pdf)** in weighted $L_p$ norms were provided by R. Munos (2003), thus justifying the use of supervised learning techniques (such as neural networks learning from samples for instance) in Approximate Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: solving MDPs with Linear Programming\n",
    "\n",
    "An alternative way of finding $V^*$ is by casting the optimality equation as a linear optimization problem. This formulation is mainly given for your curiosity and we will not study it any further.<br>\n",
    "<br>\n",
    "Recall the optimality equation:\n",
    "$$\\forall s\\in S, V(s)=\\max\\limits_{a\\in A} \\left[r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s')\\right]$$\n",
    "\n",
    "The key remark to transform this into a linear program is to rephrase it as \"$V^*$ is the smallest value that dominates over all policy values\". This can be written as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall \\pi, \\ V \\geq T^\\pi V\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "\"For all $\\pi$\" means for all possible association $s\\leftrightarrow a$, so this can be expanded as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall (s,a)\\in S\\times A, \\quad V(s) - \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V(s') \\geq r(s,a)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Which, finally, is a linear program with $|S|$ variables and $|S||A|$ constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Asynchronous Dynamic Programming\n",
    "\n",
    "We have seen that Value Iteration and Policy Iteration are Dynamic Programming algorithms. They follow a path, respectively in value function and in policy space that leads to $V^*$ and $\\pi^*$. But we can remark that they both perform *state-wise* operations such as:\n",
    "\n",
    "- $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $V(s) \\leftarrow \\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $\\pi(s) \\leftarrow \\arg\\max_a Q^{\\pi}(s,a)$\n",
    "- $V(s) \\leftarrow r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$\n",
    "\n",
    "These state-wise operations are called Bellman backups.\n",
    "\n",
    "Let's use $V$ functions to describe Value Iteration. We can define the Bellman backup operator:\n",
    "- `BBV(V,s): return` $\\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$  \n",
    "  `BBV` is the operation performed in every state, in one pass of Value Iteration\n",
    "\n",
    "Alternatively, we can operate only on $Q$ functions and define the corresponding Bellman backup operators:\n",
    "- `BBQ(Q,pi,s,a): return` $r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) Q(s',\\pi(s'))$  \n",
    "  `BBQ` serves in all *evaluation* steps,\n",
    "- `BBpi(Q,s): return` $\\max_a Q(s,a)$ and $\\arg\\max_a Q(s,a)$  \n",
    "  `BBpi` serves in all *improvement* steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `V`.\n",
    "Using `BBV`, rewrite Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      W(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `Q` and a policy `pi`. \n",
    "Using `BBQ` and `BBpi`, rewrite Value Iteration and Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      Qnew(s,a) = BBQ(Q,pi,s,a)\n",
    "    W(s), pi(s) = BBpi(Qnew,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "<br><br>\n",
    "Modified Policy Iteration:<br><code>\n",
    "while(pi not constant)\n",
    "  Q(s,a) = 0 for all s,a\n",
    "  while error>epsilon\n",
    "    for k in [1,m]\n",
    "      for s,a in SxA\n",
    "        Qnew(s,a) = BBQ(s,a)\n",
    "  for s in S\n",
    "    V, pi = BBpi(s)</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the pseudo-code of Value Iteration using a $V$ function from the exercises above. Why don't we perform directly `V(s) = BBV(V,s)`, instead of relying on the intermediate `W` function? In other terms, if we have already performed a backup in $s$, why couldn't we reuse it in the next backup? Doing so is actually called **Gauss-Seidl Value Iteration** and it opens the door to a much wider class of algorithms called **Asynchronous Value Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Using `BBV`, write Gauss-Seidl Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      V(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to note that in Gauss-Seidl Value Iteration, the order in which the states are considered for backups greatly affects of rewards are propagated through the state space and how the sequence of value functions converges to $V^*$.  \n",
    "\n",
    "But still, in Gauss-Seidl Value Iteration, states are updated once per sweep over the state space.\n",
    "Why wouldn't we update the value of some states more often than others? Would the overall value function still converge to $V^*$? A very powerful theorem actually states what follows.\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Convergence of Asynchronous Value Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `V(s)` $\\leftarrow$ `BBV(V,s)` operation as time tends to $+\\infty$, the value function $V$ converges to $V^*$\n",
    "</div>\n",
    "\n",
    "Consequently, we could pick states totally randomly in order to perform Bellman backups on $V$, and $V$ would still converge to $V^*$. Although picking states randomly for that purpose seems like a bad idea, identifying a good ordering for the backups can lead to drastic improvements in convergence speed. This is the key idea of **Asynchronous Value Iteration** and has justified (among other things) the popular **[Prioritized Sweeping](https://link.springer.com/article/10.1007/BF00993104)** and **[Real-Time Dynamic Programming](https://www.sciencedirect.com/science/article/pii/000437029400011O)** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take the pseudo-code of Modified Policy Iteration from the exercises above, using `BBQ` and `BBpi`. The evaluation step and the improvement step are clearly separated. But we know already that if we require the evaluation step to have infinite precision, $m$ needs to tend to $\\infty$. We also know that if we take an arbitrary value for $m$ and the algorithm still converges.\n",
    "\n",
    "Can we write introduce the idea of asynchronous Bellman backups in Policy Iteration? As in the value iteration case, can we update the value or policy of a given state in any ordering? Our most general theorem for Asynchronous Dynamic Programming in MDP states the following.\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "**Convergence of Asynchronous Policy Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `Q(s,a)` $\\leftarrow$ `BBQ(Q,pi,s,a)`  and the `pi(s)` $\\leftarrow$ `BBpi(Q,s)` operations as time tends to $+\\infty$, the value function $Q$ and the policy $\\pi$ converge respectively to $Q^*$ and $\\pi^*$\n",
    "</div>\n",
    "\n",
    "That is the most general framework one can give for **Asynchronous Dynamic Programming** in MDP resolution. It is often called **Asynchronous Policy Iteration**.<br>\n",
    "\n",
    "Overall, Asynchronous Policy Iteration can be written:  \n",
    "`\n",
    "Do forever:\n",
    "  Pick a set SAset={(s,a)}\n",
    "  For s,a in SAset:\n",
    "    Q(s,a) = BBQ(Q,pi,s,a)\n",
    "  Pick a set Sset={s}:\n",
    "  For s in Sset:\n",
    "    pi(s) = BBpi(Q,s)\n",
    "`\n",
    "\n",
    "So Asynchronous Policy Iteration encompasses all the previous algorithms, both synchronous (VI, PI, MPI) and asynchronous (Asynchronous VI).\n",
    "\n",
    "Of course, just as for Asynchronous Value Iteration, the most important thing with Asynchronous Policy Iteration is the order in which we pick the states and actions for backups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning optimal value functions (40 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key idea we develop in this section is that one can actually *learn* the sequence of AVI functions using interaction samples rather than *calculate* it using a model.\n",
    "\n",
    "<img src=\"img/brain.png\" width=\"400px\"></img>\n",
    "\n",
    "Although we have introduced a fair amount of abstract concepts, it is important to keep in mind that these maths simply formalize an intuitive cognitive process. By experiencing rewards and punishments, we (humans) incrementally learn to evaluate the outcomes of our actions and then decide to act accordingly. This cognitive process is thus in line with the formalism we have introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Policy evaluation as Stochastic Approximation: Temporal Differences\n",
    "\n",
    "Recall that evaluating $Q^\\pi(s,a)$ is estimating the mathematical expectation of $G^\\pi(s,a)$.\n",
    "\n",
    "Stochastic approximation theory tells us that, for a given $s,a$ pair, given a series $g^\\pi_t$ of independent realizations of $G^\\pi(s,a)$, the sequence\n",
    "$q_{t+1} = q_t + \\alpha_t \\left(g^\\pi_t - q_t\\right)$\n",
    "converges to $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$, if the sequence of $\\alpha_t$ respects the Robbins-Monro conditions ($\\sum_t \\alpha_t = \\infty$ and $\\sum_t \\alpha_t^2 < \\infty$).\n",
    "\n",
    "<a href=\"#morePpi\" data-toggle=\"collapse\">Intuitive explanation of Stochastic Approximation.</a><br>\n",
    "<div id=\"morePpi\" class=\"collapse\">\n",
    "    \n",
    "For those unfamiliar with stochastic approximation procedures, we can understand the previous update as: $g^\\pi_t$ are sample estimates of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$. If I already have an estimate $q_t$ of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$ and I receive a new sample $g^\\pi_t$, I should \"pull\" my previous estimate towards $g^\\pi_t$. But $g^\\pi_t$ carries a part of noise, so I should be cautious and only take a small step $\\alpha$ in the direction of $g^\\pi_t$.\n",
    "    \n",
    "In turn, the convergence conditions simply state that any value $Q^\\pi(s,a)$ should be reachable given any initial guess $Q(s,a)$, no matter how far is this first guess from $Q^\\pi(s,a)$; hence the $\\sum\\limits_{t=0}^\\infty \\alpha_t = \\infty$. However, we still need the step-size to be decreasing so that we don't start oscillating around $Q^\\pi(s,a)$ when we get closer; so to insure convergence we impose $\\sum\\limits_{t=0}^\\infty \\alpha_t^2 < \\infty$.\n",
    "</div>\n",
    "\n",
    "So this provides us with a way to estimate $Q^\\pi(s,a)$ from experience samples rather than from a model.  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as stochastic approximation**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic approximation updates of $Q$ under the form:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(g^\\pi(s,a) - Q(s,a)\\right).$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A more modern formulation of Stochastic Approximation is Stochastic Gradient Descent. So we will slightly generalize the formulation above.  \n",
    "\n",
    "$Q^\\pi$ is the function that minimizes\n",
    "$$L(Q) = \\frac{1}{2} \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 dsda.$$\n",
    "\n",
    "Recall that in the most general case, $Q$ is a function, but for the sake of clarity, we will momentarily suppose that $S\\times A$ is finite and thus $Q$ is equivalent to the vector of all $Q(s,a)$ values.\n",
    "\n",
    "Then, minimizing $L(Q)$ can be done via gradient descent:\n",
    "$$\\nabla_Q L(Q) = \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right) \\right] \\nabla_Q Q(s,a) dsda.$$\n",
    "\n",
    "Suppose we have a set of independently drawn states and actions $\\left\\{(s_i, a_i)\\right\\}_{i\\in [1,N]}$. Then, this gradient can be approached via a Monte Carlo estimator:\n",
    "$$\\sum_{i=1}^N \\left[ Q(s_i,a_i) - \\mathbb{E}\\left(G^\\pi(s_i,a_i)\\right)\\right] \\nabla_Q Q(s_i, a_i).$$\n",
    "\n",
    "In our example where $Q$ is the vector of values taken in each state and action, \n",
    "$\\nabla_Q Q(s_i,a_i) = \\left[ \\begin{array}{c} 0\\\\ \\vdots\\\\ 0\\\\ 1 \\\\ 0\\\\ \\vdots\\\\ 0 \\end{array} \\right]$ \n",
    "where the \"1\" is at the position corresponding to $s_i,a_i$ in the vector $Q$.\n",
    "\n",
    "As for Stochastic Approximation, if we can obtain independent realizations $g^\\pi(s_i,a_i)$ of $G^\\pi(s_i,a_i)$, then we can estimate this gradient as:\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "\n",
    "And thus we have the Stochastic Gradient Descent update:\n",
    "$$Q \\leftarrow Q - \\alpha \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "\n",
    "This update mechanism yields a sequence $Q_t$ of value functions that converges to $Q^\\pi$ if the gradient steps $\\alpha$ respect Robbins-Monro conditions.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as Stochastic Gradient Descent**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform Stochastic Gradient Descent updates on $Q$:\n",
    "$$Q \\leftarrow Q + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i,a_i) - Q(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Note that if $N=1$, the update above falls back to the Stochastic Approximation update: having a sample in $s_i,a_i$ only updates $Q(s_i,a_i)$.\n",
    "\n",
    "For the sake of simplicity, we will keep the Stochastic Approximation perspective in further developments. The transition to SGD is straightforward.\n",
    "\n",
    "So, overall, if we manage to draw independent samples $g^\\pi(s_i,a_i)$ of $G^\\pi(s,a)$ in all $s,a\\in S\\times A$, we can **learn** the value $Q^\\pi$ (or $V^\\pi$) of policy $\\pi$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the sample $(s_t,a_t,r_t,s_{t+1})$ obtained at time $t$.\n",
    "\n",
    "Once this transition is over we can update our knowledge of $Q(s_t, a_t)$ by using $r_t+\\gamma Q(s_{t+1},\\pi(s_{t+1}))$. This estimate uses $Q(s_{t+1},\\pi(s_{t+1}))$ to *bootstrap* [1] the estimator of $Q(s_t, a_t)$.\n",
    "\n",
    "[1] This *bootstrap* operation has nothing to do with the statistical procedure of *bootstrapping*.\n",
    "\n",
    "This idea, which was first introduced in R. Sutton's **[Learning to predict by the methods of temporal differences](https://link.springer.com/article/10.1007/BF00115009)** article, has a strong parallel with the evaluation equation. This equation was:\n",
    "$$Q^\\pi \\left(s,a\\right) = \\mathbb{E}_{s' \\sim p\\left(s'|s,a\\right)} \\left[ r\\left(s,a,s'\\right) + \\gamma Q^\\pi \\left(s', \\pi(s')\\right) \\right]$$  \n",
    "\n",
    "The key remark is that the sample $g^\\pi_t$ of $Q^\\pi(s_t,a_t)$ can be built by summing $r_t$ and $\\gamma Q_t(s_{t+1}, \\pi(s_{t+1}) )$:\n",
    "$$g_t = r_t + \\gamma Q_t(s_{t+1}, \\pi(s_{t+1})).$$\n",
    "\n",
    "Note that in the expression above, we have used $Q_t$ to emphasize that we use the function $Q$ as it was at time step $t$, to define the target $g^\\pi_t$ used in the update that will provide $Q_{t+1}$.\n",
    "\n",
    "Formally, this comes directly from the evaluation operator. Let's rewrite $T^\\pi$ in terms of random variables.\n",
    "$$(T^\\pi Q)(s,a) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma Q(S', \\pi(S')) \\right]$$\n",
    "\n",
    "Since $Q^\\pi$ is the fixed point of $T^\\pi$, by taking $g_t = r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$ we are taking one stochastic approximation step in the direction of $T^\\pi Q_t$. \n",
    "\n",
    "**Bootstrapping** (in this particular context) is the operation of using the value of $Q_t(s_{t+1},\\pi(s_{t+1}))$ in the update of $Q$.\n",
    "\n",
    "Then the stochastic approximation update becomes what is called the **TD(0) update**:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) update:**  \n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left(r_t + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)\\right).$$\n",
    "    \n",
    "This update consists in taking one stochastic approximation step in the direction of $T^\\pi Q$.\n",
    "</div>\n",
    "\n",
    "The SGD update is almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's insist on this point:  \n",
    "TD(0) does not directly solve $Q=\\mathbb{E}\\left[\\sum_t\\gamma^t R_t \\right]$ (this is what other methods, called *Monte Carlo*, do). Instead, it implements stochastic approximation on top of the repeated application of the $T^\\pi$ operator. So it solves $Q_{n+1} = T^\\pi Q_n$. At each step $t$, it takes the current value function $Q_t$, draws one or several samples from $T^\\pi Q_t$ and approximates $T^\\pi Q_t$ by taking one step of gradient descent from $Q_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\delta_t=r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1})) - Q_t(s_t,a_t)$ is called the prediction **temporal difference** (hence the name of the algorithm - the \"0\" won't be explained here). It is the difference between our estimate $Q_t(s_t,a_t)$ *before* obtaining the information of $r_t$, and the bootstrapped value $r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$.\n",
    "<div class=\"alert alert-success\"><b>Temporal difference:</b>\n",
    "$$\\delta=r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it seems obvious that if some state-action pair $s,a$ is never visited, then no update of its $Q(s,a)$ can ever take place. Therefore, for the TD(0) update to converge, we need to guarantee that all state action pairs will be visited frequently enough for $Q$ to converge to $Q^\\pi$.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>TD(0) temporal difference update on $Q$-functions:</b><br>\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s',\\pi(s')) - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Interestingly, this algorithm puts restrictions on the policy we apply when interacting with the environment. We will call such a policy a **behavior policy**. The behavior policy and the policy being learned might be different (in the case of TD(0), this even is an obligation since we need to enforce visitation of all state-action pairs).\n",
    "\n",
    "Vocabulary: **Off-policy** evaluation algorithms can use a behavior policy that is different than the policy being evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Live coding:</b><br>\n",
    "Let's implement TD(0) on $Q$-functions.<br>\n",
    "To insure that all states and actions are sampled infinitely often, we take a behavior policy that acts randomly in each state.<br>\n",
    "We take $\\gamma=0.9$ and run the algorithm for 2000000 time steps.<br>\n",
    "To keep things simple, we take a constant $\\alpha=0.01$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "max_steps=1000000\n",
    "Qtd = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qtrue = Qpi_sequence[-1]\n",
    "\n",
    "error = np.zeros((max_steps))\n",
    "x = env.reset()\n",
    "for t in range(max_steps):\n",
    "    a = np.random.randint(4)\n",
    "    y,r,d,_ = env.step(a)\n",
    "    Qtd[x][a] = Qtd[x][a] + alpha * (r+gamma*Qtd[y][fl.RIGHT]-Qtd[x][a])\n",
    "    error[t] = np.max(np.abs(Qtd-Qtrue))\n",
    "    if d==True:\n",
    "        x = env.reset()\n",
    "    else:\n",
    "        x=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Max error:\", np.max(np.abs(Q-Qtrue)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many improvements, like [Experience Replay (Lin, 1992)](https://link.springer.com/article/10.1007/BF00992699) for instance, produce better gradient estimates. We won't cover them here and reserve them for future classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Approximate Value Iteration as Stochastic Approximation: Q-learning\n",
    "\n",
    "We can directly adapt the idea of temporal difference learning to the Approximate Value Iteration case.\n",
    "\n",
    "In this case, we want to learn $T^* Q_t$ (instead of $T^\\pi Q_t$) so our samples are:\n",
    "$$g_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a').$$\n",
    "\n",
    "And the learning algorithm becomes the famous Q-learning algorithm, introduced by C. J. Watkins in his [PhD thesis](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf) in 1989:\n",
    "<div class=\"alert alert-success\"><b>Q-learning</b><br>\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^*$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To implement a Q-learning algorithm, one needs to decide on a behavior policy. As for TD(0), Q-learning will converge to $Q^*$, provided that all states and actions are visited infinitely often. It is actually notable that $Q$ converges to $Q^*$ even if the behavior policy does not. But it also looks like a waste of computational resources to keep exploring uniformly around the starting state.\n",
    "\n",
    "This tradeoff between exploring new actions and exploiting what has already been inferred in $Q$ is called the **exploration versus exploitation tradeoff**. It is a crucial problem that strongly affects the ability of the algorithm to discover new, interesting rewards.\n",
    "\n",
    "Here we will implement a rather naive tradeoff strategy called an $\\epsilon$-greedy behavior. It consists in picking the $Q$-greedy action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$.\n",
    "\n",
    "$\\epsilon$ will start at 1 and then will periodically be divided by 2.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Live coding:**\n",
    "\n",
    "Write a function that picks an epsilon-greedy action.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    a = np.argmax(Q[s,:])\n",
    "    if(np.random.rand()<=epsilon): # random action\n",
    "        aa = np.random.randint(env.action_space.n-1)\n",
    "        if aa==a:\n",
    "            a=env.action_space.n-1\n",
    "        else:\n",
    "            a=aa\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Live coding:**\n",
    "\n",
    "Write a Q-learning algorithm on FrozenLake. Keep track of the error w.r.t. $Q^*$ and the number of times each state-action pair is visited.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's restart from the previous Qpi\n",
    "Qql = Qpi_sequence[-1]\n",
    "max_steps = 2000000\n",
    "\n",
    "# Q-learning\n",
    "count = np.zeros((env.observation_space.n,env.action_space.n)) # to track update frequencies\n",
    "epsilon = 1\n",
    "x = env.reset()\n",
    "for t in range(max_steps):\n",
    "    if((t+1)%1000000==0):\n",
    "        epsilon = epsilon/2\n",
    "    a = epsilon_greedy(Qql,x,epsilon)\n",
    "    y,r,d,_ = env.step(a)\n",
    "    Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "    count[x][a] += 1\n",
    "    if d==True:\n",
    "        x = env.reset()\n",
    "    else:\n",
    "        x=y\n",
    "\n",
    "# Q-learning's final value function and policy\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qopt_sequence[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Live coding:**\n",
    "\n",
    "Display the visitation frequency</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Live coding:**\n",
    "\n",
    "Display the final policy.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=np.int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "print(\"Final epsilon:\", epsilon)\n",
    "pi_ql = greedyQpolicy(Qql)\n",
    "print(\"Greedy Q-learning policy:\")\n",
    "print_policy(pi_ql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Previous sections had shown how to charaterize and find optimal policies given the MDP model. We have built on the results of Approximate Dynamic Programming to **learn** optimal value functions from interaction samples.\n",
    "\n",
    "So we have built the third stage of our three-stage rocket:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**How do we learn an optimal strategy?**  \n",
    "To learn an optimal strategy, we rely on a stochastic approximation of $Q^*$, given samples drawn from the MDP. This stochastic approximation of $Q^*$ is actually a stochastic approximation of the $Q_n$ sequence of approximate value iteration. In the end, we need to find good approximation architectures and to control the exploration versus exploitation tradeoff.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: variations on TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**\n",
    "Write a function `TD_Qeval` that runs TD(0) on tabular $Q$ functions for a discrete state-action environment and a given policy (it's almost the same code than in section 7.1). Add an option for providing the true $Q$ function and monitoring the error along training. Use the code below to compute the $Q$ function of the policy that always goes right in FrozenLake and plot the evolution of the error between `Qtrue` (model-based computations of previous sections) and $Q$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=np.int)\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "Qpi0 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "Qpi0, error = TD_Qeval(pi=pi0, max_steps=3000000, env=env, alpha=alpha, gamma=gamma, Qpi=Qpi0, Qtrue=Qpi_sequence[-1])\n",
    "print(\"Max error:\", np.max(np.abs(Qpi0-Qpi0true)))\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "We wrote the derivation of (batch, SGD) TD(0) using $Q$ functions defined on the discrete $S\\times A$ space, and taking the gradient with respect to the vector $Q$. Write the same derivation with a parametric state value function $V_\\theta$, then with a parametric state-action value function $Q_\\theta$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^\\pi(s) = \\mathbb{E} (G^\\pi(s))$. So we consider samples $g^\\pi(s)$ of $G^\\pi(s)$.\n",
    "\n",
    "The loss function becomes $L(\\theta) = \\int_S \\left[ V_\\theta(s) - \\mathbb{E}\\left(G^\\pi(s)\\right)\\right]^2 ds$.\n",
    "    \n",
    "The gradient estimator becomes:\n",
    "$$d = \\sum_{i=1}^N \\left[ V_\\theta(s_i) - g^\\pi(s_i)\\right] \\nabla_\\theta V_\\theta(s_i).$$\n",
    "\n",
    "And so the update becomes:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i) - V_\\theta(s_i)\\right] \\nabla_\\theta V_\\theta(s_i)$$\n",
    "    \n",
    "For a parametric $Q_\\theta$:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i,a_i) - Q_\\theta(s_i,a_i)\\right] \\nabla_\\theta Q_\\theta(s_i).$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "We derived the TD(0) algorithm for $Q$ functions. Let us draw inspiration from the previous exercise and the derivation of this section to write TD(0) on $V$ functions.\n",
    "- First, recall $T^\\pi$ in terms of an expectation over random variables $R$ and $S'$.\n",
    "- Then define a bootstrapped sample $g_t$ of this expectation.\n",
    "- Finally write the TD(0) SGD update on parametric state value functions, and the corresponding temporal difference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $T^\\pi$ operator: $(T^\\pi V)(s) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma V(S') \\right]$\n",
    "- Bootstrap sample: $g_t = r_t + \\gamma V_t(s_{t+1})$.\n",
    "- The TD(0) update is $\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma V_\\theta(s_{i+1}) - V_\\theta(s_i) \\right] \\nabla_\\theta V_\\theta(s_i)$.  \n",
    "    The temporal difference is $\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "When deriving TD(0) on $Q$ functions, we wrote that we needed to enforce the visitation of every state-action pair when obtaining samples from the MDP. Is it still the case for TD(0) on $V$ functions? Is TD(0) on $V$ functions an off-policy algorithm?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We need $g_t$ to be a sample of $G^\\pi(s_t) = R_t + \\gamma V(S_{t+1})$. So,  $r_t$ should be a sample of $R_t$, that is the reward obtained by taking action $\\pi(s_t)$ in $s_t$. Additionally, $S_{t+1}$ should be drawn according to $p(\\cdot | s_t,\\pi(s_t)$. So the behavior policy **needs** to be $\\pi$, otherwise the samples lose all meaning. TD(0) on $V$ functions is an **on-policy** algorithm: it's behavior policy is constrained to be the one under evaluation.\n",
    "\n",
    "One could remark that since the policy applied is $\\pi$, we cannot guarantee that all states will be visited. However, the visited states will be those reachable by $\\pi$, from the initial state. Refer to section 5's exercise on the stationary distribution for a discussion on the reachable states and their visitation frequency.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider the policy that always moves right in FrozenLake and implement a TD(0) estimation for $V^\\pi$.  \n",
    "We can tolerate a 0.001 error on $V^\\pi$ so, to keep things simple, we set $\\alpha=0.001$.  \n",
    "Take $\\gamma = 0.9$ and run the algorithm for 2000000 time steps.  \n",
    "After each episode, compare the value function obtained with that computed with the model-based approaches of section 6.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will call $\\beta$ the *behavior policy*. It is the policy being applied to interact with the environment. Off-policy evaluation algorithms can use a behavior policy that is different than the policy being evaluated. We will call *behavior distribution* the distributions $\\rho^\\beta(s)$ over states and $\\rho^\\beta(s,a)$ over state-action pairs, induced by applying $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: delayed updates and experience replay for TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that TD(0), at each time step, takes a gradient step in the direction of $T^\\pi Q$.\n",
    "\n",
    "The result of this gradient step is an approximation of $T^\\pi Q$.\n",
    "\n",
    "We saw in section 6 that despite an approximation operator $\\mathcal{A}$ with controlled error, the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ still converged to a neighborhood of $Q^*$. The same result holds for the sequence $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$.\n",
    "\n",
    "One single step of stochastic gradient descent makes for a poor approximator. Given a fixed function $Q_n$, if we repeat a certain number $C$ of such gradient steps, we can hope to obtain a better estimate of $T^\\pi Q_n$. So there is an interest in keeping two $Q$ functions. The first is the current estimator, which plays the role of $Q_n$, upon which we apply $T^\\pi$, and which we call the *target* function $Q^-$. The second is the one we actually optimize upon and which aims at approximating $T^\\pi Q^-$; we write it $Q$. Every $C$ gradient steps, we replace $Q^-$ by $Q$ and repeat.\n",
    "\n",
    "Consequently, this procedure of **delayed updates** trades off advancing in the $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$ sequence for better approximation properties for $\\mathcal{A}$.\n",
    "\n",
    "This makes more apparent the remark made before that TD(0) actually solves the $Q_{n+1} = T^\\pi Q_n$ sequence and thus successively minimizes a sequence of losses:\n",
    "$$L_n(Q) = \\| Q - T^\\pi Q_n \\|_2.$$\n",
    "The loss changes everytime we replace $Q_n$ by $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What's the value of $C$ in vanilla TD(0)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Vanilla TD(0) replaces $Q^-$ by $Q$ at every step, so $C=1$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written several times the update of TD(0) with a batch of sampels. But in order to be able to perform stochastic gradient steps using more than one sample, one needs to keep samples in memory.\n",
    "\n",
    "Recall the loss we defined to introduce the stochastic gradient update:\n",
    "$$L(Q) = \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 ds da.$$\n",
    "\n",
    "Recall also that $d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$ is an unbiased estimate of $\\nabla_Q L(Q)$ only if the $g^\\pi(s_i,a_i)$ are drawn **independently** and **identically** according to the distribution of $G^\\pi(s,a)$.\n",
    "\n",
    "This last condition can only be verified if \n",
    "1. the $s_i,a_i$ are drawn independently of each other and always according to the same distribution $\\rho(s,a)$, and\n",
    "2. given $s_i,a_i$, the realizations $g^\\pi(s_i,a_i)$ are drawn independently of each other and according to the distribution of $G^\\pi(s_i,a_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What do you think? Is condition 1 verified in vanilla TD(0)? What about condition 2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Let's write $(s_i,a_i,r_i,s'_i)$ the $i$th sample and $\\beta$ the behavior policy.\n",
    "    \n",
    "Condition 1 is not verified for vanilla TD(0). It is true that if the behavior policy is constant, then on average the state samples $s_i$ are drawn **identically**, according this policy's stationary state distribution $\\rho^\\beta(s)$ and the action samples are drawn according to $\\beta(\\cdot | s)$. However, successive samples are not drawn **independently** by definition, since $\\mathbb{P}(S_{t+1})$ is actually conditioned by $S_t$ and $A_t\\sim\\beta(\\cdot | S_t)$.\n",
    "\n",
    "    \n",
    "Condition 2, on the other hand is easier to verify for TD(0) updates: since the reward $r_i$ and the next state $s'_i$ are only conditioned by $s_i$ and $a_i\\sim \\beta(\\cdot | s_i)$, the $g^\\pi(s_i,a_i) = r_i + \\gamma Q(s'_i,\\pi(s'_i))$ are all drawn identically and independently of each other.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this, TD(0) with tabular function representation still converges as long as states and actions are tried frequently enough. In some cases of function approximation this is also still true. But we might have found a major issue here in the most general case.\n",
    "\n",
    "One way to (approximately) recover the conditional independence of sampled states $s_i$ is to store a large number of samples $(s_i,a_i,r_i,s'_i)$ in memory, and draw samples uniformly at random from this memory for the TD(0) updates. This idea was first introduced by Lin in his 1992 **[Self-improving reactive agents based on reinforcement learning, planning and teaching](https://link.springer.com/article/10.1007/BF00992699)** article, under the name of *experience replay* (although his derivation was not exactly the same and applied to the Q-learning algorithm which we reserve for another class).\n",
    "\n",
    "The memory of samples is generally called an *experience replay memory* or *experience replay buffer*, since it allows the learning agent to store past experience in memory and recall it (replay it) as many times as necessary to facilitate learning.\n",
    "\n",
    "Drawing uniformly randomly from a replay buffer preserves the stationary distribution which generated the samples and breaks the conditional dependency between successive samples in a trajectory.\n",
    "\n",
    "Combined with the delayed updates introduced earlier, this yields a general, practical algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) with delayed updates and experience replay:**  \n",
    "Given a set of samples $\\left\\{(s_i,a_i,r_i,s'_i)\\right\\}_{i\\in [1,N]}$ all drawn from a fixed behavior distribution, and a *target* function $Q^-$, the gradient update is:\n",
    "$$Q \\leftarrow Q + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma Q^-(s'_{i},\\pi(s'_i)) - Q_\\theta(s_i,a_i) \\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, under the Robbins-Monro conditions, and under repeated substitution of $Q^-$ by $Q_\\theta$ every $C$ gradient updates, this procedure converges to $Q^\\pi$ that minimizes $\\|Q - Q^\\pi \\|_2$.\n",
    "</div>\n",
    "\n",
    "We can write the same update in the case of parametric function approximation.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) with delayed updates, experience replay and parametric function approximation:**  \n",
    "Given a set of samples $\\left\\{(s_i,a_i,r_i,s'_i)\\right\\}_{i\\in [1,N]}$ all drawn from a fixed behavior distribution, a *target* function $Q^-$,  and a parametric function approximator $Q_\\theta$, the gradient update is:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma Q^-(s'_{i},\\pi(s'_i)) - Q_\\theta(s_i,a_i) \\right] \\nabla_\\theta Q_\\theta(s_i,a_i)$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, under the Robbins-Monro conditions, and under repeated substitution of $Q^-$ by $Q_\\theta$ every $C$ gradient updates, this procedure converges to $\\theta^\\pi$ that minimizes $\\|Q_\\theta - Q^\\pi \\|_2$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: the importance of the behavior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written above that TD(0) on $Q$ functions is an off-policy algorithm since it will estimate $Q^\\pi$ whatever the behavior policy is, as long as this policy explores all state-action pairs infinitely often.\n",
    "\n",
    "This is actually only true if we are working with tabular representations of $Q$ functions.\n",
    "\n",
    "Let's return to the gradient descent formulation for a minute. We wrote that the loss was:\n",
    "$$L(Q) = \\int_S \\int_A \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 ds da.$$\n",
    "    \n",
    "And we wrote the Monte Carlo estimate of this loss' gradient:\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "\n",
    "This is actually only correct if the samples $(s_i, a_i)$ are drawn uniformly in $S \\times A$. But in practice, the samples are drawn in $S\\times A$ according to the behavior distribution $\\rho^\\beta(s,a)$.\n",
    "\n",
    "Let us get back to the general formulation of stochastic gradient descent. For this, we will introduce a distribution $\\Gamma$ over $S\\times A\\times \\mathbb{R}$ that describes the joint distribution of state-action pairs $(s,a)$ and long term returns $G^\\pi(s,a)$ under policy $\\pi$. So, given a behavior distribution $\\rho^\\beta(s,a)$, essentially:\n",
    "$$\\Gamma(s,a,g^\\pi) = \\rho^\\beta(s,a) \\mathbb{P}(G^\\pi(s,a)=g^\\pi|s,a)$$\n",
    "\n",
    "Then finding $Q^\\pi$ amounts to solving the minimization problem $\\min_Q L(Q)$ with:\n",
    "\\begin{align*}\n",
    "L(Q) &= \\mathbb{E}_{s,a,g^\\pi \\sim \\Gamma} \\left[ \\left(Q(s,a) - g^\\pi\\right) \\right],\\\\\n",
    "     &= \\int_S \\int_A \\int_\\mathbb{R} \\left[ Q(s,a) - g^\\pi \\right]^2 \\Gamma(s,a,g^\\pi) ds da dg^\\pi.\n",
    "\\end{align*}\n",
    "\n",
    "Then, **if the samples $(s,a,g^\\pi)$ are drawn according to $\\Gamma(s,a,g^\\pi)$**, the Monte Carlo estimate\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "is actually a correct estimate of $\\nabla_Q L(Q)$.\n",
    "\n",
    "What does it mean that $(s,a,g^\\pi)$ are drawn according to $\\Gamma(s,a,g^\\pi)$? It means state-action pairs are drawn according to $\\rho^\\beta(s,a)$ and the $g^\\pi(s_i,a_i)$ follow the distribution of $G^\\pi(s,a)$\n",
    "\n",
    "Otherwise, this Monte Carlo estimate it is a biased estimator that minimizes another loss function; for example one defined by another behavior distribution $\\rho^\\beta(s,a)$.\n",
    "\n",
    "So when we are summing elements drawn from the replay memory, or when we are using single samples drawn from the interaction with the MDP, we are actually minimizing the loss function defined specifically by $\\rho^\\beta(s,a)$. And in the end, for another behavior distribution, the resulting minimizer $Q$ of the loss might be different from the one obtained by using samples collected with $\\rho^\\beta$.\n",
    "\n",
    "So, does that mean TD(0) on $Q$ functions is not really off-policy?\n",
    "\n",
    "In the case of tabular representations, the fact that the gradients $\\nabla_Q Q(s_i,a_i)$ are actually indicator functions of $(s_i, a_i)$ limits the impact of having different distributions on $S\\times A$. In this case, the minimizers of $L(Q)$ are actually all the same across behavior distributions, as long as the behavior distribution's support spans fully $S\\times A$.\n",
    "\n",
    "However, this nice property is lost in the general case, in particular in the case of function approximation $Q_\\theta$. This has motivated the introduction of the **[Gradient Temporal Difference](https://proceedings.neurips.cc/paper/2008/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html)** family of algorithms. This topic is beyond the scope of this class but the interested reader is encouraged to look at **[H. R. Maei's PhD thesis](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Monte Carlo evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a given policy, we directly *bootstrapped* the estimator of $V^\\pi(s_t,a_t)$, by writing $g_t^\\pi = r_t + \\gamma V_t(s_{t+1})$.\n",
    "\n",
    "But an immediate way to estimate $V^\\pi(s)$ is to take the empirical average of a series if realizations of the $\\sum\\limits_{t = 0}^\\infty \\gamma^t R_t$ random variable.\n",
    "\n",
    "In plain words: simulate $\\pi$ from $s$ a certain number of times to obtain trajectories, observe and accumulate the rewards along each trajectory, take the empirical average over all trajectories.\n",
    "\n",
    "That is precisely what we did in section 5's homework to estimate the value of the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/RL1_exercise1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can we generalize this to learning the value $V^\\pi$ in all states?\n",
    "\n",
    "Let us start with a fully **offline Monte-Carlo** algorithm. The idea is simple: start from $s$, run the policy until termination (or for a long number of steps), repeat for a number of episodes, then update the value of all encountered states. This requires to store in memory full episodes (it also requires that the episodes be finite-length).\n",
    "\n",
    "But we can immediately do better with an **online Monte-Carlo** method. It is almost the same idea: start from $s$, run the policy until termination (or for a long number of steps) then update the value of all encountered states before restarting an episode.  \n",
    "\n",
    "Let $(s_0, r_0, s_1, \\ldots, s_T)$ be the sequence of transitions of such an episode.  \n",
    "For the sake of clarity we will slim down our notations: $g^\\pi(s_t)$ becomes $g_t$.  \n",
    "Then, this sequence provides a sample $g_t$ of $G^\\pi(s_t)$ for all $s_t$ visited during the simulations. \n",
    "<div class=\"alert alert-success\"><b>Monte Carlo return:</b>\n",
    "$$g_t = \\sum_{i>t} \\gamma^{i-t} r_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider the policy that always moves right in FrozenLake and implement an online Monte-Carlo estimatior for $g_t$, that you will use in a Stochastic Approximation method to obtain $V^\\pi$.  \n",
    "We decide we can tolerate a 0.001 error on $V^\\pi$ so, to keep things simple, we set a constant $\\alpha=0.001$.  \n",
    "Take $\\gamma = 0.9$. Run 100000 episodes of maximum 1000 time steps each.  \n",
    "After each episode, compare the value function obtained with that computed with the model-based approaches of previous sections (use the cell below to recall `V_pi0` from section 6's exercises).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/RL2_exercise5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So online Monte-Carlo allows us to update $V^\\pi$ episode after episode. Some values are better estimated than others depending on how often the corresponding state was visited.\n",
    "\n",
    "Monte-Carlo estimation has some flaws nonetheless. It still requires to store one full episode in memory before $V$ is updated. Also, one rare value for $r_t$ affects directly all the value of the states encountered before $s_t$. So we can question the robustness of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: TD($\\lambda$)\n",
    "\n",
    "With Monte Carlo (MC) and TD(0), we have two methods with different features:\n",
    "- TD(0): 1-sample update with bootstrapping\n",
    "- MC: $\\infty$-sample update with no bootstrapping\n",
    "\n",
    "What's inbetween?\n",
    "- inbetween: $n$-sample update with bootstrapping\n",
    "\n",
    "We define the **$n$-step target** or **$n$-step return** $G^{(n)}_t$ from state $s_t$ as the random variable:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l}\n",
    "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots & \\textrm{MC}\\\\\n",
    "G^{(1)}_t = R_t + \\gamma V_t(S_{t+1}) & 1\\textrm{-step TD = TD(0)}\\\\\n",
    "G^{(2)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 V_t(S_{t+2}) & 2\\textrm{-step TD}\\\\\n",
    "G^{(n)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots + \\gamma^n V_t(S_{t+n}) & n\\textrm{-step TD}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "And we define the **$n$-step TD update** as:\n",
    "<div class=\"alert alert-success\"><b>$n$-step TD update:<b>\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ G^{(n)}_t - V(s_t) \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose that the immediate reward $R$ has a constant variance $\\sigma^2$ and that for all states $s$ the estimator $V(s)$ of $V^\\pi(s)$ has bias $\\epsilon$.  \n",
    "What is the variance of $G_t^{(n)}$?  \n",
    "What is the bias of $\\mathbb{E}\\left(G_t^{(n)}(s)\\right)$ as an estimator of $V^\\pi(s)$?  \n",
    "Comment on the impact of choosing a certain value for $n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Reminder: $var(X+Y)=var(X)+var(Y)$ and $var(aX)=a^2 var(X)$.  \n",
    "Consequently:\n",
    "\\begin{align*}\n",
    "    var(G_t^{(n)}) &= \\sum_{i=0}^{n-1} \\gamma^{2i} \\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\\\\\n",
    "     &= \\frac{1-\\gamma^{2n}}{1-\\gamma^2}\\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\n",
    "\\end{align*}\n",
    "    \n",
    "The variance grows with $n$, both because $1-\\gamma^n$ grows with $n$ and because $S_{t+n}$ has larger variance as $n$ grows.\n",
    "    \n",
    "On the bias side:\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - V^\\pi(s) &= \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\mathbb{E}\\left(\\sum_{i=0}^{n-1} \\gamma^i R_{t+i} + \\gamma^n V_t(S_{t+n})\\right)  - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - \\mathbb{E}\\left(\\sum_{i=n}^\\infty \\gamma^t R_t \\right) \\right]\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - V_t(S_{t+n}) \\right]\\\\\n",
    "    &=\\gamma^n \\epsilon\n",
    "\\end{align*}\n",
    "    \n",
    "So the bias decreases with $n$. This makes sense since $V_t$'s importance is weighted by $\\gamma^n$.\n",
    "    \n",
    "Consequently, choosing a value for $n$ is making a bias-variance tradeoff. Small $n$ means small variance an large bias, large $n$ means large variance and small bias. Thus, choosing an intermediate value has in interest in accelerating the convergence of TD algorithms.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So MC corresponds to an $\\infty$-step TD update.  \n",
    "    \n",
    "The $n$-step TD update algorithm converges to the true $V^\\pi$ just as TD(0) or MC. It requires to wait for $n$ time steps before performing an update.\n",
    "\n",
    "Remark: for finite-length  episodes of length $T$, all $n$-step returns for $n>T-t$ are equal to the Monte Carlo return $G_t$.\n",
    "\n",
    "So $n$-step TD updates bridge a gap between MC and TD(0). But it's not quite satisfying yet because we never really know what value of $n$ is appropriate to speed up convergence for a given problem. An interesting property is that we can mix $n$ and $m$-step returns together. Consider $G^{mix}_t = \\frac{1}{3} G^{(2)}_t + \\frac{2}{3} G^{(4)}_t$.\n",
    "Then the update $V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{mix}_t - V(s_t)\\right]$ still converges to $V^\\pi$. More generally, convex sums of $n$-step returns yield update procedures that still converge to $V^\\pi$.\n",
    "\n",
    "Now, take $\\lambda\\in [0,1]$ and consider the $\\lambda$-return $G^\\lambda_t$:\n",
    "<div class=\"alert alert-success\"><b>$\\lambda$-return:</b>\n",
    "$$G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\lambda$-return is the mixing of *all* $n$-step returns, with weights $(1-\\lambda) \\lambda^{n-1}$. So, an agent performing a $\\lambda$-return update looks one step in the future and uses that step to update $V(s)$ with weight $(1-\\lambda)$, then looks 2 steps into the future and updates $V(s)$ with a weight $\\lambda (1-\\lambda)$ and so on. The illustrative figure below is an excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto.\n",
    "\n",
    "<img src=\"img/TD_lambda_forward.png\"></img>\n",
    "\n",
    "To get a better understanding of the $\\lambda$-return and to set ideas, let us consider a finite length episode $(s_t, r_t, s_{t+1}, \\ldots, s_T)$. Since the episode ends after $T$, we have $\\forall k>0, \\ G^{(T-t+k)}_t = G_t$. Thus, we can split the $\\lambda$-return sum in two:\n",
    "\n",
    "\\begin{align*}\n",
    "G^\\lambda_t & = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-1}G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-T+t} G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{k=0}^{\\infty} \\lambda^{k} G_t^{(T-t+k)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have $G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t$.\n",
    "- When $\\lambda = 0$, it is a $TD(0)$ update (hence the \"0\" in TD(0)).\n",
    "- When $\\lambda = 1$, it is a MC update.\n",
    "So we can define the **$\\lambda$-return algorithm** that generalizes on TD(0) and MC:\n",
    "<div class=\"alert alert-success\"><b>$\\lambda$-return algorithm:</b>\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{\\lambda}_t - V(s_t)\\right] $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all very nice and we have replaced the choice of $n$ by the choice of $\\lambda$, which seems less sensitive. But, still, we don't know how to compute those $n$-step returns, and the $\\lambda$-return, without running $n$-step episodes (and thus infinite episodes for the $\\lambda$-return in the general case).\n",
    "\n",
    "This is where we need to flip the little man in the drawing above to make him look backwards in time. When an agent transitions from $s$ to $s'$ and obtains reward $r$, it can compute the $1$-step return for $s$ and perform the corresponding $1$-step TD update. Then, as it transitions from $s'$ to $s''$ and observes $r'$ it can perform the $1$-step TD update in $s'$, but also the $2$-step TD update in $s$! An so on for future transitions. So, incrementally, as time unrolls, the agent will include the $n$-step updates in the $\\lambda$-return of $s$ as they become available. In the limit, when $t\\rightarrow\\infty$, the $\\lambda$-return in every state will be complete and the agent will have completed a $\\lambda$-return algorithm. This figure below (excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto) illustrates this *backward-view* on TD updates.\n",
    "\n",
    "<img src=\"img/TD_lambda_backward.png\"></img>\n",
    "\n",
    "This seems to imply that we need to remember the states we went through, which is quite the same as remembering full episodes for MC updates. But since we want to update a state seen $n$ steps ago with a weight $\\lambda^n (1-\\lambda)$, we just need to remember, for each state, the last time we visited it (and we can forget about the trajectory linking states together). This way, we store $|S|$ values at all time, instead of an increasingly long sequence of transitions. In order to do this, we introduce the notion of **eligibility trace**:\n",
    "<div class=\"alert alert-success\"><b>Eligibility trace of state $s$:</b>\n",
    "$$e_t(s) = \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e_{t-1}(s) & \\textrm{if }s\\neq s_t\\\\\n",
    "1 & \\textrm{if }s = s_t\n",
    "\\end{array}\\right.$$\n",
    "</div>\n",
    "\n",
    "Initially, all states have an eligibility trace of zero. The eligibility trace of an unvisited state decays exponentially. So $e_t(s)$ measures how old the last visit of $s$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that two alternative definitions of eligibility traces prevail:\n",
    "<ul>\n",
    "    <li> \"<b>replacing traces</b>\": $e_t(s) = 1\\textrm{ if }s = s_t$\n",
    "    <li> \"<b>accumulating traces</b>\": $e_t(s) = e_{t-1}(s) + 1\\textrm{ if }s = s_t$\n",
    "</ul>\n",
    "Often (not always), replacing traces are used in practice.<br>\n",
    "<br>\n",
    "And finally we can define the TD($\\lambda$) algorithm:\n",
    "<div class=\"alert alert-success\"><b>TD($\\lambda$) algorithm:</b><br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 & \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "Initially, $e(s)=0$.\n",
    "</div>\n",
    "\n",
    "Properties and remarks:\n",
    "- Earlier states are given $e(s)$ *credit* for the TD error $\\delta$\n",
    "- If the environment contains terminal states, then $e$ should be reset to zero whenever a new trajectory begins.\n",
    "- If $\\lambda=0$, $e(s)=0$ except in $s_t$ $\\Rightarrow$ standard TD(0)\n",
    "- For $0<\\lambda<1$, $e(s)$ indicates a distance $s \\leftrightarrow s_t$ is in the episode.\n",
    "- If $\\lambda=1$, $e(s)=\\gamma^\\tau$ where $\\tau=$ duration since last visit to $s_t$ $\\Rightarrow$ MC method<br>\n",
    "TD(1) implements Monte Carlo estimation on non-episodic problems!<br>\n",
    "TD(1) learns incrementally for the same result as MC\n",
    "- **TD($\\lambda$) is equivalent to the $\\lambda$-return algorithm.**\n",
    "- The value of $\\lambda$ can even be changed during the algorithm without impacting convergence.\n",
    "- TD($\\lambda$) is an on-policy algorithm: samples must be collected following the policy under evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TD($\\lambda$) is already a batch update (it already updates all state values) but not in the sense of SGD batches.\n",
    "\n",
    "However, in the presentation given above, since the eligibility trace $e(s)$ is defined state by state, the formulation of TD($\\lambda$) is limited to discrete state spaces and tabular function representations. The section on function approximation further down will provide an extension of TD($\\lambda$) to linear function approximation.\n",
    "\n",
    "The extension of TD($\\lambda$) to the off-policy setting has been undertaken in the more general work about **[Gradient Temporal Differences](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)** quoted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Implement a TD($\\lambda$) algorithm to estimate $V^\\pi$ fo the policy that always goes right. As before, take a constant $\\alpha=0.001$, $\\gamma=0.9$ and $\\lambda=0.5$. Run the algorithm for 2000000 steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: value function approximation\n",
    "\n",
    "Often, $S$ is not finite (or is just too large to be enumerated). Consequently, $\\mathcal{F}(S,\\mathbb{R})$ has infinite (or just too large) dimension. Thus, tabular representations of $V$ are not possible and one needs to turn to function representations $V_\\theta$ or $Q_\\theta$ with parameters $\\theta$. In this section, we provide a very short introduction to approximation methods for $V$ and $Q$.\n",
    "\n",
    "The FrozenLake example is a toy problem with very few states (moreover discrete). It does not lend itself to a convincing demonstration of value function approximation. We shall remain at the theoretical level for the following considerations and reserve practice for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear value function approximation**\n",
    "\n",
    "Suppose we write $V$ as a linear model:\n",
    "$$V(s) = \\theta^T \\varphi(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$$\n",
    "\n",
    "We wish to approximate $V(s)$ as a linear combination of features $\\varphi(s)=\\left(\\varphi_i(s)\\right)_{i\\in[1,K]}$. This way, $V$ lives in the $K$-dimensional function space $span(\\varphi)$. We have plenty of families of functions that we can rely on and the user's expertise plays a big role in choosing a proper **functional basis**. Generally speaking, we would expect the following properties from a good basis:\n",
    "- the target $V^\\pi$ can be closely approximated by its projection on $\\varphi$\n",
    "- given an initial $V_0 \\in span(\\varphi)$ and the recurrence relation $V_{n+1} = \\Pi_\\varphi (T^\\pi V_n)$ (where $\\Pi_\\varphi$ is the projection operator on $span(\\varphi)$), $V_n$ should be a \"close enough\" approximation of $T^\\pi V_n$. This property is illustrated by the figure below with $Q$ instead of $V$ - excerpt from **[Least-Squares Policy Iteration](https://www.jmlr.org/papers/v4/lagoudakis03a.html)** by M. G. Lagoudakis and R. Parr (2003).\n",
    "- $\\varphi$ should form a basis (that is $\\varphi_i \\bot \\varphi_j$)\n",
    "\n",
    "<img src=\"img/projection.png\" style=\"width: 600px;\"></img>\n",
    "\n",
    "If $\\sum_{i=1}^K \\varphi_i(s) = 1$, then $V_\\theta$ is called an *averager*. Averagers are known to be well-behaved for iterative function approximation. Otherwise, other non-averager families of functions are commonly used:\n",
    "\n",
    "- $\\cos$, $\\sin$ over state variables (mimics the Fourier transform, extends to wavelet bases)\n",
    "- polynomials of the state variables (mimics the Taylor expansion)\n",
    "- radial basis functions of the state variables (performs local approximation, extends to kernel smoothing).\n",
    "- among averagers, piecewise constant local functions $\\varphi_i(s) \\in \\{0;1\\}$ group *neighborhoods* in the state space together (note the similarity with tree-based regressors).\n",
    "\n",
    "A very straightforward way of building feature sets is to define features depending on a single state variable and then using the tensor product in order to obtain all possible combinations of sigle-variable features. More formally and more generally, suppose $S \\subset S_1\\times \\ldots \\times S_k$ and suppose $\\varphi^{(i)}$ defines $d_i$ features over $S_k$; then the tensor product $\\varphi^{(1)} \\otimes \\ldots \\otimes \\varphi^{(k)}$ yields $d=d_1\\ldots d_k$ feature functions on $S$. But there is a catch, the number of these resulting features grows exponentially with $k$ and so does the dimension of the value function's search space $span(\\varphi)$: that is the **curse of dimensionality** that makes searching for a value function exponentially more difficult as the state space dimension grows.\n",
    "\n",
    "Additionnaly, there is **no guarantee** that, for a given $V_n \\in span(\\varphi)$, $T^\\pi V_n$ actually lives in $span(\\varphi)$.\n",
    "\n",
    "But on the bright side, given an initial state $s_0$, the actual reachable space $S'$ given $\\pi$ might be much smaller than $S$. So, in practice, we just need to obtain a good approximation of $V$ on the subspace $S'$.\n",
    "\n",
    "Anyway, to conclude this short paragraph on feature engineering:\n",
    "- good feature engineering in RL is even more crucial than in supervised learning.\n",
    "- it can be very problem-dependent.\n",
    "- good function approximators (generally non-parametric to avoid the fixed $span(\\varphi)$) are of crucial importance.\n",
    "We will discuss non-linear and non-parametric function approximation a bit further in the notebook.\n",
    "\n",
    "Linear function approximation has played a major role in the RL literature, in particular for temporal differences methods. The **[Policy Evaluation with Temporal Differences](https://www.jmlr.org/papers/v15/dann14a.html)** survey by C. Dann et al. (2014) provides a great overview of this literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The tabular case is just a specific case of linear approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "If the sentence above is true, what is the set of basis functions corresponding to a tabular representation of $V$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "In the discrete state space case, consider the averager defined as:\n",
    "$$\\varphi_i(s) = \\left\\{\\begin{array}{ll}1 & \\textrm{if }s=s_i\\\\ 0 & \\textrm{otherwise}\\end{array}\\right.$$\n",
    "Feature function $\\varphi_i$ is the indicator function of state $s_i$. Therefore, we have $|S|$ feature functions. So when we write $V(s) = \\sum_{i=1}^{|S|} \\theta_i \\varphi_i(s)$, we actually have $V(s_i) = \\theta_i$. Therefore the tabular representation of $V$ is equivalent to a linear model with the $\\varphi_i$ feature functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous exercise, let us rewrite TD($\\lambda$) as a linear model update (we take the accumulating traces version; the replacing traces case is equivalent). We had previously:<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 + \\gamma \\lambda e(s)& \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "Initially, $e(s)=0$.\n",
    "\n",
    "The temporal difference can be rewritten $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "\n",
    "The eligibility trace update can be rewritten $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$.\n",
    "\n",
    "Similarly the value update can be rewritten $\\theta \\leftarrow \\theta + \\alpha e \\delta$.\n",
    "\n",
    "Remark:  \n",
    "Recall the discussion on the importance of the behavior distribution? We concluded that tabular representations were a specific case where TD(0) is truly off-policy because the influence of a sample was limited to the value of $Q$ in the corresponding $(s,a)$ pair. This discussion can be generalized for averagers (although it remains an approximation): such local models are well-behaved to suffer less from the shift in behavior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD($\\lambda$) as a linear approximation update**\n",
    "\n",
    "We generalize the previous result to the general linear model case:\n",
    "<div class=\"alert alert-success\"><b>TD($\\lambda$) with linear function approximation:</b><br>\n",
    "With $V(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$, $e \\in \\mathbb{R}^K$.<br>\n",
    "Initially, $e=0$.<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "<li> Update eligibility traces for all states $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$\n",
    "<li> Update value function $\\theta \\leftarrow \\theta + \\alpha e \\delta$\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "Note that we have provided the results above without proof. We will admit them and refer the reader to RL textbooks for a rigorous justification.\n",
    "\n",
    "Further reading on TD($\\lambda$) with linear function approximation: \n",
    "**[True Online TD($\\lambda$)](http://proceedings.mlr.press/v32/seijen14.html)** by H. Van Seijen and R. Sutton (2014).  \n",
    "An unpublished negative result that somehow follows from this article is also that in the general case it is not possible to have a TD($\\lambda$) algorithm performing on non-linear function approximation and being equivalent to the $\\lambda$-return algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parametric models**\n",
    "\n",
    "Non-parametric models generally refer to function approximators that do not rely on an a-priori fixed finite-dimensional search space and allows the representation space to evolve as needed. Among those non-parametrics models, one can count:\n",
    "- linear approximations that incrementally enrich the functional basis (e.g. **[this article](https://dl.acm.org/doi/abs/10.1145/1390156.1390251)**).\n",
    "- general supervised learning methods: SVMs, k-nearest neighbours (kernel smoothing methods), Gaussian Processes, tree-based methods, neural networks, etc.\n",
    "  In this second category, it is generally useful to distinguish between\n",
    "  - methods that explicitly minimize the L2 loss defined earlier (e.g. neural networks),\n",
    "  - methods that minimize some other loss (e.g. random forests) and provide alternate (better or worse guarantees).\n",
    "\n",
    "One quickly realizes that the frontier between parametric and non-parametric models is blur. In the general case of a $s \\mapsto V(s)$ function approximator, the general idea is to feed this approximator with samples of the form $(s, r+\\gamma V(s'))$. But beware: most of the nice results are generally lost when one leaves the realm of linear function approximation. More precisely, when one combines **function approximation**, **off-policy learning** and **bootstrapping** in a temporal difference method, all results are generally lost. This has been studied as the **[Deadly triad of Reinforcement Learning](https://arxiv.org/abs/1812.02648v1)** (H. Van Hasselt et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Generalized Policy Iteration and Actor-Critic architectures\n",
    "\n",
    "Remember how we went from Policy Iteration (figure below) to Asynchronous Policy Iteration?\n",
    "<img src=\"img/policyiteration.png\"></img>\n",
    "\n",
    "Now recall the principle of Asynchronous Policy Iteration. If we consider the two elementary operations:\n",
    "- Bellman backup on $Q$: $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum\\limits_{s'} p(s'|s,a) Q(s',\\pi(s'))$\n",
    "- Bellman backup on $\\pi$: $\\pi(s) \\leftarrow \\arg\\max_{a} Q(s,a)$\n",
    "\n",
    "Then, as long as every state and every action is visited infinitely often for Bellman backups on $Q$ or $\\pi$, the sequences of $Q_n$ and $\\pi_n$ converge to $Q^*$ and $\\pi^*$.\n",
    "\n",
    "Value Iteration: in each state, one update of $Q$ and one improvement of $\\pi$.<br>\n",
    "Policy Iteration: update $Q$ in all states until convergence, then update $\\pi$ in all states.\n",
    "\n",
    "**Generalized Policy Iteration** is the case where one has two interacting processes: policy evaluation and policy improvement, directly from samples (not from the model anymore). If these processes converge to their respective targets, then Generalized Policy Iteration converges to $Q^*$ and $\\pi^*$. Model-free policy evaluation can take many forms: indirect RL, Monte Carlo evaluations, TD methods...\n",
    "\n",
    "This leads to the definition of the general **actor-critic architectures** (excerpt from [**Algorithms for Reinforcement Learning**](https://sites.ualberta.ca/~szepesva/rlbook.html) by C. Szepesvari):\n",
    "\n",
    "<img src=\"img/actor-critic.png\"  style=\"width: 500px;\"></img>\n",
    "\n",
    "In such architectures, an *actor* chooses an action based on the current state and the information provided by the *critic*, while the *critic* constantly aims at learning relevant things in order to help the *actor* decide (value functions for example, or an approximate model).\n",
    "\n",
    "Almost all Reinforcement Learning algorithms fall into an actor-critic architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose we modify TD(0) on $Q$-functions so that, instead of picking a random action at each time step, we pick the greedy action with respect to $Q$. What is the risk in doing this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A deterministic, greedy policy will not try all actions in all states infinitely often. So we can't guarantee convergence, neither of $Q$ to any policy's $Q^\\pi$ (because if some states are not visited no updates will take place for them), nor of the $Q$-greedy policy to $\\pi^*$. The conditions for the convergence of Generalized Policy Iteration are just not met.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this issue we introduce the notion of **Greedy in the limit of infinite exploration** (GLIE) actor:  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Greedy in the limit of infinite exploration</b> (GLIE) actor:<br>\n",
    "A GLIE actor guarantees that:<br>\n",
    "- All state-action pairs $(s,a)$ are visited infinitely often for updates of $Q$, as $t\\rightarrow\\infty$:\n",
    "$$\\lim_{t\\rightarrow\\infty} count_t(s,a) = \\infty$$\n",
    "- As $t\\rightarrow\\infty$ the actor becomes $Q$-greedy, that is:\n",
    "$$\\lim_{t\\rightarrow\\infty} \\pi_t(a|s) = \\mathbb{1}\\left(a = \\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\right)$$\n",
    "</div>\n",
    "\n",
    "An example of GLIE actor is the so-called $\\epsilon$-greedy exploration strategy (that we introduced with Q Learning) that uniformly picks a non-greedy action with probability $\\epsilon$:\n",
    "$$\\pi_t(a|s) = \\left\\{\\begin{array}{ll}1-\\epsilon_t & \\textrm{if }a=\\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\\\\n",
    "\\frac{\\epsilon_t}{|A|-1} & \\textrm{otherwise} \\end{array}\\right.$$\n",
    "With a parameter $\\epsilon_t>0$ that goes to zero as $t$ tends to $\\infty$, one obtains a GLIE actor.\n",
    "\n",
    "GLIE actors (or policies) enforce the limits of the **exploration vs. exploitation tradeoff**. As long as actors respect the GLIE properties, actor-critic architectures fall within the convergence properties of Generalized Policy Iteration.\n",
    "\n",
    "With this last definition, we have all the ingredients to define algorithms that evolve to an optimal behaviour.\n",
    "\n",
    "**A remark**\n",
    "\n",
    "You might have noticed that the totally random policy we applied in the TD(0) update was a very naive choice. Even if all states are reachable from anywhere in the state space (ergodicity property), they might not all be visited with the same frequency and therefore the convergence to $Q^\\pi$ might be delayed because of this uniform exploration strategy.  \n",
    "\n",
    "The same remark applies to $\\epsilon$-greedy exploration strategies that often start with $\\epsilon=1$ and let it slowly decrease towards zero. These strategies don't account for the actual *visitation frequencies* of state-action pairs. In some states it might be good to keep a strong exploration probability because they actually have been seldom visited, while in other states, a faster decrease is desirable. This links also to the question of *value propagation* that was underpinned by the asynchronous value iteration remarks in section 6.  \n",
    "\n",
    "The point here is to notice that $\\epsilon$-greedy is a simple, very naive exploration strategy that fits within the GLIE requirements but that much better exploration policies are possible by taking the current state into account (contextual exploration) or by using the values of the temporal difference (prioritized experience replay) and the $Q$ estimate (Boltzmann policies, $E^3$ or $R_{max}$ strategies) for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SARSA**\n",
    "\n",
    "The key idea behind the SARSA algorithm is to build a critic that constantly tries to evaluate the value $Q$ of the actor's policy $\\pi$, and an actor that tends to be greedy with respect to this critic. The algorithm is written:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**SARSA**  \n",
    "In $s$, choose (*actor*) $a$ using $Q$, then repeat:\n",
    "<ol>\n",
    "<li> Observe $r$, $s'$\n",
    "<li> Choose $a'$ (<i>GLIE actor</i>) using $Q$\n",
    "<li> Temporal difference: $\\delta=r+\\gamma Q(s',a') - Q(s,a)$\n",
    "<li> Update $Q$: $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta$\n",
    "<li> $s\\leftarrow s'$, $a\\leftarrow a'$\n",
    "</ol>\n",
    "SARSA converges if the actor is GLIE and if $\\alpha$ respects the Robbins-Monro conditions.\n",
    "</div>\n",
    "\n",
    "It is important to note that SARSA is an **on-policy** critic: it constantly evaluates the current $\\pi$... that constantly shifts towards $\\pi^*$ by being $Q$-greedy.\n",
    "\n",
    "The name SARSA comes from the usage of an augmented sample $(s,a,r,s',a')$.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Let's implement an $\\epsilon$-greedy SARSA on the FrozenLake problem.<br>\n",
    "For the decrease of $\\epsilon$ we can opt for a division by 2 every million steps.<br>\n",
    "Keep track of the state-action visitation count, as in the Q-learning example.<br>\n",
    "You can compare with $Q^*$ and $\\pi^*$ obtained during the model-based class.<br>\n",
    "The utility functions below are here to make the task easier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-based policy optimization and a few utility functions\n",
    "\n",
    "def value_iteration(V,epsilon,max_iter):\n",
    "    W = np.copy(V)\n",
    "    residuals = np.zeros((max_iter))\n",
    "    for i in range(max_iter):\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q = np.zeros((env.action_space.n))\n",
    "            for a in range(env.action_space.n):\n",
    "                outcomes = env.unwrapped.P[s][a]\n",
    "                for o in outcomes:\n",
    "                    p  = o[0]\n",
    "                    s2 = o[1]\n",
    "                    r  = o[2]\n",
    "                    Q[a] += p*(r+gamma*V[s2])\n",
    "            W[s] = np.max(Q)\n",
    "            #print(W[s])\n",
    "        residuals[i] = np.max(np.abs(W-V))\n",
    "        #print(\"abs\", np.abs(W-V))\n",
    "        np.copyto(V,W)\n",
    "        if residuals[i]<epsilon:\n",
    "            residuals = residuals[:i+1]\n",
    "            break\n",
    "    return V, residuals\n",
    "\n",
    "def Q_from_V(V):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[s][a]\n",
    "            for o in outcomes:\n",
    "                p  = o[0]\n",
    "                s2 = o[1]\n",
    "                r  = o[2]\n",
    "                Q[s,a] += p*(r+gamma*V[s2])\n",
    "    return Q\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(Vstar)\n",
    "print(actions)\n",
    "print(Vstar)\n",
    "print(Qstar)\n",
    "pi_star = greedyQpolicy(Qstar)\n",
    "print_policy(pi_star)\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL4_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same remark as for TD(0) holds: with a better exploration strategy, the convergence to Qstar could be much more efficient. We can see that some states have been visited (and updated) way more often than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercices (open questions, no solutions provided):</b><br>\n",
    "<ul>\n",
    "<li><b>Context-dependent exploration</b><br>\n",
    "Can you implement an $(s,a)$-dependent $\\epsilon$-greedy exploration strategy (by using the `count` table introduced earlier for instance)?\n",
    "<li><b>Heuristic initialization on $Q$</b><br>\n",
    "For Q-learning, can you think of an initialization of $Q$ that would be better than plain zeros (for example by exploiting the maximum 1-step reward $r_{max}$)? One that, for instance, would drive the exploration towards unvisited states?\n",
    "<li><b>Reward shaping</b><br>\n",
    "Did you notice that falling into a hole brings no penalty? If we introduced a $-1$ reward for falling into a hole, would it change the optimal policy?\n",
    "<li><b>SARSA($\\lambda$)</b><br>\n",
    "SARSA is an on-policy method and we've seen that so is TD($\\lambda$) so it seems rather straightforward to implement a SARSA($\\lambda$) algorithm that, hopefully, will have better convergence properties than plain SARSA.\n",
    "<li><b>$Q(\\lambda)$</b><br>\n",
    "This time it is not as straightfoward to derive a $Q(\\lambda)$ algorithm from TD($\\lambda$), precisely because TD($\\lambda$) evaluates the policy being applied and not another one. Can you imagine a way to still perform $Q(\\lambda)$ updates? An answer is found in Watkins's thesis that introduces Q-learning in 1989. For a more recent approach and other references, see Sutton et al, <b>A new Q($\\lambda$) with interim forward view and Monte Carlo equivalence</b>, 2014)\n",
    "<li><b>SARSA and $Q$-learning with linear value function approximation</b><br>\n",
    "Can you implement an approximate version of SARSA and $Q$-learning with linear Q-function approximation $Q(s,a)=\\theta^T\\varphi(s,a)$?\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Challenges in RL and RLVS classes (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### General summary\n",
    "\n",
    "We are reaching the end of this class and it is time to summarize the take-away messages.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What is Reinforcement Learning?**  \n",
    "RL is the discipline that studies the *learning* process of optimal control policies in the MDP framework.  \n",
    "Its roots overlap Cognitive Psychology, Control Theory, Artificial Intelligence, Machine Learning.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What are the building blocks of RL?**  \n",
    "RL is built upon the framework of Markov Decision Processes.  \n",
    "It draws from the characterization of optimal policies that maximize a given criterion, notably through Bellman's equations.  \n",
    "It learns stochastic approximation (or SGD) solutions to these equations using interaction samples.\n",
    "</div>\n",
    "\n",
    "Of course we have barely touched the surface of RL. Some methods do not rely on the Bellman equations, some others don't use Value Iteration for instance. The overall goal of this class was to acquire a common vocabulary and set of concepts, so that you become comfortable with the objects often manipulated in RL. The exercises should help you go a few steps deeper.\n",
    "\n",
    "[RLVS](https://rl-vs.github.io/rlvs2021) is an online school on Reinforcement Learning that was organized in 2021. It provides insightful classes on many different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  Three intrinsic challenges in Reinforcement Learning\n",
    "\n",
    "Many topics covered in this class would deserve a more in-depth coverage, but we can already identify three challenges that make the RL problem intrinsically difficult:\n",
    "- Function approximation,\n",
    "- The exploration versus exploitation trade-off,\n",
    "- The improvement problem.\n",
    "\n",
    "As we have seen, **function approximation** is key in finding good policies. Although it needs not be tackled with stochastic gradient descent, the interplay between Deep Learning and Reinforcement Learning has triggered major advances in RL.\n",
    "\n",
    "The RLVS classes about [Deep Q-Networks and its variants](https://rl-vs.github.io/rlvs2021/dqn.html) dive deeper into how one can implement Approximate Value Iteration with Deep Neural Networks, and what really are the associated optimization problems. A particular focus is given on how [Regularized MDPs](https://rl-vs.github.io/rlvs2021/regularized-mdps.html) bring a new perspective to Approximate Dynamic Programming.\n",
    "\n",
    "Behavior policies are a cornerstone of RL: which action should one take to obtain informative samples? Should one explore uniformly the environment or rather follow a policy that takes the system towards promising states before exploring more agressively? This is called the **tradeoff between exploration and exploitation**.\n",
    "\n",
    "The RLVS classes about [Stochastic Bandits](https://rl-vs.github.io/rlvs2021/stochastic-bandits.html) algorithms, and [Monte Carlo Tree Search](https://rl-vs.github.io/rlvs2021/mcts.html) explore this aspect more in-depth. They are complemented by the [Exploration in Deep RL](https://rl-vs.github.io/rlvs2021/exploration.html) class on.\n",
    "\n",
    "The ideas we developped in this class relied on estimating value functions to deduce greedy policies. Finding such greedy policies was made easy because actions were discrete. But **finding improving policies** is actually a challenge in itself. This problem is present both when one searches for a greedy action with respect to $Q$, and when one directly aims at solving the $\\max_\\pi J(\\pi)$ problem without going through the proxy of the optimality equation.\n",
    "\n",
    "The RLVS classes cover the question of gradient-based policy search methods, called [Policy Gradient](https://rl-vs.github.io/rlvs2021/pg.html) methods.\n",
    "They also cover gradient-free [evolutionary RL](https://rl-vs.github.io/rlvs2021/evo-rl.html) methods and how [Evolving Agents that Learn More Like Animals](https://rl-vs.github.io/rlvs2021/evolving-agents.html) is an efficient way of solving the RL problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Specific questions and challenges in RL\n",
    "\n",
    "Beyond these three intrinsic challenges, there are countless, context-dependent, open questions in RL:\n",
    "- Can one define and exploit temporal abstractions (macro-actions) in Reinforcement Learning?  \n",
    "This is covered in the [Introduction to Hierarchical RL](https://rl-vs.github.io/rlvs2021/hierarchical.html) class.\n",
    "- What connection can we draw between human preferences and formal reward models for better RL?  \n",
    "This is the topic of the [Reward Processing Biases in Humans and RL Agents](https://rl-vs.github.io/rlvs2021/human-behavioral-agents.html) class.\n",
    "- How can one leverage prior knowledge about the system to control in order to learn faster or to obtain more meaningful policies? What is the interplay between learning and reasoning?    \n",
    "These is covered in the [Micro-data Policy Search](https://rl-vs.github.io/rlvs2021/micro-data.html), the [Symbolic Representations and Reinforcement Learning](https://rl-vs.github.io/rlvs2021/symbolic.html) and the [Leveraging model-learning for extreme generalization](https://rl-vs.github.io/rlvs2021/model-learning.html) classes.\n",
    "- What are the obstacles that arise when applying RL to real-world systems?  \n",
    "The classes on [Multi-armed bandits in clinical trials](https://rl-vs.github.io/rlvs2021/clinical.html) and [Efficient Motor Skills Learning in Robotics](https://rl-vs.github.io/rlvs2021/efficient-motor.html) illustrate them while the [RL tips and tricks](https://rl-vs.github.io/rlvs2021/tips-and-tricks.html) class covers the development and usage of a comprehensive RL library.\n",
    "\n",
    "And among the topics that RLVS does not cover but that deserve to be mentionned here, we can list:\n",
    "- Multi-agent RL\n",
    "- Partially observable MDPs\n",
    "- Robust RL\n",
    "- Offline RL\n",
    "- Consolidation and Transfer in RL\n",
    "- Causal RL\n",
    "- and many more !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<center><b>Welcome to the field of Reinforcement Learning!</b></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "186px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
