{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Class 0: Introduction to Reinforcement Learning**\n",
    "\n",
    "Contents of the class\n",
    "1. [The mad hatter's casino](#mad)\n",
    "2. [References](#refs)\n",
    "3. [Ruining the suspense with a general definition](#def)\n",
    "4. [Examples of RL problems](#examples)\n",
    "5. [Course syllabus](#syllabus)\n",
    "6. [Software](#gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"gpi\"></a> The mad hatter's casino\n",
    "\n",
    "Getting the main intuitions in 30 minutes by playing a fun game!\n",
    "<img src=\"img/madhatter.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Mad Hatter has invited you to play in his casino.  \n",
    "> It is a strange place. There are 4 rooms, with three slot machines in each.  \n",
    "> Whenever you pull the arm of a slot machine, you get a certain amount of tea and a tunnel opens that leads you to another room (possibly the same).\n",
    "> You can play for as long as you want.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Game on!**  \n",
    "    \n",
    "- What could possibly be the goal of the game? Can you express it mathematically?\n",
    "- If the goal is to accumulate tea, what's a good strategy?\n",
    "    \n",
    "Let's play for 5 minutes. If you get bored of following the game rules, you're allowed to cheat, change rooms at will, look at the game cards, etc. (but the questions above are posed for the non-cheating case).\n",
    "</div>\n",
    "\n",
    "In case we're not able to play this game together (it's a shame, it's a good laugh), you can emulate it with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.MadHatterCasino import MadHatterCasino\n",
    "import random\n",
    "casino = MadHatterCasino()\n",
    "room = casino.reset()  # default reset of the game, takes you to room 0\n",
    "room = casino.reset(2) # reset the game in room 2\n",
    "print(\"starting room:\", room)\n",
    "for i in range(3):\n",
    "    machine = random.randint(0,2)\n",
    "    room, tea = casino.step(machine) # pull the arm of slot machine 'machine' and be teleported to 'room' while drinking 'tea'\n",
    "    print(\"you pulled machine \", machine, \", reached room \", room, \", and drank \", tea, \" tea.\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now answer the questions above by discussing together. If you're playing this notebook on your own, here are a few discussion elements\n",
    "\n",
    "<div class=\"alert alert-danger\"><a href=\"#answers1\" data-toggle=\"collapse\"><b>What is the goal of the game?  Can you express it mathematically?</b></a><br>\n",
    "<div id=\"answers1\" class=\"collapse\">\n",
    "    \n",
    "Tricky question and many possible answers here.  \n",
    "It seems most people try to obtain as much tea as possible.\n",
    "    But it quickly appears that since the game never stops, the amount of tea you can get is just infinite.  \n",
    "    So maybe the goal is to get tea as quickly as possible?  \n",
    "    But in this case, what does \"quickly\" mean?\n",
    "    Again, most people seem to consider \"quickly\" in the sense of \"I don't know when the game will stop but there is a certain non-zero probability of termination at each step and I want to drink as much tea as possible before this happens\".  \n",
    "    Let's write $1-\\gamma$ this termination probability (and assume that it is constant over time steps). Then each time step is a two-step process: first check if the game has ended, then maybe get some tea and keep playing. The game ends with probability $1-\\gamma$ and we get $0$ tea thereafter. So the amount of tea we can expect to get after time step $t$ is $\\gamma$ times whatever the future steps will bring. Let's call $R_t$ the amount of tea we collect at time step $t$. Then, formally, the amount of tea we can expect to obtain after $T$ time steps is $\\mathbb{E}\\left( \\sum_{t=0}^T \\gamma^t R_t \\right)$.  \n",
    "    So maybe the goal of the game is to find a way to maximize $\\mathbb{E}\\left( \\sum_{t=0}^\\infty \\gamma^t R_t \\right)$ (the infinite horizon expected accumulated tea).  \n",
    "    Another frequent interpretation is to say \"I want to get the highest average inflow of tea possible over a certain horizon\".  \n",
    "    With the notation above and without termination probability, this average inflow is $\\mathbb{E}\\left( \\frac{1}{T} \\sum_{t=0}^\\infty R_t \\right)$.  \n",
    "    But again, we don't know when the game will end, so maybe the goal of the game is to maximize $\\lim_{T\\rightarrow \\infty} \\mathbb{E}(\\frac{1}{T} \\sum_{t=0}^\\infty R_t$.  \n",
    "    These two interpretations are the most common ones but many other are possible. For example you could wish to keep your tea intake as steady as possible. Or try to always have an odd cumulated amount of tea (you crazy fool!). Or you could try to get tea while avoiding a certain room (where you might believe a bear lives).  \n",
    "    Overall, the purpose of this open-ended question is to have a discussion about expressing and formalizing behavior objectives.  <br>\n",
    "    <br>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\"><a href=\"#answers2\" data-toggle=\"collapse\"><b>If the goal is to accumulate tea, what's a good strategy?</b></a><br>\n",
    "<div id=\"answers2\" class=\"collapse\">\n",
    "    \n",
    "Sorry, no answer here since it is precisely the goal of the class. However you're encouraged to play this game with your friends and fill the table below with your votes. Who believes machine $m$ should be picked in room $n$? \n",
    "    \n",
    "<img src=\"img/votes.png\"></img>\n",
    "\n",
    "Now suppose you normalize each column. Then you can read it as \"we believe the best way to play in room $n$ is to pick machine $m$ with probability $\\pi(n,m)$\". So you have expressed a strategy as a function.  \n",
    "Interestingly, this function has three important characteristics:\n",
    "- it is stationary: the best course of action changes based on the room you are in, but not the time step\n",
    "- it only depends on the current room, not on previous rooms and machines\n",
    "- it is a probability distribution over machines\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were playing this game, you followed a tought process that lead you to elaborate a reasonable strategy over several time steps.\n",
    "\n",
    "This thought process did not involve explicitly modeling the game to obtain a good strategy. Rather this strategy was adjusted using some instantaneous feedback (the amount of tea you obtain at each step). And this feedback was not representative of your long-term goal.\n",
    "\n",
    "Reinforcement Learning is the study of how this thought process can be formalized, analyzed and transformed into algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"refs\"></a>References\n",
    "\n",
    "Lots of excellent books and online resources. Here are a few freely available online.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/book_sutton2.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>Reinforcement Learning: an introduction (2nd edition)</b><br>Richard Sutton and Andrew Barto<br>2018.<br>The Reinforcement Learning bible. Both complete and didactical.<br><a href=\"http://incompleteideas.net/book/the-book.html\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"img/book_szepesvari.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>Algorithms for Reinforcement Learning</b><br>Csaba Szepesvari<br>2010.<br>The essentials in a hundred pages. A bit mathematical.<br><a href=\"https://sites.ualberta.ca/~szepesva/RLBook.html\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><img src=\"img/book_deeprl.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>An Introduction to Deep Reinforcement Learning</b><br>Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau<br>2019.<br>Deep Reinforcement Learning.<br><a href=\"https://arxiv.org/abs/1811.12560\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td><img src=\"img/web_silver.png\" style=\"width: 200px;\"></td>\n",
    "<td><b>David Silver's UCL course on RL</b><br>10 video lectures + presentation PDFs.<br>2015.<br><a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">Available here</a>.</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"def\"></a>Ruining the suspense with a general definition\n",
    "\n",
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about controlling dynamic systems.\n",
    "<img src=\"img/dynamic.png\" style=\"width: 400px;\"></img>\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"examples\"></a>Examples of RL problems\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Exiting a spiral</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td>Cart-pole balancing</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Portfolio management</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Airlines scheduling\n",
    "- Robocup soccer\n",
    "- Video game playing (Quake, CS, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"syllabus\"></a>Course syllabus\n",
    "\n",
    "This course is organized as separate and almost independent notebooks.\n",
    "- RL1 - Markov Decision Processes and model-based policy search\n",
    "- RL2 - Online Value Function Prediction\n",
    "- RL3 - Control Problems, model-free Policy Optimization\n",
    "- RL4 - Deep Reinforcement Learning\n",
    "- RL5 - Monte Carlo Tree Search\n",
    "\n",
    "The final evaluation is made on a challenge that will be revealed near the middle of the course, for which you will be asked to hand in your commented code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"gym\"></a> Software\n",
    "\n",
    "This class requires a recent version of Python 3 and scikit-learn (available in the <a href=\"https://www.anaconda.com/download\">Anaconda distribution</a>).\n",
    "\n",
    "You will need standard elements of Anaconda (numpy, matplotlib, scikit-learn, scikit-image) and graphviz.\n",
    "```sh\n",
    "conda install graphviz\n",
    "conda install python-graphviz\n",
    "```\n",
    "\n",
    "Although some environments will be provided as independent files, we will make use of the <a href=\"https://github.com/openai/gym\">OpenAi Gym</a> collection of Reinforcement Learning environments.\n",
    "\n",
    "In case this notebook becomes outdated, refer to the <a href=\"https://github.com/openai/gym\">installation instructions for Gym</a>. On Debian-based GNU/Linux distribution, this should do the trick:\n",
    "```\n",
    "sudo apt install -y g++ libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig\n",
    "pip install gym gym[atari] gym[classic_control] gym[box2d] gym[algorithms]\n",
    "```\n",
    "\n",
    "Test your installation (if the code below runs fine, you're sorted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extras/colab_gym_setup.sh\n",
    "# If you're running this notebook on Colab, uncomment the line above and run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extras/virtualdisplay_gym_setup.py\n",
    "# If you're running this notebook on Binder or Colab, uncomment the line above and run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should display a 4x4 grid of letters and open a window of the Breakout game.\n",
    "# Don't close the window yourself (it shouldn't work anyway)\n",
    "import gym\n",
    "env0 = gym.make('FrozenLake-v0')\n",
    "env0.render()\n",
    "env1 = gym.make('Breakout-v0')\n",
    "env1.render()\n",
    "env2 = gym.make('LunarLander-v2')\n",
    "env2.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should close the Breakout and LunarLander windows\n",
    "env1.close()\n",
    "env2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "397.717px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
