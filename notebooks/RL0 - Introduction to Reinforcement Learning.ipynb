{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/reinforcement-learning/\">https://supaerodatascience.github.io/reinforcement-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 0: Introduction to Reinforcement Learning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Contents of the class\n",
    "1. [The mad hatter's casino](#mad)\n",
    "2. [References](#refs)\n",
    "3. [Ruining the suspense with a general definition](#def)\n",
    "4. [Examples of RL problems](#examples)\n",
    "5. [RL in the taxonomy of Machine Learning](#taxonomy)\n",
    "6. [Key questions and keywords in RL](#keywords)\n",
    "7. [Software](#gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"gpi\"></a>The mad hatter's casino\n",
    "\n",
    "Getting the main intuitions in 30 minutes by playing a fun game!\n",
    "<img src=\"img/madhatter.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Mad Hatter has invited you to play in his casino.  \n",
    "> It is a strange place. There are 4 rooms, with three slot machines in each.  \n",
    "> Whenever you pull the arm of a slot machine, you get a certain amount of tea and a tunnel opens that leads you to another room (possibly the same).\n",
    "> You can play for as long as you want.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Game on!**  \n",
    "    \n",
    "- What could possibly be the goal of the game? Can you express it mathematically?\n",
    "- If the goal is to accumulate tea, what's a good strategy?\n",
    "    \n",
    "Let's play for 5 minutes. If you get bored of following the game rules, you're allowed to cheat, change rooms at will, look at the game cards, etc. (but the questions above are posed for the non-cheating case).\n",
    "</div>\n",
    "\n",
    "In case we're not able to play this game together (it's a shame, it's a good laugh), you can emulate it with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.MadHatterCasino import MadHatterCasino\n",
    "import random\n",
    "casino = MadHatterCasino()\n",
    "room = casino.reset()  # default reset of the game, takes you to room 0\n",
    "room = casino.reset(2) # reset the game in room 2\n",
    "print(\"starting room:\", room)\n",
    "for i in range(3):\n",
    "    machine = random.randint(0,2)\n",
    "    room, tea = casino.step(machine) # pull the arm of slot machine 'machine' and be teleported to 'room' while drinking 'tea'\n",
    "    print(\"you pulled the arm of machine \", machine, \", reached room \", room, \", and drank \", tea, \" tea.\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now answer the questions above by discussing together. If you're playing this notebook on your own, here are a few discussion elements\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>What is the goal of the game?  Can you express it mathematically?</b></summary>\n",
    "\n",
    "Tricky question and many possible answers here.  \n",
    "It seems most people try to obtain as much tea as possible.\n",
    "    But it quickly appears that since the game never stops, the amount of tea you can get is just infinite.  \n",
    "    So maybe the goal is to get tea as quickly as possible?  \n",
    "    But in this case, what does \"quickly\" mean?\n",
    "    Again, most people seem to consider \"quickly\" in the sense of \"I don't know when the game will stop but there is a certain non-zero probability of termination at each step and I want to drink as much tea as possible before this happens\".  \n",
    "    Let's write $1-\\gamma$ this termination probability (and assume that it is constant over time steps). Then each time step is a two-step process: first check if the game has ended, then maybe get some tea and keep playing. The game ends with probability $1-\\gamma$ and we get $0$ tea thereafter. So the amount of tea we can expect to get after time step $t$ is $\\gamma$ times whatever the future steps will bring. Let's call $R_t$ the amount of tea we collect at time step $t$. Then, formally, the amount of tea we can expect to obtain after $T$ time steps is $\\mathbb{E}\\left( \\sum_{t=0}^T \\gamma^t R_t \\right)$.  \n",
    "    So maybe the goal of the game is to find a way to maximize $\\mathbb{E}\\left( \\sum_{t=0}^\\infty \\gamma^t R_t \\right)$ (the infinite horizon expected accumulated tea).  \n",
    "    Another frequent interpretation is to say \"I want to get the highest average inflow of tea possible over a certain horizon\".  \n",
    "    With the notation above and without termination probability, this average inflow is $\\mathbb{E}\\left( \\frac{1}{T} \\sum_{t=0}^T R_t \\right)$.  \n",
    "    But again, we don't know when the game will end, so maybe the goal of the game is to maximize $\\lim_{T\\rightarrow \\infty} \\mathbb{E} \\left(\\frac{1}{T} \\sum_{t=0}^T R_t \\right)$.  \n",
    "    These two interpretations are the most common ones but many other are possible. For example you could wish to keep your tea intake as steady as possible. Or try to always have an odd cumulated amount of tea (you crazy fool!). Or you could try to get tea while avoiding a certain room (where you might believe a bear lives).  \n",
    "    Overall, the purpose of this open-ended question is to have a discussion about expressing and formalizing behavior objectives, i.e. \"what's your goal in this game?\" and \"what makes a strategy quantitatively better than another?\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>If the goal is to accumulate tea, what's a good strategy?</b></summary>\n",
    "\n",
    "Sorry, no answer here since it is precisely the goal of the class. However you're encouraged to play this game with your friends and fill the table below with your votes. Who believes machine $m$ should be picked in room $n$? \n",
    "    \n",
    "<img src=\"img/votes.png\"></img>\n",
    "\n",
    "Now suppose you normalize each column. Then you can read it as \"we believe the best way to play in room $n$ is to pick machine $m$ with probability $\\pi(n,m)$\". So you have expressed a strategy as a function.  \n",
    "Interestingly, this function has three important characteristics:\n",
    "- it is stationary: the best course of action changes based on the room you are in, but not the time step\n",
    "- it only depends on the current room, not on previous rooms and machines\n",
    "- it is a probability distribution over machines\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were playing this game, you followed a tought process that lead you to elaborate a reasonable strategy over several time steps.\n",
    "\n",
    "This thought process did not involve explicitly modeling the game to obtain a good strategy. Rather this strategy was adjusted using some instantaneous feedback (the amount of tea you obtain at each step). And this feedback was not representative of your long-term goal.\n",
    "\n",
    "Reinforcement Learning is the study of how this thought process can be formalized, analyzed and transformed into algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"refs\"></a>References\n",
    "\n",
    "Lots of excellent books and online resources. Here are a few freely available online.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/book_sutton2.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>Reinforcement Learning: an introduction (2nd edition)</b><br>Richard Sutton and Andrew Barto<br>2018.<br>The Reinforcement Learning bible. Both complete and didactical.<br><a href=\"http://incompleteideas.net/book/the-book.html\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"img/book_szepesvari.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>Algorithms for Reinforcement Learning</b><br>Csaba Szepesvari<br>2010.<br>The essentials in a hundred pages. A bit mathematical.<br><a href=\"https://sites.ualberta.ca/~szepesva/RLBook.html\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><img src=\"img/book_deeprl.jpg\" style=\"width: 200px;\"></td>\n",
    "<td><b>An Introduction to Deep Reinforcement Learning</b><br>Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau<br>2019.<br>Deep Reinforcement Learning.<br><a href=\"https://arxiv.org/abs/1811.12560\">PDF available online</a>.</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td><img src=\"img/web_silver.png\" style=\"width: 200px;\"></td>\n",
    "<td><b>David Silver's UCL course on RL</b><br>10 video lectures + presentation PDFs.<br>2015.<br><a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">Available here</a>.</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"def\"></a>Ruining the suspense with a general definition\n",
    "\n",
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about controlling dynamic systems.\n",
    "<img src=\"img/dynamic.png\" style=\"width: 400px;\"></img>\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"examples\"></a>Examples of RL problems\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Exiting a spiral</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td>Cart-pole balancing</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Portfolio management</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Airlines scheduling\n",
    "- Robocup soccer\n",
    "- Video game playing (Quake, CS, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"taxonomy\"></a>RL in the taxonomy of Machine Learning\n",
    "\n",
    "You may have had classes on Machine Learning before. There are three strongly distinct categories of problems in ML:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "Let's try to answer the following questions for each category.\n",
    "- What's the abstract problem we are trying to solve?\n",
    "- What's the data provided to the algorithms?\n",
    "- Give examples of algorithms in SL/UL/RL.  \n",
    "\n",
    "<center>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td> <b>Question</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Supervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Unsupervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Reinforcement</b> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $f(x)=y$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $x\\in X$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\pi(s)=a$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target (rephrased) </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Predict outputs given inputs</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Discover structure in data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Find an optimal behavior </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(x,y\\right)\\right\\}$ supervisor's labels </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{x\\right\\}$ unlabelled data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(s,a,r,s'\\right)\\right\\}$ experience samples </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Output </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Classifier or regressor</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Clusters or dimension reduction </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Policies, value functions </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Key algorithms </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Neural networks, SVMs, etc.</td>\n",
    "    <td style=\"border-left: 1px solid black\"> k-means, PCA, etc. </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Q-learning, Policy Gradients, etc. </td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "This table helps distinguish the different natures of the problems tackled. The RL problem is about finding the optimal policy for a given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"keywords\"></a>Key questions and keywords in RL\n",
    "\n",
    "The problem RL tries to solve is the *evaluation* and the *improvement* of the agent's behavior, based on experience samples:\n",
    "$$(s,a,r,s')$$\n",
    "\n",
    "One can distinguish two subproblems in RL:\n",
    "- **Value prediction**: what is this policy's value?<br>\n",
    "$\\rightarrow$ Useful for decision support applications: predict the cost of using the water in a hydro-electric reservoir, predict the expected gain of an investment policy.\n",
    "- **Policy optimization**: what is the best control policy for this system?<br>\n",
    "$\\rightarrow$ Useful for control applications: robotic actuator control, operations planning, elevator scheduling, agro-ecosystems management, etc.\n",
    "\n",
    "The core hypothesis of RL, is that the environment behaves as a Markov Decision Process, although we do not know an explicit model of this process.\n",
    "\n",
    "Finally, there are different contexts and challenges in RL that we can discuss via a few keywords:\n",
    "- curse of **dimensionnality**<br>\n",
    "The more state/actions variables, the larger the state space and the harder it is to efficiently optimize the policy.\n",
    "- **exploration/exploitation dilemma**<br>\n",
    "Suppose we have discovered some region of the state space yields great rewards. Should we rush towards that region or explore the unknown in search for even better rewards?\n",
    "- **model-based vs. model-free RL**<br>\n",
    "One straightforward approach to RL would be to collect samples $(s,a,r,s')$ and approximate the underlying MDP in order to use the algorithms seen above. Model-free RL tries to infer the optimal policy without this intermediate step. This type of RL is the one discussed in this class.\n",
    "- **online vs. interactive vs. non-interactive RL**<br>\n",
    "Online: we directly interact with the environment, as a player in a game.<br>\n",
    "Interactive: we control the interaction with the environment, in particular the choice of the current state or the possility of resets, as if we had a simulator that we can query.<br>\n",
    "Offline: somebody collected that $\\left\\{\\left(s,a,r,s'\\right)\\right\\}$ pile of data and left it here without a note. Can we still get something out of it?\n",
    "- **on-policy vs. off-policy learning**<br>\n",
    "Can we learn something about $\\pi$ while applying $\\pi'$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"gym\"></a> Software\n",
    "\n",
    "This class requires a recent version of Python 3 and scikit-learn (available in the <a href=\"https://www.anaconda.com/download\">Anaconda distribution</a>).\n",
    "\n",
    "You will need standard elements of Anaconda (numpy, matplotlib, scikit-learn, scikit-image) and graphviz.\n",
    "```sh\n",
    "conda install graphviz\n",
    "conda install python-graphviz\n",
    "```\n",
    "\n",
    "Although some environments will be provided as independent files, we will make use of the <a href=\"https://github.com/openai/gym\">OpenAi Gym</a> collection of Reinforcement Learning environments.\n",
    "\n",
    "In case this notebook becomes outdated, refer to the <a href=\"https://github.com/openai/gym\">installation instructions for Gym</a>. On Debian-based GNU/Linux distribution, this should do the trick:\n",
    "```\n",
    "sudo apt install -y g++ libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig\n",
    "pip install gym gym[atari] gym[classic_control] gym[box2d] gym[algorithms]\n",
    "```\n",
    "\n",
    "Test your installation (if the code below runs fine, you're sorted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extras/colab_gym_setup.sh\n",
    "# If you're running this notebook on Colab, uncomment the line above and run this cell.\n",
    "# solution borrowed from https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html\n",
    "\n",
    "%%bash\n",
    "\n",
    "# install required system dependencies\n",
    "apt-get install -y xvfb x11-utils\n",
    "\n",
    "# install required python dependencies (might need to install additional gym extras depending)\n",
    "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extras/virtualdisplay_gym_setup.py\n",
    "# If you're running this notebook on Binder, uncomment the line above and run this cell.\n",
    "# solution borrowed from https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                    size=(1400, 900))\n",
    "_display.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should display a 4x4 grid of letters and open a window of the Breakout game.\n",
    "# Don't close the window yourself (it shouldn't work anyway)\n",
    "import gym\n",
    "env0 = gym.make('FrozenLake-v0')\n",
    "env0.render()\n",
    "#env1 = gym.make('Breakout-v0')\n",
    "#env1.render()\n",
    "env2 = gym.make('LunarLander-v2')\n",
    "env2.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should close the Breakout and LunarLander windows\n",
    "#env1.close()\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "397.717px",
    "left": "10px",
    "top": "150px",
    "width": "301px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
