{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MCS5xukfKEeJ"
      },
      "source": [
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://softwareengineeringdaily.com/wp-content/uploads/2020/02/ray-logo.png\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "# Why Ray ? \n",
        "As you all know, Python is an interpreted language: An interpreter executes the lines of code one by one. \n",
        "As a result, a python program takes much longer to execute than a binary file compiled with C or C++. \n",
        "Ok, but for a lot of reasons, we don't want to use C++ ...\n",
        "What's left : \n",
        "- **Optimize your code**: \n",
        "\n",
        "it seems obvious and yet we often forget it. Of course it has its limits ...\n",
        "\n",
        "- **Multithreading**: \n",
        "\n",
        "Distribute your code in several tasks that all have access to the same memory. However, if you know well python, you know the limits imposed by the GIL (Global Lock Interpreter) :\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pbs.twimg.com/media/EZzAw78WAAE7d_D.jpg\" alt=\"drawing\" width=\"400\" />\n",
        "</p> \n",
        "\n",
        "\n",
        "- **Multiprocessing**: \n",
        "\n",
        "We're left with multiprocessing, but there are multiple ways to implement multiprocessing. You can use the python multiprocessing library. Also you will have to deal with the memory. In addition, when you want to scale efficiently on an HPC you will have to use a different framework like MPI. \n",
        "Also, You will probably have to rethink your machine learning application from the beginning.\n",
        "\n",
        "**The answer to your problems is Ray** : \n",
        "\n",
        "Ray offers an extremely simple api to distribute your code with minimal changes. In addition, Ray offers a whole bunch of libraries that allow machine learning applications while optimizing the use of resources to get the best models, as quickly as possible. \n",
        "Finally, one of the most interesting capabilities (to me) is that Ray scales very well on clusters.\n",
        "\n",
        "\n",
        "\n",
        "Today we will see the different librairies that ray offers but the focus will be on RLlib. \n",
        "**Part 1** of this tutorial presents very simply how ray core works. **The second** part gives an overview of the different machine learning libraries that ray offers and refers to introductory tutorials, the idea being just to draw your attention to the fact that these libraries exist.\n",
        "**The third** part is the heart of the tutorial, in this part we will see how to use RLlib a library of Ray that allows to do distributed RL.\n",
        "\n",
        "1. Introduction to Ray Core (25 min):\n",
        "  * init \n",
        "  * remote \n",
        "  * serialization\n",
        "  * ray on HPC (links)\n",
        "2. Ray librairies overview (10 min):\n",
        "  * Ray Data (links)\n",
        "  * Ray Train (links)\n",
        "  * Ray Tune (links)\n",
        "3. TP RLLIB (2h40): \n",
        "  * RLib overview \n",
        "  * First training : **CartPole-v0 with PPO** (1h)\n",
        "    * Hyper-parameters setting\n",
        "    * Ray Tune API\n",
        "    * Tensorboard or Weights and Biases : Framework to monitor your training\n",
        "    * Customize your training\n",
        "<p align=\"center\">\n",
        "<img src=\"https://bytepawn.com/images/cartpole.gif\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "  * Second training : **Pong with Rainbow** (1h40)\n",
        "    * Hyper-parameters setting\n",
        "    * Custom Environment\n",
        "    * Change your Model (Neural Network)\n",
        "    * Change the Loss (implementing Contrastive Loss)\n",
        "    * Quiz\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://blog.floydhub.com/content/images/2018/12/gif1.gif\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "  * Optional (Also i'm here to assist): \n",
        "    * Test other algorithms\n",
        "    * Create your own environment \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJnkR63eUryS"
      },
      "source": [
        "# Ray core\n",
        "Ray Core provides a small number of core primitives (i.e., tasks, actors, objects) for building and scaling distributed applications. Below weâ€™ll walk through simple examples that show you how to turn your functions and classes easily into Ray tasks and actors, and how to work with Ray objects.\n",
        "\n",
        "See more : [Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIqV3AsdWYXx"
      },
      "source": [
        "Fist let's install the library on your Colab sever :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dbZdL4q7fy"
      },
      "outputs": [],
      "source": [
        "!pip install ray > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8g2K8IPhDNl"
      },
      "outputs": [],
      "source": [
        "! ray --version "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RlMJcp2hUWy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "print(pickle.format_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek_5N7YhWl79"
      },
      "source": [
        "## Init\n",
        "Ray provides a really simple API that allows you to initialize the module and use all its parallelization capabilities :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrmTrgZ1qs-6"
      },
      "outputs": [],
      "source": [
        "import ray \n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fh6mTVYY5v"
      },
      "source": [
        "Now that the ray server has been initialized, you can check your available ressources : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw-a8p-XYYIv"
      },
      "outputs": [],
      "source": [
        "ressources = ray.available_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWm6AH2LYvoK"
      },
      "source": [
        "* Unless you already changed your runtime type, the default one has no GPU. If you want a GPU : you can change the runtime type in the settings (Also you need to rerun everything)\n",
        "* Also you can call the ray init with multiple args such as **num_cpus** or **nump_gpus** to specify explicitly the ressources you will be using. Important when working on shared machines. \n",
        "See all the [parameters](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-init) of ray.init.\n",
        "* If you call twice ray.init in the notebook it will crash. You can call **ray.shutdown** if you want to ... shutdown the ray server ! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mawZyiHFZppS"
      },
      "source": [
        "## Remote\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjRhCvdtUqG"
      },
      "source": [
        "Let's start with a simple example which make sense in an industrial context : let's create a function that counts up to n and check how long does it take to execute 2 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93f-zkm6bAVg"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "def usefull(n):\n",
        "  count=0\n",
        "  for _ in range(n):\n",
        "    count+=1\n",
        "  return count\n",
        "\n",
        "n=int(4e8)\n",
        "t0=time.time()\n",
        "[usefull(n) for k in range(2)]\n",
        "print(\"It takes {}s to count 2 times up to {} without ray\".format(time.time()-t0,n))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFfxZ4NTDq1J"
      },
      "source": [
        "Now with ray :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hERvaoyZDuj-"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "def usefull(n):\n",
        "  count=0\n",
        "  for _ in range(n):\n",
        "    count+=1\n",
        "  return count\n",
        "\n",
        "n=int(4e8)\n",
        "t0=time.time()\n",
        "ray.get([usefull.remote(n) for k in range(2)])\n",
        "print(\"It takes {}s to count 2 times up to {} with ray\".format(time.time()-t0,n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJTwiZR2EkBB"
      },
      "source": [
        "Ok it's faster. Unfortunately we can't see the clear advantages of using ray core because on Colab we have only 2 cores. But you get the idea.\n",
        "Also, it makes sense to use ray when the task at hand consumes more resources than the cost of setting up ray."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvc2w6wu2Ui6"
      },
      "source": [
        "## Serialization\n",
        "\n",
        "To better understand how Ray works you need to understand its [key concepts](https://docs.ray.io/en/latest/ray-core/key-concepts.html) \n",
        "Basically there are three concepts : \n",
        "\n",
        "* Tasks (which we just saw)\n",
        "* Actors (allowing to distribute classes and their methods)\n",
        "* Objects (allowing to distribute any object so that it can be called from any node)\n",
        "\n",
        "When ray is being used in a cluster mode, the informations needed to compute the task on the node are stored on the RAM of the node.\n",
        "Also some informations are not serializable making it impossible for ray to store.\n",
        "Let's see how to store object in the [Object store](https://docs.ray.io/en/releases-1.11.0/ray-core/memory-management.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoQGG60J21zT"
      },
      "outputs": [],
      "source": [
        "y=1\n",
        "y_obj=ray.put(y)\n",
        "print('obj ref : ', y_obj)\n",
        "print('obj value : ', ray.get(y_obj))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrpuMSwh2qEw"
      },
      "source": [
        "Ray uses [cloudpickle](https://github.com/cloudpipe/cloudpickle) with which you can serialize **almost** anything. \n",
        "Now let's say you to want to use ray to distribute the training of your agent on a custom environment which call a specific framework. \n",
        "You won't be able to serialize the environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHQCemdaLhYO"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "class Env:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.conn = sqlite3.connect(path) #this can't be serialized\n",
        "\n",
        "original = Env(\"/tmp/db\")\n",
        "ray_obj = ray.get(ray.put(original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iZIjbXMHkR"
      },
      "source": [
        "To enable the serialization, you need to provide ray with the serializable data needed to rebuilt the object on another node. This is done with __reduce__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AODt1DUUMeGT"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "class Env:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.conn = sqlite3.connect(path) #this can't be serialized\n",
        "\n",
        "    def __reduce__(self):\n",
        "        deserializer = Env\n",
        "        serialized_data = (self.path,)\n",
        "        return deserializer, serialized_data\n",
        "\n",
        "original = Env(\"/tmp/db\")\n",
        "ray_obj = ray.get(ray.put(original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvMQ8eGuAaYx"
      },
      "source": [
        "usefull tips : \n",
        "\n",
        "* **Ray actor definition** : You only need to decorate the class with ray.remote\n",
        "* **ray.remote args** : you can specify the ressources used by each actors with the args of the decorator\n",
        "* **Object memory management** : In the ray.init or ray.remote you can specify the capacity (or the capacity used) of the object store memory to limit the RAM usage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGe3hQiBZ1Tr"
      },
      "source": [
        "## Ray on HPC\n",
        "\n",
        "As you can see ray is really simple and it allows you distribute efficiently your code with no major changes. \n",
        "The most outstanding thing is that once you have designed your code to integrate Ray, there is almost nothing to do to run it on an HPC. So you can forget about abominations like MPI.\n",
        "Unfortunately we won't have time to run the example (This part is just to let you know that this feature exists).\n",
        "If you want to run basic example on Pando, here is all you need : \n",
        "\n",
        "Doc for running on [ AWS clursters ](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/index.html).\n",
        "\n",
        "Doc for running on [Slurm](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm.html) (Pando)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q1F0YW7Mtsu"
      },
      "source": [
        "# Ray librairies overview\n",
        "\n",
        "Unfortunately we won't have time to discover in depth all the librairies.\n",
        "Here is a brief summary of the capabilities of these librairies\n",
        "\n",
        "## Ray Data\n",
        "\n",
        "Ray Data is a library for building distributed data pipelines with Ray. It provides a high-level interface for defining and executing data processing tasks, as well as tools for managing the lifecycle of those tasks. \n",
        "Ray Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for GPU batch inference. They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
        "\n",
        "Usefull link : [Processing NYC taxi data using Ray Datasets](https://docs.ray.io/en/latest/data/examples/nyc_taxi_basic_processing.html)\n",
        "\n",
        "## Ray Train \n",
        "Ray Train scales model training for popular ML frameworks such as Torch, XGBoost, TensorFlow, and more. It seamlessly integrates with other Ray librairies such as Tune:\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/train-specific.svg\" alt=\"drawing\" width=\"600\" />\n",
        "</p> \n",
        "\n",
        "Here is the [Quick Start](https://docs.ray.io/en/latest/train/train.html#quick-start)\n",
        "\n",
        "## Ray Tune\n",
        "Ray Tune is a library that allows you to perform an optimal hyperparameter search for a given training. Indeed, not only this library allows to realize these evaluations in a distributed way but it also allows to improve this research with state-of-the-art methods such as bayesian optimization.\n",
        "Here is the [Quick start](https://docs.ray.io/en/latest/tune/getting-started.html#tune-tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Vt1ZFVltXI"
      },
      "source": [
        "# RLlib\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/rllib-logo.png\" alt=\"drawing\" width=\"300\" />\n",
        "</p> \n",
        "\n",
        "## RLlib overview\n",
        "\n",
        "RLlib Algorithm classes coordinate the distributed workflow of running rollouts and optimizing policies. Algorithm classes leverage parallel iterators to implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of these patterns:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/a2c-arch.svg\" alt=\"drawing\" width=\"600\" />\n",
        "</p> \n",
        "\n",
        "As it can be seen on the figure, RLlib uses multiple RolloutWorkers which are actually ray core Actors in order to maximize the number of sample collected. \n",
        "Once the ReplayBuffer filled, the trainer sample batches and train the model (learner). Once the model updated, the new weights are sent to the Rollout Workers. And it goes on ...\n",
        "\n",
        "That's actually what's going on underneath, Also there are multiple API levels which allow you to customize the workflow. We will start with the high level APIs and finish with low levels.\n",
        "\n",
        "## CartPole with PPO\n",
        "Let's install everything so that we can use [gym](https://www.gymlibrary.dev/) properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xNdyKC3uI5A"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install gym[classic_control] > /dev/null 2>&1\n",
        "!git clone https://github.com/Paul-antoineLeTolguenec/ray_course.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pe1yF0O2W_-"
      },
      "source": [
        "### Environment \n",
        "We can't render the envrionment in colab so for each rollout we will record a video and watch it afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "wtp-QWVGxFFX",
        "outputId": "8fb3da36-29bc-49b0-9c5d-8a54f9e72477"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import pygame\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from ray_course.gym_recorder import *\n",
        "gymlogger.set_level(40) #error only\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "\n",
        "env=wrap_env(gym.make('CartPole-v1')) #env\n",
        "\n",
        "# rollout\n",
        "s = env.reset()\n",
        "d=False\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    a = env.action_space.sample() \n",
        "    s, r, d, i = env.step(a) \n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vxw4KJq2eoj"
      },
      "source": [
        "### Hyper-parameter setting\n",
        "\n",
        "Now we are going to use the PPO (Proximal Policy Optimization).\n",
        "Usefull links :     \n",
        "* [PPO article](https://arxiv.org/abs/1707.06347)\n",
        "* [PPO summary](https://paperswithcode.com/method/ppo#:~:text=Proximal%20Policy%20Optimization%2C%20or%20PPO,using%20only%20first%2Dorder%20optimization.) \n",
        "* [PPO definition in RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ppo)\n",
        "Before going on that part it might be usefull to refresh your memory using the PPO summary.\n",
        "\n",
        "Now we are going to setup the configuration of the PPO algorithm using a simple dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK3oqsT72kDN"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'CartPole-v1',\n",
        "\t\t# \"env_config\": ENV_CONFIG, #the env config is the dictionary that's pass to the environment when built\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "    \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "    # PPO config\n",
        "    \"gamma\": 0.95,\n",
        "    \"use_critic\": True,\n",
        "    \"use_gae\": True, #Generalized Advantage Estimate\n",
        "    \"lambda\": 1,\n",
        "    \"kl_coeff\": 0.2,\n",
        "    \"rollout_fragment_length\":1024, #number of steps in the environment for each Rollout Worker\n",
        "    \"train_batch_size\": 1024, \n",
        "    \"sgd_minibatch_size\": 64,\n",
        "    \"shuffle_sequences\": True, #Kind of experience replay for PPO\n",
        "    \"num_sgd_iter\": 16,\n",
        "    \"lr\": 1e-3,\n",
        "    \"lr_schedule\": None,\n",
        "    \"vf_loss_coeff\": 1.0,\n",
        "    \"model\": {\n",
        "        \"vf_share_layers\": False, \n",
        "    },\n",
        "    \"entropy_coeff\": 0.0,\n",
        "    \"entropy_coeff_schedule\": None,\n",
        "    \"clip_param\": 0.4,\n",
        "    \"vf_clip_param\": 10.0,\n",
        "    \"grad_clip\": None,\n",
        "    \"observation_filter\": \"NoFilter\"\n",
        "\t}\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2frmuM_dNA0D"
      },
      "source": [
        "Questions : \n",
        "\n",
        "* What's the clip_param ? \n",
        "* How does it affect the training ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tip7JRKSTXND"
      },
      "source": [
        "**Usefull tips** : There used to be a common conf dictionary where you could access all the variables of the configuration (but since the new version, i'm not able to access it anymore): \n",
        "* [Here](https://chuacheowhuan.github.io/RLlib_trainer_config/) is the file i'm talking about.\n",
        "* Alternatively you can import the Algorithm config from RLlib and plot it as a dict : example below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6CALAgUTQ3y"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "algo = PPOConfig()\n",
        "algo.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXXOijwMTaI"
      },
      "source": [
        "Now that the config is defined, we are ready to train : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmWqwYsOMF7k"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo import PPO\n",
        "ray.shutdown() #shutdown before re-init\n",
        "ray.init() #re-init\n",
        "algo = PPO(config=CONFIG)\n",
        "for epoch in range(30):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxFA_T_0Mb1t"
      },
      "source": [
        "Once, you consider the training over, you can save the model so that it can be reused later : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHwR_FALMZ3V"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNXKgwtnzI4x"
      },
      "source": [
        "Let's see how to re-instanciate the model you trained :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYzpn4CFOnwL"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "algo = Algorithm.from_checkpoint(checkpoint_dir) #load the state of the algorithm where it was : Optimizer state, weights, ...\n",
        "policy=algo.get_policy() #get the policy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mqi17EtPFmx"
      },
      "source": [
        "Now, let's eval the model in the environment : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE5AM7_jLF-0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyvirtualdisplay import Display\n",
        "import gym\n",
        "# Eval\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "env = gym.make('CartPole-v1')\n",
        "episode_reward = 0\n",
        "d = False\n",
        "s = env.reset()\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    logits,_= policy.model({'obs': np.expand_dims(s,axis=0)})\n",
        "    a=np.argmax(logits)\n",
        "    s,r,d,i= env.step(a)\n",
        "    episode_reward += r\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adHOBXv7PUit"
      },
      "source": [
        "Note : As you can see, i used my own policy (argmax) on the logits. Alternatively, the policy has the method **compute_single_action**. However (for some really weird reasons) this method sample an action from the distribution instead of giving the optimal action. Also there is NO way (or maybe i didn't find it) to make it deterministic. (if you find a way please be kind and tell me)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yMg8aTjzRkM"
      },
      "source": [
        "### Ray Tune (RLlib API)\n",
        "When sing an algorithm to train your model, it might be usefull to test different combinations of hyper-parameters. The ray Tune API allow you this feature. The following example, show you how to lunch multiple training to test various learning rates with Tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXnocDwv042a"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray import air, tune\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "config = PPOConfig().training(lr= tune.grid_search([0.01, 0.001, 0.0001])).rollouts(num_rollout_workers=1).resources(num_gpus=0).environment(env=\"CartPole-v1\")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    \"PPO\",\n",
        "    run_config=air.RunConfig(\n",
        "        stop={\"episode_reward_mean\": 200},\n",
        "        local_dir=\"./results\", \n",
        "        name=\"PPO\"\n",
        "    ),\n",
        "    param_space=config,\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "# Get the best result based on a particular metric.\n",
        "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
        "\n",
        "# Get the best checkpoint corresponding to the best result.\n",
        "best_checkpoint = best_result.checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqW1y7zg2rkG"
      },
      "source": [
        "### TensorBoard or Weights and Biaises (Monitoring your algorithm)\n",
        "\n",
        "We just saw how to use the basic API of RLlib. However we didn't have much data during the training (which can be nice for monitoring). \n",
        "These informations are essential because they can allow you, for example, to stop a training when assymptotically we see that there is no learning.\n",
        "From now you have multiple solutions :\n",
        "\n",
        "* **Tensorboard** :\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.tensorflow.org/static/site-assets/images/project-logos/tensorboard-logo-social.png\" alt=\"drawing\" width=\"400\" />\n",
        "</p> \n",
        "\n",
        "TensorBoard stores the data localy on logfile and you can access the interface in your browser on the port you specified.\n",
        "It's well suited for RLlib because the library generates its own logfile\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzufCK2_AI27"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorboardx > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdpghdm8_f7p"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/results/PPO/NAME_OF_YOUR_EXPERIMENT_FOLDER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-JyBClpUW8Q"
      },
      "source": [
        "* **Weights and Biases** :\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://assets.website-files.com/5ac6b7f2924c656f2b13a88c/6077a58f02c7ef0e37fde627_weights%20and%20biases%20workspace.jpg\" alt=\"drawing\" width=\"400\" />\n",
        "\n",
        "Weights and Biases is a great tool to visualize data in real time (or near real time). \n",
        "The advantage over tensorboard is that the data is stored in the cloud.\n",
        "Steps to use weights and biases : \n",
        "1. Create an account : [here](https://wandb.ai/site)\n",
        "2.Copy your API key : [here](https://wandb.ai/home)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M7KehmLdJ4U"
      },
      "outputs": [],
      "source": [
        "!pip install wandb > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dLTzdd1dsps",
        "outputId": "671f7bcc-9701-4c9f-8420-ce9a3d1a9bca"
      },
      "outputs": [],
      "source": [
        "! wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HVfKXpNrCqN"
      },
      "source": [
        "Basic usage of wandb: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V0AqpnSkQ4-"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project='Test')\n",
        "wandb.run.name='run'\n",
        "for k in range(50):\n",
        "    wandb.log({'data': k})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB_WRr719UY8"
      },
      "source": [
        "To use wandb with Ray, it is necessary to implement a specific CallBack( functions that are called to produce the log) that call wandb.\n",
        "[Here](https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks) is the DefaultCallback. You can overide functions so that you output your own metrics. I propose an implementation that uses wandb in the ray_course.custom_callbacks. Feel free to modify it. In the below example we see how to specify the CustomCallback : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH6eZTT4fC_c"
      },
      "outputs": [],
      "source": [
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'CartPole-v1',\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "    \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "    'reuse_actors':True,\n",
        "    \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "    \"callbacks\": CustomCallbacks,\n",
        "    # PPO config\n",
        "    \"use_critic\": True,\n",
        "    \"use_gae\": True,\n",
        "    \"lambda\": 1,\n",
        "    \"kl_coeff\": 0.2,\n",
        "    \"rollout_fragment_length\":1024,\n",
        "    \"train_batch_size\": 1024,\n",
        "    \"sgd_minibatch_size\": 64,\n",
        "    \"shuffle_sequences\": True,\n",
        "    \"num_sgd_iter\": 16,\n",
        "    \"lr\": 1e-3,\n",
        "    \"lr_schedule\": None,\n",
        "    \"vf_loss_coeff\": 1.0,\n",
        "    \"model\": {\n",
        "        \"vf_share_layers\": False,\n",
        "    },\n",
        "    \"entropy_coeff\": 0.0,\n",
        "    \"entropy_coeff_schedule\": None,\n",
        "    \"clip_param\": 0.4,\n",
        "    \"vf_clip_param\": 10.0,\n",
        "    \"grad_clip\": None,\n",
        "    \"kl_target\": 0.01,\n",
        "    \"batch_mode\": \"truncate_episodes\",\n",
        "    \"observation_filter\": \"NoFilter\"\n",
        "\t}\n",
        "algo = PPO(config=CONFIG)\n",
        "for k in range(10):\n",
        "\tresult=algo.train()\n",
        "\tprint(pretty_print(result))\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83xYH7vBu1KX"
      },
      "source": [
        "**Task for you**\n",
        "\n",
        "\n",
        "Now that you have seen these tools. Try to optimize the training, play with the hyper-parmeters, customize your own metrics, ... \n",
        "Make the model learn to control the CartPole."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVBx0CdRSpTY"
      },
      "source": [
        "# Pong with **Rainbow**\n",
        "\n",
        "Now we're going to see another algorithm on another environment\n",
        " \n",
        "Fisrt let's install the atri suite for gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZxbBsfgVRmS"
      },
      "outputs": [],
      "source": [
        "!pip install \"gym[atari]\" \"gym[accept-rom-license]\" > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0xaKWKtqR9V"
      },
      "source": [
        "Normally you need to : **RESTART RUNTIME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uim6g5_Z3_U"
      },
      "source": [
        "Let's check our new environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-SoHF7OUgmW"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import ray\n",
        "from pyvirtualdisplay import Display\n",
        "import pygame\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from ray_course.gym_recorder import *\n",
        "gymlogger.set_level(40) #error only\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "env=wrap_env(gym.make('PongDeterministic-v0')\n",
        ") #env\n",
        "# rollout\n",
        "s = env.reset()\n",
        "print(s.shape)\n",
        "d=False\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    a = env.action_space.sample() \n",
        "    s, r, d, i = env.step(a) \n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCCKLg-xZDHC"
      },
      "source": [
        "Useful links for Rainbow algorithm: \n",
        "\n",
        "* [Rainbow article](https://arxiv.org/abs/1710.02298)\n",
        "* [Rainbow summary](https://yhyu13.github.io/2017/12/16/DeepMind-Rainbow-Review/)\n",
        "* [Rainbow on RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#dqn)\n",
        "\n",
        "\n",
        "Now let's see if we can learn a good policy with rainbow. Here is the new CONFIG and the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br0-54xVaRR8"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'PongDeterministic-v0',\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'num_gpus': 1,\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 10000,\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = DQN(config=CONFIG)\n",
        "now = datetime.now()\n",
        "name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "wandb.run.name='rainbow'+name\n",
        "for k in range(100):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pMMJqNzbYOh"
      },
      "source": [
        "## Custom Environment\n",
        "\n",
        "Cool! It works but as you can see the state is defined as the image at the given time step (just like when you play ! ). However, when learning on Atari it is more efficient to define the state of the agent as a stack of frame so that it is aware of the dynamic (like the speed vector of the ball). Otherwise it won't learn much.\n",
        "\n",
        "When you call RLlib with Atari environment, RLlib recognizes the Atari environment and prepocess it with [DeepMind Preprocessor](https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/atari_wrappers.py) so that the state become the stack of frames.\n",
        "\n",
        "Now we are going to see how to wrap an Atari environment so that the state is defined as stack of num_frames=3. Also, instead of having the state being an rgb images we will set it as a gray scale image. \n",
        "Summary : we had observation_space.shape=(1,height, width, C) C=3 for RGB channel, and now we have exactly the same space but the three channels are the 3 frames.\n",
        "\n",
        "Let see how to do that :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-B1bQnR8roC"
      },
      "outputs": [],
      "source": [
        "from numpy.core.memmap import uint8\n",
        "import numpy as np \n",
        "import gym \n",
        "import cv2\n",
        "import numpy as np\n",
        "from gym.spaces import Box\n",
        "import matplotlib.pyplot as plt \n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self,dict_env={}) -> None:\n",
        "        super(CustomEnv, self).__init__()\n",
        "        self.dict_env=dict_env\n",
        "        self.old_env= gym.make('PongDeterministic-v0')\n",
        "        self.num_frames=3\n",
        "        self.shape_new_image=(84, 84, 1)\n",
        "        self.observation_space=Box(0, 255, (self.shape_new_image[0], self.shape_new_image[1], self.shape_new_image[2]*3), dtype=uint8)\n",
        "        self.action_space=self.old_env.action_space\n",
        "        self.seq_s=[np.zeros(shape=self.shape_new_image) for _ in range(self.num_frames)]\n",
        "        \n",
        "    def reset(self):\n",
        "        # self.seq_s=[np.zeros(shape=self.shape_new_image) for _ in range(self.num_frames)] #reset sequence of states\n",
        "        old_env_s= self.old_env.reset() #state from the old environment\n",
        "        old_env_s=self.image_filter(np.array(old_env_s))\n",
        "        self.seq_s.pop(0) #delete older\n",
        "        self.seq_s.append(old_env_s)\n",
        "        s = np.concatenate(self.seq_s,axis=-1) \n",
        "        return s\n",
        "\n",
        "    def step(self, a):\n",
        "        old_env_s, r, d, i = self.old_env.step(a) \n",
        "        old_env_s=self.image_filter(np.array(old_env_s))\n",
        "        self.seq_s.pop(0)\n",
        "        self.seq_s.append(old_env_s)\n",
        "        s = np.concatenate(self.seq_s,axis=-1)\n",
        "        return s, r, d, i\n",
        "\n",
        "    def render(self,args):\n",
        "        self.old_env.render(args)\n",
        "    \n",
        "    def image_filter(self,img):\n",
        "        img_gs= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        resized = cv2.resize(img_gs, self.shape_new_image[:-1], interpolation = cv2.INTER_AREA)/255.0 #normalize the data\n",
        "        return np.expand_dims(resized,axis=-1)\n",
        "\n",
        "    def __reduce__(self):\n",
        "        deserializer = CustomEnv\n",
        "        serialized_data = (self.dict_env, )\n",
        "        return deserializer, serialized_data\n",
        "\n",
        "\n",
        "\n",
        "env=CustomEnv()\n",
        "id=ray.put(env) #check if it can be serialized\n",
        "print(env.observation_space)\n",
        "s=env.reset()\n",
        "print(s.shape)\n",
        "# plt.imshow(s)\n",
        "s, r, d, i = env.step(env.action_space.sample())\n",
        "print(s.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KB1-JervDeM"
      },
      "source": [
        "# Change your model\n",
        "\n",
        "Now that we have a new state let's also implement our own model.\n",
        "As we saw, ray has several API levels. The very high level with Tune and lower levels.\n",
        "When implementing a RL algo it is essential to change the structure of the model.\n",
        "To do this we will modify the ModelTF2 class (feel free to use the [pytorch class](https://docs.ray.io/en/latest/rllib/rllib-models.html#custom-pytorch-models))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTioOdekwLxm"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.misc import normc_initializer\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.visionnet import VisionNetwork as MyVisionNetwork\n",
        "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
        "from ray.rllib.utils.framework import try_import_tf\n",
        "from ray.rllib.utils.metrics.learner_info import LEARNER_INFO, LEARNER_STATS_KEY\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "tf1, tf, tfv = try_import_tf()\n",
        "class AtariModel(TFModelV2):\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name=\"atari_model\"):\n",
        "        super(AtariModel, self).__init__(obs_space, action_space, num_outputs, model_config,\n",
        "                         name)\n",
        "        inputs = tf.keras.layers.Input(shape=obs_space.shape, name='observations')\n",
        "        # Convolutions on the frames on the screen\n",
        "        layer1 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [8, 8],\n",
        "                strides=(4, 4),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(inputs)\n",
        "        layer2 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [4, 4],\n",
        "                strides=(2, 2),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(layer1)\n",
        "        layer3 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [3, 3],\n",
        "                strides=(1, 1),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(layer2)\n",
        "        layer4 = tf.keras.layers.Flatten()(layer3)\n",
        "        layer5 = tf.keras.layers.Dense(\n",
        "                512,\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=normc_initializer(1.0))(layer4)\n",
        "        action = tf.keras.layers.Dense(\n",
        "                num_outputs,\n",
        "                activation=\"linear\",\n",
        "                name=\"actions\",\n",
        "                kernel_initializer=normc_initializer(0.01))(layer5)\n",
        "        self.base_model = tf.keras.Model(inputs, action)\n",
        "        # self.register_variables(self.base_model.variables)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.base_model(input_dict[\"obs\"])\n",
        "        return model_out, state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgijl-M_9lpb"
      },
      "source": [
        "now you can register the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UggXMnIB9ksS"
      },
      "outputs": [],
      "source": [
        "ray.init()\n",
        "ModelCatalog.register_custom_model(\"AtariModel\", AtariModel)\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-DPZDRn97eY"
      },
      "source": [
        "Now let's use your own model with your CustomEnv in the CONFIG dict: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6BPlGbE-AMC"
      },
      "outputs": [],
      "source": [
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import ray \n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": CustomEnv,\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'num_gpus': 1,\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 1000,\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'custom_model': AtariModel,\n",
        "            'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = DQN(config=CONFIG)\n",
        "now = datetime.now()\n",
        "name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "wandb.run.name='rainbow_custom_model'+name\n",
        "for k in range(10):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpEspwE4EL-4"
      },
      "source": [
        "# Change the Loss (Implementing CURL)\n",
        "\n",
        "Ok so we've seen how to customize the hyper-parameters, how to pass a specific environment to the algorithm, and how to customize your own estimator. \n",
        "One important thing left is how to change the loss. Once, you've seen this last feature you will be able to implement almost all the DRL algorithms with RLlib.\n",
        "\n",
        "**CURL (Contrastive Unsupervised Representations for Reinforcement Learning)**\n",
        "\n",
        "Rainbow is great, but we can be more efficient. More specifically, we can be sample efficient. In order to make our algorithm more sample efficient we're going to use contrastive learning.\n",
        "\n",
        "**Contrastive Learning** :\n",
        "In contrastive learning, the main idea is to train a neural network to learn representations of data that are \"close\" for similar data points, and \"far\" for different data points. This is achieved by defining a distance metric between the representations of the input data in a feature space, and then training the network to minimize the distance between the representations of similar data points and maximize the distance between the representations of dissimilar data points. The distance between the representations is commonly measured by a contrastive loss function. This approach aims to learn useful and informative features from the input data.\n",
        "\n",
        "Useful links :\n",
        "\n",
        "* [Blog on Contrastive Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)\n",
        "* [Curl paper](https://arxiv.org/abs/2004.04136)\n",
        "* [Video summarizing Curl](https://www.youtube.com/watch?v=-Drowt9r4zY)\n",
        "\n",
        "For this we are going to use the Policy API which allows you to change the loss functions, the actions computations, ... see [Policy API](https://docs.ray.io/en/latest/rllib/package_ref/policy/policy.html)\n",
        "\n",
        "We will implement the algorithm in 4 steps : \n",
        "\n",
        "* Build the model for Curl (encoder q,k,...)\n",
        "* Implement the custom loss\n",
        "* Build the policy with the new loss \n",
        "* Modify the training step method of the algorithm to make the EMA (exponential moving average) update\n",
        "\n",
        "Again i will be using tensorflow ([here is the pytorch alternative](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#building-policies-in-tensorflow-eager)): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2vAm5dioux"
      },
      "source": [
        "1. First we need to change the model so that we have the key encoder and the query encoder. Also we can defined the function that will allow us to make the EMA update of the k encoder in the direction of the q encoder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Q4rBjGHl65"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.misc import normc_initializer\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.visionnet import VisionNetwork as MyVisionNetwork\n",
        "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
        "from ray.rllib.utils.framework import try_import_tf\n",
        "from ray.rllib.utils.metrics.learner_info import LEARNER_INFO, LEARNER_STATS_KEY\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "tf1, tf, tfv = try_import_tf()\n",
        "\n",
        "class AtariModel(TFModelV2):\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name=\"atari_model\"):\n",
        "        super(AtariModel, self).__init__(obs_space, action_space, num_outputs, model_config,\n",
        "                         name)\n",
        "        inputs = tf.keras.layers.Input(shape=obs_space.shape, name='observations')\n",
        "        self.beta=0.9\n",
        "        # Convolutions on the frames on the screen\n",
        "        # data aug layer k\n",
        "        # layer_aug_k_flip=tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n",
        "        # layer_aug_k=tf.keras.layers.RandomRotation(0.2)(layer_aug_k_flip)\n",
        "        #encoder keys\n",
        "        layer_k_1 = tf.keras.layers.Conv2D(32,[8, 8],strides=(4, 4),activation=\"relu\",data_format='channels_last',name='layer_k_1',trainable=False)(inputs)\n",
        "        layer_k_2 = tf.keras.layers.Conv2D(64,[4, 4],strides=(2, 2),activation=\"relu\",data_format='channels_last',name='layer_k_2',trainable=False)(layer_k_1)\n",
        "        layer_k_3 = tf.keras.layers.Conv2D(64,[3, 3],strides=(1, 1),activation=\"relu\",data_format='channels_last',name='layer_k_3',trainable=False)(layer_k_2)\n",
        "        flatten_k = tf.keras.layers.Flatten(name='layer_k_4',trainable=False)(layer_k_3)\n",
        "        encoder_k= tf.keras.layers.Dense(1024,activation=\"relu\",kernel_initializer=normc_initializer(1.0), name='layer_k_5')(flatten_k)\n",
        "        # data aug layer q\n",
        "        # layer_aug_q_flip=tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n",
        "        # layer_aug_q=tf.keras.layers.RandomRotation(0.2)(layer_aug_q_flip)\n",
        "        #encoder queries\n",
        "        layer_q_1 = tf.keras.layers.Conv2D(32,[8, 8],strides=(4, 4),activation=\"relu\",data_format='channels_last',name='layer_q_1')(inputs)\n",
        "        layer_q_2 = tf.keras.layers.Conv2D(64,[4, 4],strides=(2, 2),activation=\"relu\",data_format='channels_last',name='layer_q_2')(layer_q_1)\n",
        "        layer_q_3 = tf.keras.layers.Conv2D(64,[3, 3],strides=(1, 1),activation=\"relu\",data_format='channels_last',name='layer_q_3')(layer_q_2)\n",
        "        flatten_q = tf.keras.layers.Flatten(name='layer_q_4')(layer_q_3)\n",
        "        encoder_q = tf.keras.layers.Dense(1024,activation=\"relu\",kernel_initializer=normc_initializer(1.0), name='layer_q_5')(flatten_q)\n",
        "        # Project Matrix W\n",
        "        # W=tf.keras.layers.Dense(64, activation=None, use_bias=False)(encoder_k)\n",
        "        # Head layer\n",
        "        layer5 = tf.keras.layers.Dense(512,activation=\"relu\",kernel_initializer=normc_initializer(1.0))(encoder_q)\n",
        "        action = tf.keras.layers.Dense(num_outputs,activation=\"linear\",name=\"actions\",kernel_initializer=normc_initializer(0.01))(layer5)\n",
        "        self.base_model = tf.keras.Model(inputs, [action, encoder_q,encoder_k])\n",
        "\n",
        "        # self.register_variables(self.base_model.variables)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out, self._encoder_q_out, self._encoder_k_out= self.base_model(input_dict[\"obs\"])\n",
        "        return model_out, state\n",
        "    \n",
        "    # def projection_function(self):\n",
        "    #     return self._W\n",
        "    \n",
        "    def encoder_q_function(self):\n",
        "        return self._encoder_q_out\n",
        "\n",
        "    def encoder_k_function(self):\n",
        "        return self._encoder_k_out\n",
        "\n",
        "\n",
        "    def ema_update(self):\n",
        "        for k in range(1,5):\n",
        "            layer_k=self.base_model.get_layer('layer_k_'+str(k))\n",
        "            layer_q=self.base_model.get_layer('layer_q_'+str(k))\n",
        "            weights_k_l=layer_k.get_weights()\n",
        "            weights_q_l=layer_q.get_weights()\n",
        "            new_weights_k=[]\n",
        "            for i in range(len(weights_k_l)):\n",
        "                new_weights_k.append(weights_k_l[i]*float(self.beta)+(1-float(self.beta))*weights_q_l[i])\n",
        "            self.base_model.get_layer('layer_k_'+str(k)).set_weights(new_weights_k)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XCbESaUs6rC"
      },
      "source": [
        "2. Now let's modify the initial loss to add the contrastive loss in the loss_fn function. The image augmentation is defined just like in the article (random_crop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5ID6EUsL66E"
      },
      "outputs": [],
      "source": [
        "from torch import logit\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import build_q_losses\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CosineSimilarity\n",
        "\n",
        "\n",
        "def custom_loss(policy, model, _, train_batch):\n",
        "    # RAINBOW LOSS\n",
        "    dqn_loss= build_q_losses(policy, model, _, train_batch)\n",
        "\n",
        "    # OUR CONTRASTIVE LOSS\n",
        "    cosine_loss = tf.keras.losses.CosineSimilarity(axis=-1)\n",
        "    size_crop=20\n",
        "    input_batch=train_batch[SampleBatch.CUR_OBS]\n",
        "    # augmented view for query \n",
        "    i_x=np.random.randint(0,size_crop)\n",
        "    i_y=np.random.randint(0,size_crop)\n",
        "    cropped_s= tf.keras.layers.Cropping2D(cropping=((i_x, size_crop-i_x), (i_y, size_crop-i_y)))(input_batch)\n",
        "    augmented_q = tf.image.resize(cropped_s, (input_batch.shape[1], input_batch.shape[2]), method = tf.image.ResizeMethod.GAUSSIAN) #gausian interpolation\n",
        "    model_output, _ = model({'obs' : augmented_q})\n",
        "    # latent q\n",
        "    z_q=model.encoder_q_function()\n",
        "    # augmented view for key \n",
        "    i_x=np.random.randint(0,size_crop)\n",
        "    i_y=np.random.randint(0,size_crop)\n",
        "    cropped_s= tf.keras.layers.Cropping2D(cropping=((i_x, size_crop-i_x), (i_y, size_crop-i_y)))(input_batch)\n",
        "    augmented_k = tf.image.resize(cropped_s, (input_batch.shape[1], input_batch.shape[2]), method = tf.image.ResizeMethod.GAUSSIAN)\n",
        "    model_output, _ = model({'obs' : augmented_k})\n",
        "    # latent k\n",
        "    z_k=model.encoder_k_function()\n",
        "    # contrastive loss\n",
        "    pos_pairs_loss=cosine_loss(z_k,z_q)\n",
        "    neg_pairs_loss=0\n",
        "    for i in range(input_batch.shape[0]):\n",
        "        neg=tf.concat((z_k[:i],z_k[i+1:]),axis=0) #neg pairs\n",
        "        anchor=tf.repeat(tf.expand_dims(z_q[i],axis=0), repeats=neg.shape[0], axis=0)\n",
        "        neg_pairs_loss+=-cosine_loss(neg,anchor)\n",
        "    contrastive_loss=pos_pairs_loss+neg_pairs_loss\n",
        "    print('contrastive_loss : ',contrastive_loss)\n",
        "    print('dqn_loss : ',dqn_loss)\n",
        "    return dqn_loss + contrastive_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2VqTHcrzhBG"
      },
      "source": [
        "3. Build the policy. As you can see we can modify the initial policy with the **with_updates** function (really cool feature) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCQs2amfIwM_"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.policy.tf_policy_template import build_tf_policy\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import DQNTFPolicy\n",
        "\n",
        "CustomPolicy = DQNTFPolicy.with_updates(\n",
        "    name=\"CustomDQNPolicy\",\n",
        "    loss_fn=custom_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKXCBB1FWuM"
      },
      "source": [
        "4. As it can be seen in the article. After each update of to minimize the loss, there is an ema update of k toward q.\n",
        "We already implemented the ema_function. Now we need to call it in the algorithm flow. \n",
        "We are going to inherite from the DQN algorithm so that you see the structure of the **training_step** function that we are going to modify. \n",
        "Alternatively, we could have modify the **learn_on_batch** function of the policy.\n",
        "I just copy/pasted the **training_step** method of the dqn algorithm available [here](https://github.com/ray-project/ray/blob/ec3243d78726a2840f1323f997a210d1f33e5656/rllib/algorithms/dqn/dqn.py) and overided the function.\n",
        "The modification can be seen at the *******custom update***** comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1GCK-mwFVAk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "from typing import List, Optional, Type, Callable\n",
        "import numpy as np\n",
        "\n",
        "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import DQNTFPolicy\n",
        "from ray.rllib.algorithms.dqn.dqn_torch_policy import DQNTorchPolicy\n",
        "from ray.rllib.algorithms.simple_q.simple_q import (\n",
        "    SimpleQ,\n",
        "    SimpleQConfig,\n",
        ")\n",
        "from ray.rllib.execution.rollout_ops import (\n",
        "    synchronous_parallel_sample,\n",
        ")\n",
        "from ray.rllib.policy.sample_batch import MultiAgentBatch\n",
        "from ray.rllib.execution.train_ops import (\n",
        "    train_one_step,\n",
        "    multi_gpu_train_one_step,\n",
        ")\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
        "from ray.rllib.utils.typing import ResultDict\n",
        "from ray.rllib.utils.metrics import (\n",
        "    NUM_ENV_STEPS_SAMPLED,\n",
        "    NUM_AGENT_STEPS_SAMPLED,\n",
        ")\n",
        "from ray.rllib.utils.deprecation import (\n",
        "    Deprecated,\n",
        ")\n",
        "from ray.rllib.utils.metrics import SYNCH_WORKER_WEIGHTS_TIMER\n",
        "from ray.rllib.execution.common import (\n",
        "    LAST_TARGET_UPDATE_TS,\n",
        "    NUM_TARGET_UPDATES,\n",
        ")\n",
        "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
        "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "\n",
        "def calculate_rr_weights(config: AlgorithmConfig) -> List[float]:\n",
        "    \"\"\"Calculate the round robin weights for the rollout and train steps\"\"\"\n",
        "    if not config[\"training_intensity\"]:\n",
        "        return [1, 1]\n",
        "\n",
        "    # Calculate the \"native ratio\" as:\n",
        "    # [train-batch-size] / [size of env-rolled-out sampled data]\n",
        "    # This is to set freshly rollout-collected data in relation to\n",
        "    # the data we pull from the replay buffer (which also contains old\n",
        "    # samples).\n",
        "    native_ratio = config[\"train_batch_size\"] / (\n",
        "        config.get_rollout_fragment_length()\n",
        "        * config[\"num_envs_per_worker\"]\n",
        "        # Add one to workers because the local\n",
        "        # worker usually collects experiences as well, and we avoid division by zero.\n",
        "        * max(config[\"num_workers\"] + 1, 1)\n",
        "    )\n",
        "\n",
        "    # Training intensity is specified in terms of\n",
        "    # (steps_replayed / steps_sampled), so adjust for the native ratio.\n",
        "    sample_and_train_weight = config[\"training_intensity\"] / native_ratio\n",
        "    if sample_and_train_weight < 1:\n",
        "        return [int(np.round(1 / sample_and_train_weight)), 1]\n",
        "    else:\n",
        "        return [1, int(np.round(sample_and_train_weight))]\n",
        "\n",
        "class Curl(DQN):\n",
        "    def __init__(self, config):\n",
        "        super(DQN, self).__init__(config)\n",
        "    @override(DQN)\n",
        "    def training_step(self) -> ResultDict:\n",
        "        \"\"\"DQN training iteration function.\n",
        "        Each training iteration, we:\n",
        "        - Sample (MultiAgentBatch) from workers.\n",
        "        - Store new samples in replay buffer.\n",
        "        - Sample training batch (MultiAgentBatch) from replay buffer.\n",
        "        - Learn on training batch.\n",
        "        - Update remote workers' new policy weights.\n",
        "        - Update target network every `target_network_update_freq` sample steps.\n",
        "        - Return all collected metrics for the iteration.\n",
        "        Returns:\n",
        "            The results dict from executing the training iteration.\n",
        "        \"\"\"\n",
        "        train_results = {}\n",
        "\n",
        "        # We alternate between storing new samples and sampling and training\n",
        "        store_weight, sample_and_train_weight = calculate_rr_weights(self.config)\n",
        "\n",
        "        for _ in range(store_weight):\n",
        "            # Sample (MultiAgentBatch) from workers.\n",
        "            new_sample_batch = synchronous_parallel_sample(\n",
        "                worker_set=self.workers, concat=True\n",
        "            )\n",
        "\n",
        "            # Update counters\n",
        "            self._counters[NUM_AGENT_STEPS_SAMPLED] += new_sample_batch.agent_steps()\n",
        "            self._counters[NUM_ENV_STEPS_SAMPLED] += new_sample_batch.env_steps()\n",
        "\n",
        "            # Store new samples in replay buffer.\n",
        "            self.local_replay_buffer.add(new_sample_batch)\n",
        "\n",
        "        global_vars = {\n",
        "            \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
        "        }\n",
        "\n",
        "        # Update target network every `target_network_update_freq` sample steps.\n",
        "        cur_ts = self._counters[\n",
        "            NUM_AGENT_STEPS_SAMPLED\n",
        "            if self.config.count_steps_by == \"agent_steps\"\n",
        "            else NUM_ENV_STEPS_SAMPLED\n",
        "        ]\n",
        "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
        "            for _ in range(sample_and_train_weight):\n",
        "                # Sample training batch (MultiAgentBatch) from replay buffer.\n",
        "                train_batch = sample_min_n_steps_from_buffer(\n",
        "                    self.local_replay_buffer,\n",
        "                    self.config.train_batch_size,\n",
        "                    count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
        "                )\n",
        "\n",
        "                # Postprocess batch before we learn on it\n",
        "                post_fn = self.config.get(\"before_learn_on_batch\") or (lambda b, *a: b)\n",
        "                train_batch = post_fn(train_batch, self.workers, self.config)\n",
        "\n",
        "                # for policy_id, sample_batch in train_batch.policy_batches.items():\n",
        "                #     print(len(sample_batch[\"obs\"]))\n",
        "                #     print(sample_batch.count)\n",
        "\n",
        "                # Learn on training batch.\n",
        "                # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
        "                # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
        "                if self.config.get(\"simple_optimizer\") is True:\n",
        "                    train_results = train_one_step(self, train_batch)\n",
        "                else:\n",
        "                    train_results = multi_gpu_train_one_step(self, train_batch)\n",
        "\n",
        "                # Update replay buffer priorities.\n",
        "                update_priorities_in_replay_buffer(\n",
        "                    self.local_replay_buffer,\n",
        "                    self.config,\n",
        "                    train_batch,\n",
        "                    train_results,\n",
        "                )\n",
        "                #*****************************************custom update*****************************************\n",
        "                for pid in self.workers.local_worker().policy_map.keys():\n",
        "                    self.workers.local_worker().policy_map[pid].model.ema_update()\n",
        "                #*****************************************end of the custom update*****************************************\n",
        "                last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
        "                if cur_ts - last_update >= self.config.target_network_update_freq:\n",
        "                    to_update = self.workers.local_worker().get_policies_to_train()\n",
        "                    self.workers.local_worker().foreach_policy_to_train(\n",
        "                        lambda p, pid: pid in to_update and p.update_target()\n",
        "                    )\n",
        "                    self._counters[NUM_TARGET_UPDATES] += 1\n",
        "                    self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
        "                # print(\"Worker dict : \",self.workers.local_worker().policy_dict())\n",
        "                # Update weights and global_vars - after learning on the local worker -\n",
        "                # on all remote workers.\n",
        "                with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
        "                    self.workers.sync_weights(global_vars=global_vars)\n",
        "\n",
        "        # Return all collected metrics for the iteration.\n",
        "        return train_results\n",
        "\n",
        "    def get_default_policy_class(self, config):\n",
        "        return CustomPolicy\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UXiTqsZHtsw"
      },
      "source": [
        "Now let's train Curl !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrKPTS9AHvrK"
      },
      "outputs": [],
      "source": [
        "import ray \n",
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": CustomEnv,\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        # \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 500, #10000\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'custom_model': AtariModel,\n",
        "            'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = Curl(config=CONFIG)\n",
        "# now = datetime.now()\n",
        "# name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "# wandb.run.name='rainbow_custom_model'+name\n",
        "for k in range(2):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUEbk8MIepXT"
      },
      "source": [
        "**Questions** :\n",
        "\n",
        "There is an hyper-parameter really important in order for curl to work, can you guess which one it is ? \n",
        "\n",
        "We made an error in our implementation, can you guess what it is ? (indication : it's in the loss definition)\n",
        "\n",
        "Modify the implementation so that it is exactly Curl\n",
        "\n",
        "\n",
        "Try to use the **learn_on_batch** function to make the ema update : [here's where you begin](https://github.com/ray-project/ray/blob/ec3243d78726a2840f1323f997a210d1f33e5656/rllib/policy/tf_policy.py)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
