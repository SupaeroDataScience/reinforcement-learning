{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961d16e8-4429-4049-9851-9efd7b92c214",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 4: Policy Gradient and Actor-Critic Algorithms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdcb57",
   "metadata": {},
   "source": [
    "1. <a href=\"#policies\">Learning parametric policies</a>\n",
    "2. <a href=\"#PG\">Policy gradient methods</a>\n",
    "3. <a href=\"#objective\">Objective function</a>\n",
    "4. <a href=\"#theorem\">The Policy Gradient theorem</a>\n",
    "6. <a href=\"#ac\">Actor-Critic methods</a>\n",
    "7. <a href=\"#pi\">The link with Policy Iteration</a>\n",
    "8. <a href=\"#deep\">Deep Actor-Critic algorithms and references</a>\n",
    "9. <a href=\"#practice\">Practice</a>\n",
    "    1. <a href=\"#why\">Why use Actor-Critic algorithms?</a>\n",
    "    2. <a href=\"#environments\">Environments</a>\n",
    "    3. <a href=\"#ddpg\">Deep Deterministic policy gradient</a>\n",
    "    4. <a href=\"#sac\">Soft-Actor Critic</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7d87-91a4-4f0c-b545-542b943bf014",
   "metadata": {},
   "source": [
    "# <div id=\"policies\"></div> Learning parametric policies\n",
    "\n",
    "**Bottomline question:**<br>\n",
    "The previous classes have focussed on *action-value methods*; they aimed at estimating $Q^*$ in order to deduce $\\pi^*$. Could we directly optimize $\\pi$?\n",
    "\n",
    "Suppose we have a policy $\\pi_\\theta$ parameterized by a vector $\\theta$. Our goal is to find the parameter $\\theta^*$ corresponding to $\\pi^*$.\n",
    "\n",
    "Remarks:\n",
    "- $\\pi_\\theta$ might not be able to represent $\\pi^*$. We will take a shortcut and call $\\pi^*$ the best policy among the $\\pi_\\theta$ ones.\n",
    "- for discrete state and action space, the tabular policy representation is a special case of policy parameterization.\n",
    "- policy parameterization is a (possibly useful) way of introducing prior knowledge on the set of the desired policies.\n",
    "- the optimal deterministic policies might not belong to the policy subspace of $\\pi_\\theta$, thus it makes sense to consider stochastic policies for $\\pi_\\theta$.\n",
    "- for problems with significant policy approximation, the best approximate policy (among $\\pi_\\theta$ ones) may very well be stochastic.\n",
    "- it makes even more sense to consider stochastic policies that it opens the family of environments that we can tackle, like partially observable MDPs or multi-player games.\n",
    "\n",
    "For stochastic policies, we shall write $\\pi_\\theta(a|s)$.\n",
    "\n",
    "In the remainder of the class, we will assume that $\\pi_\\theta$ is differentiable with respect to $\\theta$.\n",
    "\n",
    "To directly optimize $\\theta$ we need a criterion $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6b408-92f5-4f04-b096-35968958fbae",
   "metadata": {},
   "source": [
    "# <div id=\"PG\"></div>Policy gradient methods\n",
    "\n",
    "Suppose now we define some performance metric $J(\\pi_\\theta) = J(\\theta)$. If $J$ is differentiable and a stochastic estimate $\\nabla_\\theta J(\\theta)$ of the gradient is available, then we can define the gradient ascent update procedure:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta).$$\n",
    "\n",
    "We will call **policy gradient methods** all methods that follow such a procedure (whether or not they also learn a value function or not).\n",
    "\n",
    "Remarks: \n",
    "- Note that $J$ is a more general criterion that might differ from $Q$ in the definition above (even though it seems reasonable to assume both should be related). For example, $J$ could be defined as the value of a starting state (or a distribution of starting states) in episodic cases, or as the undiscounted reward over a certain horizon, or as the average reward.\n",
    "- Why is it interesting to look at policy gradient methods? Because for continuous actions there is no maximization step ($\\max_a Q(s,a)$) during evaluation but only a call to $\\pi_\\theta(s)$ (or a draw from $\\pi_\\theta(a|s)$). This makes Policy Gradient a method of choice for continuous actions domains (especially common in Robotics).\n",
    "- When do policy gradient approaches outperform value-based ones? It's hard to give a precise criterion; it really depends on the problem. One thing that comes into play is how easy it is to approximate the optimal policy or the optimal value function. If one is simpler than the other (by \"simpler\", we mean \"it is easier to find a parameterization whose spanned function space almost includes the function to approximate\"), then it is a good heuristic to try to approximate it. But this criterion might itself be hard to assess.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9b968-130a-4046-adfb-922b657af06c",
   "metadata": {},
   "source": [
    "# Notations\n",
    "\n",
    "- We consider probability density functions $p(X)$ for all random variables $X$.\n",
    "- For a policy $\\pi_\\theta$ and a random variable $X$ we write indifferently $p(X|\\pi_\\theta) = p(X|\\theta)$.\n",
    "- A trajectory is noted $\\tau = (s_t,a_t)_{t\\in [0,\\infty]}$.\n",
    "- The state random variable at step $t$ is $S_t$ and its law's density is $p_t(s)$.\n",
    "- The action random variable at step $t$ is $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92181633-74e0-4224-9733-359678412101",
   "metadata": {},
   "source": [
    "# <div id=\"objective\"></div> Rewriting the policy optimization's objective function.\n",
    "\n",
    "Although it is not strictly necessary for the following sections, let us play a bit with the policy optimization's objective function.\n",
    "\n",
    "We defined the policy optimization's objective as:  \n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim p_0} \\left[ V^{\\pi} (s) \\right].$$\n",
    "Or equivalently:  \n",
    "$$J(\\pi) = \\mathbb{E}_{(s_i,a_i)_{i \\in [0,\\infty]}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi \\right].$$\n",
    "We can switch the sum and the expectation and get:  \n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{(s_i,a_i)_{i \\in [0,\\infty]}} \\left[ r(s_t,a_t)  | \\pi \\right]$$\n",
    "But $\\mathbb{E}_{(s_i,a_i)_{i \\in [0,\\infty]}} \\left[ r(s_t,a_t)  | \\pi \\right] = \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right]$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right].$$\n",
    "Now let's introduce the density of $(s_t,a_t)$:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s_t,a_t) p(s_t,a_t|\\pi) ds_t da_t.$$\n",
    "But $p(s_t,a_t|\\pi) = p(s_t|\\pi) p(a_t|s_t,\\pi)$. By definition, $p(s_t|\\pi) = p_t(s|\\pi)$ and $p(a_t=a|s_t=s,\\pi) = \\pi(a|s)$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s,a) p_t(s|\\pi) \\pi(a|s) ds da.$$\n",
    "Let us isolate the terms that concern only states:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi(a|s) da \\right] \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi) ds.$$\n",
    "Let's note $\\rho^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi)$. We will call this quantity the density of the *improper state distribution under policy $\\pi$*. Then we have:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi (a|s) da \\right] \\rho^\\pi(s) ds.$$\n",
    "And so finally:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ r(s,a) \\right].$$\n",
    "\n",
    "In plain words, the value of a policy $\\pi$ is the average value of the rewards when states are sampled according to $\\rho^\\pi$ and actions are sampled according to $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e75e9-5387-489f-a39e-23cfb200e291",
   "metadata": {},
   "source": [
    "# <div id=\"theorem\"></div>The Policy Gradient theorem\n",
    "\n",
    "The crucial problem of computing $\\nabla_\\theta J(\\theta)$ lies in the fact that when $\\theta$ changes, both $\\pi$ and $\\rho^\\pi$ change in turn. So there seems to be no straighforward way of evaluating this gradient. One could fall back on a *finite differences* approach to estimating this gradient, but this would require trying out a series of increments $\\Delta \\theta$ which quickly becomes impractical (because the increment size is hard to tune, especially in stochastic systems, and also because of the sample inefficiency of the approach).\n",
    "\n",
    "The key result of this class is that one can express the gradient of $J(\\theta)$ as directly proportional to the value of $Q^\\pi$ and the gradient of $\\pi$:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "\n",
    "The proof of this result is simple but a bit tedious. We can however give the general intuition. Let's consider trajectories $\\tau = (s_0,a_0,r_0,...)$ drawn according to $\\pi$ from the starting state. Each of these trajectories has an overall payoff of $G(\\tau) = \\sum_t \\gamma^t r_t$ and is drawn with probability density $p(\\tau|\\theta)$. Then the objective function can be written:\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\mathbb{E}_\\tau \\left[ G(\\tau) | \\theta \\right]\\\\\n",
    " &= \\int G(\\tau) p(\\tau | \\theta) d\\tau\n",
    "\\end{align}\n",
    "\n",
    "So the objective function's gradient is:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\int G(\\tau) p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) \\nabla_\\theta p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) p(\\tau|\\theta) \\frac{\\nabla_\\theta p(\\tau|\\theta)}{p(\\tau|\\theta)} d\\tau,\\\\\n",
    " &= \\mathbb{E}_\\tau \\left[ G(\\tau) \\nabla_\\theta \\log p(\\tau|\\theta) \\right].\n",
    "\\end{align}\n",
    "\n",
    "Let us study a little the $\\nabla_\\theta \\log p(\\tau|\\theta)$ term along a series of remarks.\n",
    "\n",
    "**Remark 1**: law of $s_{t+1},a_{t+1}$ given the policy and history.  \n",
    "One has $p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta)$.  \n",
    "But the transition model is Markovian, so $p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t)$.  \n",
    "And the law of $a_{t+1}$ is given by the policy, so $p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta) = \\pi_\\theta(a_{t+1}|s_{t+1})$.  \n",
    "Consequently:\n",
    "$$p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1}).$$\n",
    "\n",
    "**Remark 2**: probability density of a trajectory.  \n",
    "Recall that $p(\\tau|\\theta) = p((s_t,a_t)_{t\\in [0,\\infty]}|\\theta)$.  \n",
    "This joint probability can be decomposed into conditional probabilities:  \n",
    "$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta)$.\n",
    "By the previous remarks allows us to simplify to:\n",
    "$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1})$.\n",
    "By expanding the first term into $p(s_0)\\pi_\\theta(a_0|s_0)$ and reordering the terms inside the product, we obtain:\n",
    "$$p(\\tau|\\theta) = p(s_0) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_t|s_t).$$\n",
    "\n",
    "**Remark 3**: the grad-log-prob trick.  \n",
    "Now let us consider the full $\\nabla_\\theta \\log p(\\tau|\\theta)$ term. The previous remarks tells us that  \n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\nabla_\\theta \\log p(s_0) + \\sum_{t=0}^\\infty \\left[ \\nabla_\\theta \\log p(s_{t+1} | s_t, a_t) + \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right].$$\n",
    "But the initial state distribution and the transition model do not depend on $\\theta$, so this expression boils down to:\n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t).$$\n",
    "\n",
    "And we will admit the step which leads to:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right].$$\n",
    "\n",
    "And finally, since $Q^\\pi(s,a) = \\mathbb{E} [G(\\tau) | S_0=s, A_0=a, \\theta]$, we obtain that:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right].$$\n",
    "\n",
    "We can try to interpret intuitively the components of the equation:\n",
    "- The $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ term \"selects\" the part of the network that represents a particular action we took in a particular state. Conceptually, this corresponds to the part of the network returning a vector with positive values for neurons that positively influenced the network's output $\\pi_{\\theta}(a_t|s_t)$, and negative values for neurons that negatively influenced the network's output.\n",
    "- $G(\\tau)$ is a scalar that represents how much we want to want to encourage or discourage this type of action. Multiplying $G(\\tau)$ by the gradient of the log probability of the action we took in the state we were in produces a vector that represents an update in a direction that would have increased the network's reward when making the choice $\\pi_{\\theta}(a_t|s_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb32db2-c58e-4b84-b0e6-7302097a6c83",
   "metadata": {},
   "source": [
    "# <div id=\"ac\"></div>Actor-Critic methods\n",
    "\n",
    "Suppose now that we don't want a Monte Carlo estimate of $Q^\\pi(s,a)$ in the Policy Gradient theorem, and are rather willing to store a function approximator for $Q^\\pi(s,a)$. This leads us to store both a policy and a value function. The value function *criticizes* the policy's selected actions, hence the names of *critic* and *actor*.\n",
    "\n",
    "Remark that the temporal difference at each time step $\\delta = r + \\gamma V^\\pi(s') - V^\\pi(s)$ is an estimate of the advantage $A^\\pi(s,a)$. Using this remark, a simple one-step Actor-Critic method based on TD(0) and a value function $V_w$ goes as follows:\n",
    "1. In $s$, draw $a \\sim \\pi$\n",
    "2. Observe $r, s'$\n",
    "3. Compute $\\delta = r + \\gamma V_w(s') - V_w(s)$\n",
    "4. Update critic's parameters (TD(0) step) $w \\leftarrow w + \\alpha \\delta \\nabla_w V_w(s)$\n",
    "5. Update actor's parameters (policy gradient theorem) $\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta \\log \\pi(a|s)$\n",
    "6. $s\\leftarrow s'$ and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b10e13-5634-4256-863e-24bcde4c07f3",
   "metadata": {},
   "source": [
    "# <div id=\"pi\"></div>The link with Policy Iteration\n",
    "\n",
    "Let's take a step back and reconsider the Actor-Critic architecture we have just introduced.\n",
    "\n",
    "Basically, on the one hand, we have a critic $V_w$ or $Q_w$ that aims at estimating the $V^\\pi$ or $Q^\\pi$ value function of policy $\\pi$. And on the other hand, we have an actor whose policy $\\pi_\\theta$ is incrementally improved so as to maximize $J(\\theta)$. This should sound familiar.\n",
    "\n",
    "In part, this is familiar because this resembles a lot the SARSA update. Let's take a minute to spot the differences.\n",
    "\n",
    "But more generally, this actually belongs to the class of approximate Policy Iteration algorithms. Let's recall the Policy Iteration procedure:\n",
    "1. Solve $Q=T^\\pi Q$\n",
    "2. Solve $\\pi = Greedy(Q)$\n",
    "3. Repeat\n",
    "\n",
    "And let's now allow for an approximate resolution of these steps, via gradient descent:\n",
    "1. Approximately solve $\\min \\|Q - T^\\pi Q \\|$ via gradient descent\n",
    "2. Approximately solve $\\pi = Greedy(Q)$ using the policy gradient theorem\n",
    "3. Repeat\n",
    "\n",
    "After each collected sample, the update of the Actor-Critic algorithm above performs exactly one gradient step on the critic and one gradient step for the actor.\n",
    "\n",
    "This perspective allows to define a much broader family of Actor-Critic methods that perform various number of gradient steps on the critic or the actor, use n-step returns, introduce a sequence of $Q_{i+1} = T^\\pi Q_i$ functions, soften the policy gradient steps, etc. in order to make the overall Actor-Critic algorithm more efficient and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52a8b4-df62-4f2f-8299-f3f825dc1076",
   "metadata": {},
   "source": [
    "# <div id=\"deep\"></div>Deep Actor-Critic algorithms and references\n",
    "\n",
    "At this stage, it is really tempting to throw a neural network at our critic and our actor, start collecting samples in a replay buffer, and try to design Deep Policy Gradient methods. Let's review some of the key ones from the litterature.\n",
    "\n",
    "- [Asynchronous Advantage Actor-Critic (A3C) (2016)](https://arxiv.org/abs/1602.01783). Builds a unique network that approximates $V$ and $\\pi$, replaces the replay buffer with an army of asynchronous actors that provide independent samples for the gradient computations. It is the direct adaptation of the Actor-Critic algorithm above. Its little brother A2C discards the asynchronous aspect, while keeping the good overall performance.\n",
    "- [Trust Region Policy Optimization (TRPO) (2015)](https://arxiv.org/abs/1502.05477). Imposes small policy gradient steps by introducing a \"maximum KL divergence between successive policies\" constraint in the actor's update.\n",
    "- [Proximal Policy Optimization (PPO) (2017)](https://arxiv.org/abs/1707.06347). Same philosophy as TRPO but simpler and more efficient. Instead of a KL divergence constraints, it imposes a gradient clipping for the policy gradient.\n",
    "- [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) (2017)](https://arxiv.org/abs/1708.05144). Uses Kronecker factorization to make TRPO's update more efficient.\n",
    "- [Sample Efficient Actor-Critic with Experience Replay (ACER) (2017)](https://arxiv.org/abs/1611.01224). Several improvements upon TRPO and A2C.\n",
    "- [Soft Actor Critic Algorithms (SAC) (2019)](https://arxiv.org/abs/1812.05905). Introduces an entropy regularization term in the objective function.\n",
    "- [Modified Actor Critic algoritms (MoPPO) (2019)](https://arxiv.org/abs/1907.01298). Casts the critic update in a modified policy iteration scheme by building the sequence of $Q_{i+1} = T^\\pi Q_i$ functions, applies this to PPO.\n",
    "\n",
    "One thing that was not covered in this class is the [Deterministic Policy Gradient theorem](http://proceedings.mlr.press/v32/silver14.html). which allows to perform policy gradient steps on deterministic policies (with many benefits). This family of algorithms spanned their own deep counterparts. Notably:\n",
    "- [Deep Deterministic Policy Gradients (DDPG) (2015)](https://arxiv.org/abs/1509.02971). Implements the DPG theorem on deep neural networks, with a replay buffer.\n",
    "- [Twin Delayed Deep Deterministic Policy Gradients (TD3) (2018)](https://arxiv.org/abs/1802.09477). Introduces three improvements over DDPG, namely a double critic update, two separate networks for the critic and target policy smoothing.\n",
    "- [Distributed Distributional Deep Deterministic Policy Gradients (D4PG) (2018)](https://arxiv.org/abs/1804.08617). Improves on DDPG with parallel actors, a distributional value function estimator, and batch normalization.\n",
    "\n",
    "One can remark that the (stochastic) Policy Gradient update is an *on-policy* update: it requires the samples to have been drawn from the current policy's stationary distribution. This was generalized to off-policy updates in the [Off-Policy Actor Critic](https://arxiv.org/abs/1205.4839) paper (2012) and has been used in most of the algorithms above.\n",
    "\n",
    "Lilian Weng keeps a nice overview (and zoo) of Actor-Critic methods [on her blog](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html).\n",
    "\n",
    "The introduction to Policy Gradients from [OpenAI's Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#) is also a great reading after this class.\n",
    "\n",
    "Finally, to grasp an overview of Policy search methods (even beyond the scope of Policy Gradients) a good read is the [Policy Search in Continuous Action Domains: an Overview](https://arxiv.org/abs/1803.04706) paper (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a089a2-5f11-432e-991f-83afbec7a36a",
   "metadata": {},
   "source": [
    "# <div id=\"practice\"></div>Practice\n",
    "\n",
    "In this section we'll focus on implementing two of these Actor-Critic algorithms: the stochastic SAC and the deterministic DDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a6961-0e08-48f6-9377-eaa99f897734",
   "metadata": {},
   "source": [
    "## <div id=\"why\"></div>Why use Actor-Critic algorithms?\n",
    "\n",
    "We have seen in the previous classes that the DQN algorithm is quite easy to implement and can be used to tackle a whole category of problems that can be modelised with MDPs.\n",
    "\n",
    "However, there's a whole family of MDPs that present an issue when trying to apply DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bf288-339a-4b02-8073-d0f8887e47d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Can you guess which kind of problem?</br>\n",
    "    Which step of the DQN algorithm can't be directly applied as we've done before?</br>\n",
    "    <strong>Hint</strong>: we quickly mentioned it in the beginning of this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14910181-58d9-460e-bcf1-92626801e55b",
   "metadata": {},
   "source": [
    "Recall the TD update of Q-learning:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "To compute the temporal difference $\\delta$, we need to compute the max of $Q(s', a')$ over all the possible actions $a' \\in A$.\n",
    "\n",
    "Until now, we focused on MDPs with an action state of finite cardinality. In the case where the action space is continuous, getting the maximum of the Q-function on the action space becomes a complex optimisation problem.\n",
    "\n",
    "Although we could try to solve this problem directly, it's more practical to have a way to represent the policy explicitely, hence the use of an Actor-Critic architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93991838",
   "metadata": {},
   "source": [
    "## <div id=\"environments\"></div> Environments\n",
    "\n",
    "First, let's introduce the environments we are going to use.\n",
    "\n",
    "### [Pendulum](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "\n",
    "In this environment, the objective is to control a pendulum that is fixed on one end by applying a torque on the free end in order to swing it into an upright position.</br>\n",
    "The state of the pendulum is given by $s = (cos(\\theta), sin(\\theta), \\dot{\\theta})$.</br>\n",
    "The action space is continuous: it corresponds to the torque applied on the pendulum $a \\in [-2, 2]$\n",
    "\n",
    "<center><img src=\"https://gymnasium.farama.org/_images/pendulum.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f1ee8-9f03-46fe-a0c4-883b57b49800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pendulum = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "print(\"action space:\", pendulum.action_space)\n",
    "print(\"observation space:\", pendulum.observation_space)\n",
    "\n",
    "s, _ = pendulum.reset()\n",
    "for i in range(100):\n",
    "    _, _, done, _, _ = pendulum.step(pendulum.action_space.sample())\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1ebe",
   "metadata": {},
   "source": [
    "### [LunarLanderContinuous](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "The objective in this environment is to control a lunar lander space ship in order to land on a small pad located at $(0, 0)$.\n",
    "Here is the state space and action space descriptions from the environment webpage:\n",
    "\n",
    "> The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "> The first coordinate of an action determines the throttle of the main engine, while the second coordinate specifies the throttle of the lateral boosters. Given an action np.array([main, lateral]), the main engine will be turned off completely if main < 0 and the throttle scales affinely from 50% to 100% for 0 <= main <= 1 (in particular, the main engine doesn’t work with less than 50% power). Similarly, if -0.5 < lateral < 0.5, the lateral boosters will not fire at all. If lateral < -0.5, the left booster will fire, and if lateral > 0.5, the right booster will fire. Again, the throttle scales affinely from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively).\n",
    "\n",
    "<center><img src=\"https://gymnasium.farama.org/_images/lunar_lander.gif\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df019f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lunar = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"human\")\n",
    "\n",
    "print(\"observation space:\", lunar.observation_space)\n",
    "print(\"action space:\", lunar.action_space)\n",
    "\n",
    "s, _ = lunar.reset()\n",
    "while True:\n",
    "    _, _, done, _, _ = lunar.step(lunar.action_space.sample())\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "lunar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb278307",
   "metadata": {},
   "source": [
    "## Code template\n",
    "\n",
    "Although we have to adapt the core architecture of the algorithm, we'll reuse some of the previous tools we developed such as the code for the Replay Buffer.\n",
    "\n",
    "We'll also write a `simulate` function that takes an environment and an agent as arguments, and simply run a number of episodes in this environment following the actions given by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f580879-0d48-48e0-8111-48617a485305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return [torch.Tensor(np.array(x)).to(device) for x in zip(*batch, strict=True)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9e022-a5c3-49b4-9849-8ecc4c2def0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def simulate(environment, agent, nb_episodes=200, verbose=True):\n",
    "    episodes_rewards_sum = []\n",
    "\n",
    "    for episode_id in range(nb_episodes):\n",
    "        state, _ = environment.reset()\n",
    "        agent.last_state = state\n",
    "\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, trunc, _ = environment.step(action)\n",
    "            done = done or trunc\n",
    "\n",
    "            agent.replay_buffer.append(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        rewards_sum = sum(episode_rewards)\n",
    "        episodes_rewards_sum.append(rewards_sum)\n",
    "        environment.close()\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Episode {episode_id:3d}, episode return {rewards_sum:4.1f},\",\n",
    "                f\"last 20 average: {mean(episodes_rewards_sum[-20:]):4.1f}\"\n",
    "            )\n",
    "    return episodes_rewards_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d2bc7-76e0-4610-a8d0-d7525e73a256",
   "metadata": {},
   "source": [
    "## <div id=\"ddpg\"></div>Deep-Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "In this section, we'll implement the DDPG algorithm, one of the most straightforward actor-critic algorithm.</br>\n",
    "The idea behind DDPG is simple: we learn a Q-function using a deep NN as in DQN, but we also learn in parallel an explicit deterministic policy $\\mu_\\theta(s)$ whose objective is to return the action that maximizes the Q-function, i.e. $\\mu_\\theta(s) \\approx \\mathrm{argmax}_{a} Q(s, a)$.\n",
    "\n",
    "To learn this deterministic policy $\\mu$, we can show using the Deterministic Policy Gradient theorem that we can simply perform gradient ascent on the policy parameters to solve $$\\max_\\theta \\mathbb{E}_{s \\sim D} \\left[ Q_\\phi(s, \\mu_\\theta(s)) \\right]$$\n",
    "\n",
    "As in DQN, DDPG keeps one target network for the actor and one for the critic, that *lag* behind to improve stability.\n",
    "This time, we'll use a smooth update (also called **Polyak update**) for the target networks: $\\theta^- \\leftarrow \\tau \\theta^- + (1 - \\tau) \\theta_n$.\n",
    "\n",
    "For exploration, instead of using an $\\epsilon$-greedy policy DDPG instead adds a centered Gaussian noise on the action.\n",
    "\n",
    "Note that DDPG is an off-policy algorithm, i.e. the policy used to train the critic is different than the one used to generate samples, which means that as for DQN we can store transitions in a Replay Buffer and sample from it to train the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7e751",
   "metadata": {},
   "source": [
    "Let's start by defining the architecture of our neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "def initialize_weights(layer, bound=None):\n",
    "    if bound is None:\n",
    "        bound = 1.0 / np.sqrt(layer.weight.data.size()[0])\n",
    "    torch.nn.init.uniform_(layer.weight.data, -bound, bound)\n",
    "    torch.nn.init.uniform_(layer.bias.data, -bound, bound)\n",
    "\n",
    "\n",
    "class DefaultNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        layer_1_dim,\n",
    "        layer_2_dim,\n",
    "        output_dim,\n",
    "        learning_rate,\n",
    "        device,\n",
    "        last_activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.last_activation = last_activation\n",
    "        self.layer_1 = nn.Linear(input_dim, layer_1_dim)\n",
    "        self.layer_norm_1 = nn.LayerNorm(layer_1_dim)\n",
    "\n",
    "        self.layer_2 = nn.Linear(layer_1_dim, layer_2_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(layer_2_dim)\n",
    "\n",
    "        self.layer_3 = nn.Linear(layer_2_dim, output_dim)\n",
    "\n",
    "        initialize_weights(self.layer_1)\n",
    "        initialize_weights(self.layer_2)\n",
    "        initialize_weights(self.layer_3, bound=0.003)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        if self.last_activation is not None:\n",
    "            x = self.last_activation(x)\n",
    "        return x\n",
    "\n",
    "    def update_towards(self, other_model, tau=0.01):\n",
    "        \"\"\"Polyak update towards `other_model`.\"\"\"\n",
    "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(\n",
    "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd8e70-a498-43f6-9079-00c084c90c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE FOLLOWING CODE TEMPLATE\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        actor_lr=0.000025,\n",
    "        critic_lr=0.00025,\n",
    "        tau=0.001,\n",
    "        gamma=0.99,\n",
    "        buffer_size=1000000,\n",
    "        layer1_size=200,\n",
    "        layer2_size=150,\n",
    "        batch_size=64,\n",
    "        noise_std=0.1,\n",
    "    ):\n",
    "        self.name = \"DDPG\"\n",
    "\n",
    "        assert isinstance(action_space, gym.spaces.Box)\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.action_low = torch.Tensor(action_space.low)\n",
    "        self.action_high = torch.Tensor(action_space.high)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Noise distribution to add to the action for exploration\n",
    "        self.noise_distribution = torch.distributions.normal.Normal(\n",
    "            torch.zeros(nb_actions),\n",
    "            noise_std * torch.ones(nb_actions),\n",
    "        )\n",
    "\n",
    "        # Define the actor and critic networks\n",
    "        self.actor = ...\n",
    "        self.critic = ...\n",
    "\n",
    "        # Define the corresponding target networsk\n",
    "        self.target_actor = deepcopy(self.actor)\n",
    "        self.target_critic = deepcopy(self.critic)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "\n",
    "        # Get action from the actor\n",
    "        with torch.no_grad():\n",
    "            action = ...\n",
    "\n",
    "        # Add noise to action while making sure it stays valid\n",
    "        noise = self.noise_distribution.sample()\n",
    "        action = torch.clamp(action + noise, self.action_low, self.action_high)\n",
    "\n",
    "        return action.cpu().detach().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(\n",
    "            self.batch_size\n",
    "        )\n",
    "\n",
    "        # Compute Q(s, a)\n",
    "        critic_value = ...\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute \\mu(s')\n",
    "            target_actions = ...\n",
    "\n",
    "            # Compute Q(s', \\mu(s')), which should approximate max_a' Q(s', a')\n",
    "            next_critic_values = ...\n",
    "\n",
    "        # Target r + \\gamma * (1 - done) * Q(s', \\mu(s'))\n",
    "        target = ...\n",
    "\n",
    "        # Compute critic loss as in DQN (=TD error)\n",
    "        critic_loss = ...\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        # Update actor in order to maximize Q(s, \\mu(s))\n",
    "        actor_loss = ...\n",
    "        actor_loss = torch.mean(actor_loss)\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.target_critic.update_towards(self.critic, tau=self.tau)\n",
    "        self.target_actor.update_towards(self.actor, tau=self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9cb04-a910-469b-a52f-4f72e6a61c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/RL6_DDPG.py\n",
    "# To get an example of implementation, uncomment the line above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba72e0-9370-476f-bfdd-ca7bc1e2092f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pendulum = gym.make(\"Pendulum-v1\")\n",
    "agent = DDPGAgent(pendulum.observation_space, pendulum.action_space)\n",
    "rewards = simulate(pendulum, agent, verbose=True)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424913ec-5c22-4c4a-9684-18c4d37d964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "rewards = simulate(pendulum, agent, nb_episodes=1, verbose=False)[0]\n",
    "print(\"Total rewards:\", rewards)\n",
    "pendulum.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fab8b1a-62c6-42ea-9756-0decf28f3341",
   "metadata": {},
   "source": [
    "## <div id=\"sac\"></div>Soft Actor-critic (SAC)\n",
    "\n",
    "Let's now focus on a stochastic algorithm: Soft Actor-Critic. This algorithm is more or less the direct application of the Policy Gradient Theorem as we wrote it in the beginning of this notebook, with an additional entropy term to encourage exploration.\n",
    "\n",
    "### Stochastic actor implementation\n",
    "\n",
    "Compared to DDPG, the actor in SAC is stochastic, i.e. it outputs a probability distribution over all the actions. We usually achieve this by making the actor outputs a vector of size $(n, 2)$ (with $n$ the dimension of the action space) corresponding to the mean and standard deviation of a Gaussian distribution for each action dimension.\n",
    "As we need the standard deviation to be positive, we usually interpret the actor output as the $\\textrm{log}$ of the standard deviation:\n",
    "$$\\forall k \\in [1; n], a_k \\sim \\mathcal{N}(\\mu_k, \\sigma_k)$$\n",
    "$$\\textrm{ with } (\\mu_k, \\log{\\sigma_k}) = \\pi_k(s)$$\n",
    "\n",
    "To select an action from our policy, we can then sample it from the resulting multi-dimensional normal distribution.\n",
    "However, sampling directly the action would prevent any gradient propagation as sampling is not a differentiable operation, so in order to keep the gradient flow we use the **reparameterization trick**.\n",
    "\n",
    "The idea is simple: instead of sampling the action $a \\sim \\mathcal{N}(\\mu, \\sigma)$, we introduce a random variable $\\epsilon \\sim \\mathcal{N}(0, 1)$ corresponding to a noise term and compute the action as $a = \\mu + \\epsilon \\sigma$.\n",
    "This reparametrization trick is already implemented in `torch` for multiple distributions (including Normal): [cf doc here](https://docs.pytorch.org/docs/stable/distributions.html#pathwise-derivative).\n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRtHJvIDnw3fZ0A2_H4hp2KKhqteU1xBmRSgw&s\"></center>\n",
    "\n",
    "When sampling our actions this way, another issue is that the normal distribution is unbounded whereas our environment action space is not.\n",
    "To fix this, we can apply the `tanh` function to the sampled action to get a value between -1 and 1, and then scale it to our action space range.\n",
    "\n",
    "However, this also changes the computation of the log probabilities of our distribution: the effect of enforcing the bounds is computed explicitely in the [Appendix C. of the SAC original paper (p.16)](https://arxiv.org/pdf/1812.05905):\n",
    "<center><img src=\"img/SAC_bounds.png\" width=\"60%\"></center>\n",
    "\n",
    "\n",
    "### Entropy term\n",
    "\n",
    "For a random variable $X$ with a law of density $p(X)$, we define its entropy as $$\\mathcal{H}(X) = -\\int_{X} p(x) log (p(x))$$\n",
    "Intuitively, as in the discrete case with Shannon entropy, it measures the uncertainty of a random variable: it is maximal when $X$ follows a uniform distribution and minimal when $X$ can only take a single value (i.e. $p(X)$ is a Dirac).\n",
    "\n",
    "In addition to the stochastic actor, SAC also modifies the agent objective with the expected entropy of the policy:\n",
    "$$J(\\pi) = \\sum_{t = 0}^{T} \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\left[ r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(. | s_t)) \\right]$$\n",
    "\n",
    "About $\\alpha$, directly from the paper:\n",
    "> The temperature parameter $\\alpha$ determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy.\n",
    "> The maximum entropy objective differs from the standard maximum expected reward objective used in conventional reinforcement learning, though the conventional objective can be recovered in the limit as $\\alpha \\rightarrow 0$. \n",
    "\n",
    "The authors also describe the advantages of using such an objective term:\n",
    "> This objective has a number of conceptual and practical advantages. First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues.\n",
    "> Second, the policy can capture multiple modes of nearoptimal behavior. In problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions.\n",
    "> Lastly, prior work has observed improved exploration with this objective\n",
    "\n",
    "In order to apply RL to this modified objective, we adapt the definition of the Q-function to take the entropy term into account and define the **soft Q-value function** which can be computed by iteratively applying a soft Bellman operator:\n",
    "$$\\delta = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t + 1} \\sim p, a_{t + 1} \\sim \\pi} \\left[ Q(s_{t + 1}, a_{t + 1}) - \\log \\pi(a_{t + 1} | s_{t + 1}) \\right]$$\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\beta \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5387f51-8d8e-4d84-973b-68fb2dadbe98",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935498c",
   "metadata": {},
   "source": [
    "Now you can see that entropy regularisation improve so much DDPG performances.\n",
    "\n",
    "You don't need to remeber every performance improvement tricks we used in this notebook, but you should understand how DDPG and SAC works, and what are the differences between them, and between DQN and them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efff3b-3dfd-4821-92ec-bd15ef1c9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL6_SAC.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        actor_lr=0.001,\n",
    "        critic_lr=0.002,\n",
    "        gamma=0.99,\n",
    "        buffer_size=100000,\n",
    "        tau=0.01,\n",
    "        layer1_size=64,\n",
    "        layer2_size=64,\n",
    "        batch_size=32,\n",
    "        alpha=0.5,\n",
    "    ):\n",
    "        self.name = \"SAC\"\n",
    "\n",
    "        assert isinstance(action_space, gym.spaces.Box)\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.action_low = torch.Tensor(action_space.low)\n",
    "        self.action_high = torch.Tensor(action_space.high)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        state_size = state_space.shape[0]\n",
    "        nb_actions = self.action_space.shape[0]\n",
    "\n",
    "        self.actor = (\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(state_size, layer1_size),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(layer1_size, layer2_size),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(layer2_size, 2 * nb_actions),\n",
    "            )\n",
    "            .to(device)\n",
    "            .float()\n",
    "        )\n",
    "        self.critic = (\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(state_size + nb_actions, layer1_size),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(layer1_size, layer2_size),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(layer2_size, 1),\n",
    "            )\n",
    "            .to(device)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.target_critic = deepcopy(self.critic)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_towards(self, model, other_model, tau=None):\n",
    "        \"\"\"Polyak update towards `other_model`.\"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        for self_param, other_param in zip(model.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(self_param.data * (1.0 - tau) + other_param.data * tau)\n",
    "\n",
    "    def sample_action(self, state, reparameterize=False):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "\n",
    "        if len(state.shape) == 1:  # Add batch dimension\n",
    "            state = state.unsqueeze(0)\n",
    "\n",
    "        # Get actor outputs\n",
    "        actor_output = ...\n",
    "\n",
    "        # Compute action mean and std\n",
    "        actions_means = ...\n",
    "        actions_stds = ...\n",
    "\n",
    "        # Get action distribution (cf. torch.distributions)\n",
    "        actions_distribution = ...\n",
    "\n",
    "        # Sample from the action distribution using the reparametrization trick if necessary\n",
    "        actions = ...\n",
    "\n",
    "        # Bound actions to [-1, 1]\n",
    "        bounded_actions = ...\n",
    "\n",
    "        # Compute log probabilities (/!\\ Don't forget the transformation term!)\n",
    "        log_probs = ...\n",
    "\n",
    "        # Scale actions to the environment bounds\n",
    "        scaled_actions = (bounded_actions + 1) / 2  # In [0, 1]\n",
    "        scaled_actions = (self.action_high - self.action_low) * scaled_actions + self.action_low  # In [low, high]\n",
    "\n",
    "        return scaled_actions, log_probs\n",
    "\n",
    "    def get_action(self, state):\n",
    "        actions, _ = self.sample_action(state, reparameterize=False)\n",
    "        return actions.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, done = self.replay_buffer.sample(\n",
    "            self.batch_size\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute a' = \\pi(s') and log \\pi(s')\n",
    "            next_actions, next_log_probs = ...\n",
    "\n",
    "            # Compute Q(s', \\pi(s'))\n",
    "            next_q_values = ...\n",
    "\n",
    "        # Compute target = r + \\gamma * (1 - d) * [ Q(s', \\pi(s')) - \\alpha log \\pi(s') ]\n",
    "        td_error = ...\n",
    "\n",
    "        # Compute Q(s, a)\n",
    "        q_values = ...\n",
    "\n",
    "        # Compute critic loss (TD error)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = ...\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.update_towards(self.target_critic, self.critic)\n",
    "\n",
    "        # Train actor: compute Q(s, \\pi(s))\n",
    "        actions, log_probs = ...\n",
    "        critic_values = ...\n",
    "\n",
    "        # Compute soft actor loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = ...\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe2c91-2459-4f79-b50b-4a7cf4957c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Test our agent on LunarLander\n",
    "lunar = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "print(\" > Training SAC\")\n",
    "sac_agent = SACAgent(lunar.observation_space, lunar.action_space)\n",
    "sac_results = simulate(lunar, sac_agent, nb_episodes=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63290c-fd0b-4e3d-a9fc-ed4b4d32acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"human\")\n",
    "rewards = simulate(lunar, agent, nb_episodes=1, verbose=False)[0]\n",
    "print(\"Total rewards:\", rewards)\n",
    "lunar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
