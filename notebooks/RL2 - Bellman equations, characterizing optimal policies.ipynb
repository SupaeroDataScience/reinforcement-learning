{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 2: Bellman equations, characterizing optimal policies**\n",
    "\n",
    "\n",
    "1. [Everything you need to know](#everything)\n",
    "2. [Reminder](#reminder)\n",
    "3. [The Bellman equations](#bellman)\n",
    "    1. [State-action value functions](#Qfunc)\n",
    "    6. [Evaluation equation](#eval)\n",
    "    7. [Optimality equation](#optimality)\n",
    "4. [Model-based algorithms](#model)\n",
    "    1. [Value Iteration](#vi)\n",
    "    2. [Policy Iteration and Modified Policy Iteration](#pi)\n",
    "4. [Approximate Dynamic Programming](#adp)\n",
    "    1. [Approximate Value Iteration](#avi)\n",
    "    2. [Approximate Policy Iteration](#api)\n",
    "5. [Linear Programming](#lp)\n",
    "6. [Asynchronous Dynamic Programming](#asyncdp)\n",
    "    1. [Asynchronous Value Iteration](#asyncvi)\n",
    "    2. [Asynchronous Policy Iteration](#asyncpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous class (RL1) introduced the model of Markov Decision Processes as a way to describe discrete-time, stochastic, dynamical systems. Our focus is on controling such systems. For this we want to characterize what makes a policy optimal and how to find it.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Systems of linear equations\n",
    "- Contraction mappings\n",
    "- Definition of a Markov Decision Process, a policy, a value function\n",
    "\n",
    "**Useful but not compulsory:**\n",
    "- Dynamic Programming\n",
    "- Linear Programming\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"everything\"></a>Everything you need to know\n",
    "\n",
    "\n",
    "Everything you should remember after this session.\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> Evaluation equation: $V^\\pi \\in \\mathcal{F}\\left(S,\\mathbb{R}\\right)$ is the only solution to $V\\left(s\\right) = (T^\\pi V)(s) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)$<br>\n",
    "Similarly, $Q^\\pi \\in \\mathcal{F}\\left(S\\times A,\\mathbb{R}\\right)$ is the only solution to $Q\\left(s,a\\right) = (T^\\pi Q)(s,a) = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q\\left(s', \\pi\\left(s'\\right)\\right)$\n",
    "<li> Properties of $T^\\pi$ 1) $V=T^\\pi V$ is a linear system, 2) $T^\\pi$ is a contraction mapping over $\\mathcal{F}(S,\\mathbb{R})$ or $\\mathcal{F}(S\\times A,\\mathbb{R})$.\n",
    "<li> Finding $V^\\pi$ or $Q^\\pi$: by matrix inversion or by repeatedly applying $T^\\pi$ to any initial function.\n",
    "<li> Optimality equation: $V^* \\in \\mathcal{F}\\left(S,\\mathbb{R}\\right)$ is the only solution to $V(s) = (T^* V) (s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s') \\right]$<br>\n",
    "Similarly, $Q^* \\in \\mathcal{F}\\left(S\\times A,\\mathbb{R}\\right)$ is the only solution to $Q(s,a) = (T^* Q) (s,a) = r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q(s',a')$\n",
    "<li> $T^*$ is a contraction mapping.\n",
    "<li> Value Iteration: finding $V^*$ or $Q^*$ by repeatedly applying $T^*$ to any initial function.\n",
    "<li> Policy Iteration: finding $V^*$ or $Q^*$ and $\\pi^*$ by building the sequence $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ that converges to $\\pi^*$. Repeatedly alternates an evaluation and an improvement phase.\n",
    "<li> Modified Policy Iteration: apply $T^\\pi$ for $m$ steps (evaluation phase), then define $\\pi$ as the greedy policy (improvement phase) and repeat.\n",
    "<li> Approximate Value Iteration and Approximate Policy Iteration consist in approximating value functions within Value and Policy Iterations. They do not converge but reach policies whose values are within a bounded range of the optimal value function.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Of course, all this seems very obscure right now and the block above will only serve as a reminder when you re-open the notebook later. We will introduce every concept intuitively and progessively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"reminder\"></a>Reminder\n",
    "\n",
    "Recall the main results from the previous class.\n",
    "\n",
    "We have introduced the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$.\n",
    "\n",
    "Based on the analysis of the previous section, we introduce several algorithms that will lay a solid ground for model-free RL algorithms in the next class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitions in a few drawings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the maze below, where an agent can move North, South, East or West, the resulting transition is deterministic and a reward of $+1$ is gained when exiting the maze (which terminates the game). Otherwise all rewards are zero. Bumping into a wall leaves you in the same place.\n",
    "\n",
    "<img src=\"img/grid_raw.png\" width=\"200px\"></img>\n",
    "\n",
    "Let's consider the policy $\\pi$ that always moves East.\n",
    "\n",
    "<img src=\"img/grid_policy.png\" width=\"200px\"></img>\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Without writing any equation, what is the value of the top-right cell under this policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim1\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim1\" class=\"collapse\">\n",
    "$V^\\pi((3,3)) = 1$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take $\\gamma=0.9$.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Without writing any equation, what is the value of the top-middle cell under this policy? What is the value of the bottom-right cell?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim2\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim2\" class=\"collapse\">\n",
    "\n",
    "The value of $(2,3)$ is the expected discounted sum of what one gets from applying $\\pi$ from $(2,3)$. Since the $\\pi$ is deterministic and the transitions are deterministic too, $\\pi(2,3)$ always take us to state $(3,3)$. So $V^\\pi((2,3)) = \\gamma \\times V^\\pi((3,3)) = 0.9$.\n",
    "    \n",
    "The value of $(3,1)$ is the expected infinite sum of discounted rewards from $(3,1)$. Since the agent keeps bumping into the wall when applying $\\pi$, it never exits the maze and this is an infinite sum of zero terms. Hence $V^\\pi((2,3)) = 0$.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the value function.\n",
    "\n",
    "<img src=\"img/grid_vpi.png\" width=\"200px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are currently in cell $(1,2)$ and would like to choose what action to take. Suppose also that you know the value function above. You need to put a scalar value on all four actions. To evaluate each action, let's estimate what we can get by applying the action and then using $\\gamma \\times V^\\pi(s)$ to estimate what can obtain in the long run after this first action. Define $Q^\\pi((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "What is $Q^\\pi((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we follow $\\pi$ after?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim3\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim3\" class=\"collapse\">\n",
    "\n",
    "$Q^\\pi((1,2),N) = 0.729$  \n",
    "$Q^\\pi((1,2),S) = 0$  \n",
    "$Q^\\pi((1,2),E) = 0$  \n",
    "$Q^\\pi((1,2),W) = 0$  \n",
    "The best action seems to be $N$.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimal policy is quite easy to guess. Let's draw the optimal value function (the value function of any optimal policy).\n",
    "\n",
    "<img src=\"img/grid_vopt.png\" width=\"200px\"></img>\n",
    "\n",
    "Define $Q^*((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$ if it is followed by an optimal policy.\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "What is $Q^*((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we act optimally after? Rank the actions by utility.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim4\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim4\" class=\"collapse\">\n",
    "\n",
    "$Q^*$ is what we gain immediately, plus $\\gamma$ times what we expect to receive from applying an optimal policy in the state we reach by applying $a$.  \n",
    "$Q^*((1,2),N) = 0 + \\gamma\\times\\gamma^2=\\gamma^3$  \n",
    "$Q^*((1,2),S) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),E) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),W) = 0 + \\gamma\\times\\gamma^3=\\gamma^4$  \n",
    "The best action seems to be $N$, followed by $W$, after that $S$ and $E$ are tied.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "What property has the policy that always picks greedily the $Q^*$ maximizing action in each state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim5\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim5\" class=\"collapse\">\n",
    "\n",
    "It is an optimal policy.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose $(1,2)$ is a special slippery cell. Going North has a $0.7$ probability of actually reaching $(1,3)$, but also a $0.2$ probability of staying in $(1,2)$ and a $0.1$ probability of ending in $(2,2)$. Note that this changes the problem and the optimal expected return function $V^*$.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Given this new problem, can you write $Q^*((1,2),N)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim6\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim6\" class=\"collapse\">\n",
    "\n",
    "When we take action $N$ in $(1,2)$, there are 3 possible outcomes:\n",
    "- with probability $0.7$, reach $(1,3)$ and get reward $0$,\n",
    "- with probability $0.2$, reach $(1,2)$ and get reward $0$,\n",
    "- with probability $0.1$, reach $(2,2)$ and get reward $0$.\n",
    "\n",
    "So what we can expect to get from applying $N$ in $(1,2)$ is:  \n",
    "\\begin{align*}\n",
    "    Q^*((1,2), N) &= 0.7 \\times (0+\\gamma V^*(1,3)) + 0.2\\times(0+\\gamma V^*(1,2)) + 0.1\\times(0+\\gamma V^*(2,2))\\\\\n",
    "    &= \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can remark that if we knew the action $\\pi^*((1,2))$ taken by an optimal policy in $(1,2)$, then $Q^*((1,2), \\pi^*(1,2))$ would actually be precisely the optimal long-term return $V^*$ (since it would be the expected return of a policy that acts optimally at every time step, including the first one).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Suppose an oracle tells us that $\\pi^*((1,2))=N$. Using the previous exercice, write $V^*(1,2)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#prelim7\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"prelim7\" class=\"collapse\">\n",
    "\n",
    "We have $V^*((1,2)) = Q^*((1,2),N)$, so\n",
    "$$V^*((1,2)) = \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)$$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced the key concepts upon which this class is built: $V$ and $Q$ functions, and the relation between $V(s)$ and $V(s')$ when $s'$ can be reached from $s$ in one action. The next steps are now to write all this formally, prove strong properties and derive algorithms for computing value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"bellman\"></a> The Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Qfunc\"></a>State-action value functions\n",
    "\n",
    "Let's play a little bit (mathematically) with our new toys.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "What's the value of \"$a$ for the first step, then $\\pi$, starting from $s$\"?\n",
    "</div>\n",
    "\n",
    "Note: the answer uses the discounted criterion's definition and the value function of $\\pi$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersQsa\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersQsa\" class=\"collapse\">\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) & = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\mathbb{E}\\left( \\sum\\limits_{t=1}^\\infty \\gamma^t r\\left(S_t, A_t\\right) \\bigg| S_0 = s, A_0 = a, \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) \\mathbb{E}\\left( \\sum\\limits_{t=1}^\\infty \\gamma^{t-1} r\\left(S_t, A_t\\right) \\bigg| S_1 = s', \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) V^\\pi\\left(s'\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have introduced the very important state-action value function $Q^\\pi$.\n",
    "<div class=\"alert alert-success\"><b>State-action value function</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>\n",
    "\n",
    "To be precise and reuse the full notations from the MDP definition:\n",
    "$Q^\\pi(s,a)=\\mathbb{E}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t=\\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right].$\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_{s'} \\left[ r(s,a,s') + \\gamma V^\\pi(s') \\right]$$\n",
    "\n",
    "$$Q^\\pi(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/Qfunctions.png\" style=\"height: 200px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Write a function that computes $Q^\\pi$ given $V^\\pi$ for FrozenLake. Use $\\gamma=0.9$.<br>\n",
    "Suppose $V^\\pi(s)=0$ in all $s$. Use your function to compute $Q^\\pi(s,a)$ given $V^\\pi$. Comment.\n",
    "</div>\n",
    "\n",
    "To help you, recall that in the previous class, we introduced a few utility functions and accessed the transition probabilities and rewards of FrozenLake using the `env.unwrapped.P` attribute as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "state = to_s(0,1)\n",
    "action = 0\n",
    "outcomes = env.unwrapped.P[state][action]\n",
    "print(outcomes)\n",
    "print()\n",
    "print(\"outcomes for the transition from state \", to_row_col(state), \" and action \", actions[action], \":\", sep='')\n",
    "for o in outcomes:\n",
    "    proba      = o[0]\n",
    "    next_state = o[1]\n",
    "    reward     = o[2]\n",
    "    isTerminal = o[3]\n",
    "    print(\" reach state \", to_row_col(next_state), \\\n",
    "          \" and get reward \", reward, \\\n",
    "          \" with proba \", proba, \". \", sep='', end=\"\")\n",
    "    if isTerminal:\n",
    "        print(\"Transition is terminal.\")\n",
    "    else:\n",
    "        print(\"Transition is not terminal.\")\n",
    "        \n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting: given a policy $\\pi$, the best one-step lookahead action can be selected by maximizing $Q^\\pi$. To improve on a policy $\\pi$, it is more useful to know $Q^\\pi$ than $V^\\pi$ and pick the *greedy* action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Write a function that takes a $Q$ function for FrozenLake and returns the greedy policy.<br>\n",
    "Use the function below to print the greedy policy for the $Q$ function of the previous exercise in a human-friendly format.<br>\n",
    "Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to really understand the importance of these $Q$-functions.\n",
    "- If one provides me with $Q^\\pi$ for a given $\\pi$, then I have a way to select a good action in any state $s$ I might encounter (by picking action $\\arg\\max_a Q^\\pi(s,a)$.\n",
    "- $Q^\\pi$ is \"what I can expect to get if I perform $a$ for $1$ step and then follow $\\pi$ for $\\infty$ steps\".\n",
    "- So $Q^\\pi(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"eval\"></a>Evaluation equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remark that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. Let's replace $a$ by $\\pi(s)$ above and we obtain an important equation to characterize $V^\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/V-DP.png\" style=\"height: 200px;\"></img>\n",
    "$$V^\\pi(s) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]$$\n",
    "\n",
    "This equation uses $V^\\pi(s')$ in all $s'$ reachable from $s$ to define $V^\\pi(s)$.  \n",
    "Since this equation is true in all $s$, this provides as many equations as we have states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Evaluation equation</b><br>\n",
    "$V^\\pi$ obeys the linear system of equations:\n",
    "$$\n",
    "V^\\pi\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]\\\\\n",
    "$$\n",
    "Similarly:\n",
    "$$\n",
    "Q^\\pi\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\n",
    "$$\n",
    "</div>\n",
    "\n",
    "This leads to the introduction of the **Bellman evaluation operator**:\n",
    "<div class=\"alert alert-success\"><b>Evaluation operator $T^\\pi$</b><br>\n",
    "$T^\\pi$ is an operator on value functions, that transforms a function $V:S\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi V\\left(s\\right) &= r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V(s') \\right]\\\\\n",
    " &= r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)\\\\\n",
    "T^\\pi V &= r^\\pi + \\gamma P^\\pi V^\\pi\n",
    "\\end{align*}\n",
    "    \n",
    "Similarly we can introduce an evaluation operator (with the name name $T^\\pi$) over state-action value functions. <br> \n",
    "$T^\\pi$ is an operator on state-action value functions, that transforms a function $Q:S\\times A\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi Q\\left(s,a\\right) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\\\\\n",
    " &= r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q^\\pi\\left(s', \\pi\\left(s'\\right)\\right)\\\\\n",
    "T^\\pi Q &= r + \\gamma P Q^\\pi\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that, fundamentally, we have written 6 times the same thing in the block above.  \n",
    "So finding $V^\\pi$ (resp. $Q^\\pi$) boils down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#morePpi\" data-toggle=\"collapse\"> What are $P^\\pi$, $r^\\pi$ and $T^\\pi$ here?</a><br>\n",
    "<div id=\"morePpi\" class=\"collapse\">\n",
    "When the state space is discrete,   <br>\n",
    "  &nbsp;&nbsp;&nbsp;$V$ is a vector of size $|S|$,  <br>\n",
    "  &nbsp;&nbsp;&nbsp;$P^\\pi$ is a matrix containing the values $P^\\pi_{ij} = p\\left(s_j|s_i,\\pi(s_i)\\right)$   <br>\n",
    "  &nbsp;&nbsp;&nbsp;and, similarly, $r^\\pi$ is a vector containing the values $r^\\pi_i = r(s_i,\\pi(s_i))$. <br>\n",
    "  &nbsp;&nbsp;&nbsp;In better words, $P^\\pi$ is the <i>transition kernel</i> of the Markov chain describing the state dynamics under policy $\\pi$ and $r^\\pi$ is the associated reward model.<br>\n",
    "<br>\n",
    "This generalizes straightforwardly to the continuous states case:    <br>\n",
    "  &nbsp;&nbsp;&nbsp;$V$ is a function in the $\\mathcal{F}(S,\\mathbb{R})$ function space (the generalization of the vector in the previous sentence),    <br>\n",
    "  &nbsp;&nbsp;&nbsp;$r^\\pi$ becomes the function $s\\mapsto r(s,\\pi(s))$    <br>\n",
    "  &nbsp;&nbsp;&nbsp;and  $P^\\pi$ becomes the operator over $\\mathcal{F}(S,\\mathbb{R})$ that maps function $V$ to function $s\\mapsto \\int\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)ds'$.<br>\n",
    "<br>\n",
    "In the same fashion, one can define the $T^\\pi$ operator.   <br>\n",
    "  &nbsp;&nbsp;&nbsp;In the discrete state space case, $T^\\pi$ is the linear operator in $\\mathbb{R}^{|S|}$ that maps $V$ to $r^\\pi + \\gamma P^\\pi V$.   <br>\n",
    "  &nbsp;&nbsp;&nbsp;In the continuous state space case, $T^\\pi$ is the linear operator in $\\mathcal{F}(S,\\mathbb{R})$ that maps $V$ to the function $s\\mapsto r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)$.\n",
    "</div>\n",
    "\n",
    "<a href=\"#moreEval\" data-toggle=\"collapse\"> Variations on a theme.</a><br>\n",
    "<div id=\"moreEval\" class=\"collapse\">\n",
    "For stochastic policies:\n",
    "\\begin{align*}\n",
    "    \\quad V^\\pi(s) &= \\mathbb{E}_{a\\sim\\pi(a|s)} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ V^\\pi\\left(s'\\right) \\right] \\right] \\\\\n",
    "        &= \\sum\\limits_{a\\in A} \\pi(a|s) \\left(r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V^\\pi(s') \\right)\n",
    "\\end{align*}<br>\n",
    "<br>\n",
    "If you prefer using $r(s,a,s')$ instead of $r(s,a)$:<br>\n",
    "\\begin{align*}\n",
    "    V^\\pi\\left(s\\right) &= \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ r(s,\\pi(s),s') + \\gamma V^\\pi\\left(s'\\right) \\right]\\\\\n",
    "        &= \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) \\left[ r\\left(s,\\pi\\left(s\\right),s'\\right) + \\gamma V^\\pi\\left(s'\\right) \\right]\n",
    "    \\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gone far from our original FrozenLake problem. Let's make all this very concrete:\n",
    "- A policy $\\pi$ is an agent's behaviour\n",
    "- In every state $s$, one can expect to gain $V^\\pi(s)$ in the long run by applying $\\pi$\n",
    "- $V^\\pi(s)$ is the sum of the reward on the first step $r(s,\\pi(s))$ and the expected long-term return from the next state $\\gamma \\mathbb{E}_{s'} \\left[V^\\pi(s')\\right]$ \n",
    "- The function $V^\\pi$ actually obeys the linear system of equations above that simply link the value of a state with the values of its successors in an episode.\n",
    "\n",
    "We can stop for a minute on the $T^\\pi$ evaluation operator (that maps a function $S\\rightarrow\\mathbb{R}$ to a function $S\\rightarrow\\mathbb{R}$) and the search for $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^\\pi$</b><br>\n",
    "<ol>\n",
    "<li> $T^\\pi$ is an affine operator, it defines a linear system of equations.<br>\n",
    "$\\Rightarrow$ Solving $V = T^\\pi V$?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With $\\gamma<1$, $V^\\pi = \\left(I-\\gamma P^\\pi\\right)^{-1}r^\\pi$\n",
    "<li> With $\\gamma<1$, $T^\\pi$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^\\pi$ (resp. $Q^\\pi$) is the unique solution to the (linear) fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^\\pi V$ (resp. $Q=T^\\pi Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Use the first property above to compute $V^\\pi$ for the policy that always moves right. To do this, you'll need to compute $r^\\pi$ and $P^\\pi$. Again, $\\gamma = 0.9$.\n",
    "Check if your result for $V^\\pi(s_0)$ is consistent with the Monte Carlo estimate of the previous class.\n",
    "</div>\n",
    "\n",
    "Use the cell below to recall the solution to the Monte Carlo estimation exercise from the previous class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercise1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Generalize the code above to a function that takes a policy as input. We'll suppose in this case that the policy is an array of actions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "    \n",
    "\n",
    "Use the second property above to compute $V^\\pi$ for the policy that always moves right. To do this, you'll need to remember that since $T^\\pi$ is a contraction mapping, the sequence $V_{n+1}=T^\\pi V_n$ converges to $T^\\pi$'s fixed point (which happens to be $V^\\pi$ according to the property). Again, $\\gamma = 0.9$. For a start, apply $T^\\pi$ for a fixed number of steps `max_iter`.\n",
    "Check if your result is consistent with the previous estimate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between two iterations in the algorithm above, the distance between $V_{n+1}$ and $V_n$ decreases as $\\|V_{n+1}-V_n\\| = \\|r^\\pi + \\gamma P^\\pi V_n - V_n\\|$. Since $T^\\pi$ is a contraction mapping, we have $\\|V_{n+1}-V_n\\| < \\|V_{n}-V_{n-1}\\|$. Let's call this distance at time step $n$ the **residual**. Then the successive residuals monotonically tend to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "    \n",
    "\n",
    "Now, can you use the property on the residuals to replace `max_iter` by a precision parameter `epsilon` that specifies the maximum error on $V^\\pi$? Advice: still keep `max_iter` to stop the computation in case you specify an `epsilon` that is too small. Return both $V^\\pi$ and the sequence of residuals.  \n",
    "Plot the sequence of residuals and display the number of iterations necessary to reach the chose precision `epsilon`. Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"optimality\"></a>Optimality equation\n",
    "\n",
    "Ok, so given a policy, we have a way (two actually) to compute its value function. Now we would like to find the optimal policy and we'd rather not enumerate all possible policies.\n",
    "\n",
    "Let's introduce some notations and start with two trivial remarks. We write:\n",
    "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
    "\n",
    "Then we can note that:\n",
    "1. $Q^*\\left(s,a\\right) = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S}p\\left(s'|s,a\\right) V^*\\left(s'\\right)$\n",
    "2. If $\\pi^*$ is an optimal policy, then $V^*\\left(s\\right) = Q^*\\left(s,\\pi^*\\left(s\\right)\\right)$\n",
    "\n",
    "From the first remark, we note that the maximum expected return one can get from a trajectory initiated in $s$ starts with the action $a$ that maximizes $Q^*(s,a)$. So:\n",
    "<div class=\"alert alert-success\"><b>Optimal greedy policy</b><br>\n",
    "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, finding $\\pi^*$ is equivalent to finding $Q^*$.\n",
    "\n",
    "Here comes the key theorem of this class:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality equation</b><br>\n",
    "The optimal value function obeys:\n",
    "\\begin{align*}\n",
    "    V^*(s) &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V^*(s') \\right]\\\\\n",
    "        &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]\n",
    "\\end{align*}\n",
    "or in terms of $Q$-functions:\n",
    "\\begin{align*}\n",
    "    Q^*(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q^*(s',a') \\right]\\\\\n",
    "        &= r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')\n",
    "\\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the evaluation equation, we have actually written 4 times the same thing in the block above.  \n",
    "We have also defined the **Bellman optimality operator $T^*$** (on $V$ and $Q$ functions) as:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality operator</b><br>\n",
    "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V(s') \\right]$$\n",
    "$$\\left(T^*Q\\right)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q(s',a') \\right]$$\n",
    "</div>\n",
    "\n",
    "So finding $V^*$ (resp. $Q^*$) boils down to solving $V= T^* V$ (resp. $Q = T^* Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does that unfold intuitively from the previous remarks? Simply because of the Dynamic Programming principle: any subpath of an optimal path is itself optimal. That might still sound obscure. So let's say that if you start a trajectory in $s$ and if you know you will get the maximum value from any state $s'$ you reach, then the maximum value you can get from $s$ is precisely $\\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]$. The actual proof is quite trickier but the key ideas are here.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-success\"><b>Properties of $T^*$</b><br>\n",
    "<ol>\n",
    "<li> $T^*$ is non-linear.<br>\n",
    "<li> With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "    \n",
    "\n",
    "Use the property above to compute $V^*$. To do this, you'll need to remember that since $T^*$ is a contraction mapping, the sequence $V_{n+1}=T^* V_n$ converges to $T^*$'s fixed point (which happens to be $V^*$ according to the property). Again, $\\gamma = 0.9$.  \n",
    "Plot the residuals and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"model\"></a>Model-based algorithms\n",
    "\n",
    "Based on the analysis of the previous section, we introduce several algorithms that will lay a solid ground for model-free RL algorithms in the next classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"vi\"></a>Value Iteration\n",
    "\n",
    "Value iteration is actually the algorithm we defined in the `vf_optim` function in the correction of the previous exercise. It directly exploits the contraction mapping property of $T^*$ and iterates over value functions in order to converge to $V^*$. Once $V^*$ is found, finding the optimal policy is a matter of writing $Q^*$ and defining a policy that is $Q^*$-greedy.\n",
    "\n",
    "Value Iteration was introduced by R. E. Bellman's seminal paper **[A Markovian Decision Process](https://www.jstor.org/stable/24900506)** (1957)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersVI\" class=\"collapse\">\n",
    "    Input data: $V$, $\\epsilon$<br>\n",
    "    Init: $\\Delta = \\epsilon+1$<br>\n",
    "    While $\\Delta \\geq \\epsilon$:<br>\n",
    "    &nbsp;&nbsp;&nbsp; For $s \\in S$:  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a\\in A$:  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{new}(s) = \\max_a Q(s,a)$  <br>\n",
    "    &nbsp;&nbsp;&nbsp; $\\Delta = \\| V_{new}-V \\|_\\infty$  <br>\n",
    "    &nbsp;&nbsp;&nbsp; $V = V_{new}$  <br>\n",
    "    Return $V$  <br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "    \n",
    "\n",
    "Compute and display an optimal policy for the FrozenLake game, using Value Iteration. If you copy-paste the results of exercises 1, 2 and 7, you almost don't need to write any new code.  \n",
    "Comment the obtained policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise8.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those familiar with the principles of Dynamic Programming will note that Value Iteration is a Dynamic Programming algorithm that operates in value function space, monotonically hopping from value function to value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is the time complexity of Value Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersComplexVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersComplexVI\" class=\"collapse\">\n",
    "$O(S^2 A)$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is often viewed as an algorithm that maintains a memory of a $V$ function and iterates until this function is close enough to $V^*$. Still, it is interesting (and will be useful later in the class) to rephrase the algorithm in alternate terms:  \n",
    "\n",
    "Start with a state-action value function $Q$\n",
    "- Define $\\pi(s) = \\arg\\max_a Q(s,a)$ in all states and actions.  \n",
    "  In the algorithm above, this operation is performed when the $\\max_a$ is solved,  \n",
    "  just the action is not explicitly stored.\n",
    "- $Q \\leftarrow T^\\pi Q$,  \n",
    "  that is, for all $s$ and $a$, $Q(s,a) \\leftarrow r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ Q(s',\\pi(s')) \\right]$.  \n",
    "- repeat\n",
    "\n",
    "So Value Iteration can be seen as picking the greedy action with respect to $Q$ and then updating $Q$ with just one application of $T^\\pi$. This application of $T^\\pi$ alone is not sufficient for $Q$ to reach $Q^\\pi$ but it changes $Q$ and so it changes what the greedy action will be at the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"pi\"></a>Policy Iteration and Modified Policy Iteration\n",
    "\n",
    "The Policy Iteration algorithm stems from the following remark. Suppose we have a policy $\\pi$ and know its value function $V^\\pi$ and state-action value function $Q^\\pi$. Then, the non-stationary policy $\\pi'$ that acts greedily with respect to $Q^\\pi$ for the first time step and then follows $\\pi$ has a value function $V^{\\pi'}$ that is greater or equal to $V^\\pi$ (equal if $\\pi$ is optimal, strictly greater otherwise). Actually, the contraction property of $T^*$ insures that the stationary policy $\\pi'$ that is greedy with respect to $Q^\\pi$ is at least as good as $\\pi$, that is $V^{\\pi'}\\geq V^\\pi$. Consequently, the sequence of policies defined by $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ has a monotonically improving corresponding sequence of value functions $V^{\\pi_n}$ and converges to $\\pi^*$.<br>\n",
    "<br>\n",
    "Let's make this more simple with a drawing. Policy iteration alternates two phases:\n",
    "1. Evaluate $\\pi_n$ $\\rightarrow Q^{\\pi_n}$\n",
    "2. Compute $\\pi_{n+1}$ as the $Q^{\\pi_n}$-greedy policy\n",
    "\n",
    "<img src=\"img/policyiteration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above defines a sequence of policies **and** value functions. Since, for finite state and action spaces, the number of policies is finite, Policy Iteration is guaranteed to converge in a finite number of iterations.\n",
    "\n",
    "Policy Iteration was introduced in R. A. Howard's book **[Dynamic Programming and Markov Processes](https://psycnet.apa.org/record/1961-01474-000)** (1960).\n",
    "\n",
    "Before we start implementing, let's first note that $\\pi_{n+1} = \\pi_n$ is not a valid termination criterion for Policy Iteration!\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Can you explain why? What would be a sound termination criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersPIstop\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersPIstop\" class=\"collapse\">\n",
    "    \n",
    "The improvement step of policy iteration consists in computing $\\pi_{n+1}(s) = \\arg\\max_a Q_n(s,a)$.  \n",
    "But there is no reason for the maximizing action to be unique.  \n",
    "In the case of multiple maximizing actions, the sequence of policies $\\pi_n$ could very well oscillate between several equivalent optimal policies.  \n",
    "This important lesson to keep from this is that the Bellman equation guarantees that $V^*$ is unique, but also that several policies might be optimal (and have value $V^*$).  \n",
    "    \n",
    "Also, oscillation between two quasi-equivalent policies might occur when the computation of $Q^{\\pi_n}$ carries small errors.  \n",
    "    \n",
    "\n",
    "One solution is to define a tie-breaker between equivalent actions. However, even if it is a bit more costly computationally, it might be safer to compare $V^{\\pi_{n+1}}$ and $V^{\\pi_n}$ and allow a precision error of $\\epsilon$. To observe this phenomenon, one could compare two versions of Policy Iteration: one that uses the `policy_eval_iter_mat` and one that uses the `policy_eval_lin` functions from exercice 6 of the previous class. The latter carries the errors of matrix inversion which causes the algorithm to never terminate and alternate between equivalent optimal policies.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Policy Iteration using the contraction property of $T^\\pi$.  \n",
    "For the sake if simplicity and despite the warning of the previous exercise, use $\\pi_n=\\pi_{n-1}$ as the termination condition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersPI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersPI\" class=\"collapse\">\n",
    "    Input data: $\\pi$, $V$, $\\epsilon$<br>\n",
    "    Init: $\\Delta = \\epsilon+1$<br>\n",
    "    Do:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation: apply $T^\\pi$ to $V$ until precision $\\epsilon$<br>\n",
    "&nbsp;&nbsp;&nbsp; While $\\Delta \\geq \\epsilon$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) = r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V_{old}(s')$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta = \\| V-V_{old} \\|_\\infty$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; $\\pi_{old} = \\pi$ <br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) = \\max_a Q(s,a)$<br>\n",
    "    While $\\pi_{old} \\neq \\pi$ <br>\n",
    "    Return $\\pi$  <br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise was a little convoluted. We can make things simpler.\n",
    "\n",
    "In all rigor, Policy Iteration is the algorithm that applies the Bellman evaluation operator an infinite number of times to $V$ so that it reaches $V^\\pi$, then defines $\\pi$ as the greedy policy with respect to $V$.\n",
    "\n",
    "Obviously, an infinite number of applications of $T^\\pi$ is not very practical. In the previous exercises, we controlled the error made in the convergence to $V^\\pi$ with a parameter $\\epsilon$, that allowed to perform a finite number of $T^\\pi$ applications.\n",
    "\n",
    "A simpler way is to define a certain number $m$ of applications of $T^\\pi$. This provides the **Modified Policy Iteration** algorithm, that applies $T^\\pi$ $m$ times to update $V$, then defines $\\pi$ as the greedy policy with respect to $V$.\n",
    "\n",
    "Modified Policy Iteration was introduced by M. L. Puterman and M. C. Shin in **[Modified Policy Iteration Algorithms for Discounted Markov Decision Problems](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.24.11.1127)** (1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersMPI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersMPI\" class=\"collapse\">\n",
    "    Input data: $\\pi$, $m$<br>\n",
    "    Init: $\\Delta = \\Delta_\\pi = \\epsilon+1$<br>\n",
    "    While $\\Delta_\\pi \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation, solve $V=T^\\pi V$<br>\n",
    "&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp; For $i \\in [1,m]$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) \\leftarrow (T^\\pi V)(s)$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) = \\max_a Q(s,a)$<br>\n",
    "    Return $\\pi$  <br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Modified Policy Iteration benefits from the same convergence properties as Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is Modified Policy Iteration with $m=1$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersMPIVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersMPIVI\" class=\"collapse\">\n",
    "\n",
    "According to the last remark about Value Iteration, that's exactly Value Iteration!  \n",
    "So Modified Policy Iteration is actually a continuum of algorithms between Value Iteration and Policy Iteration.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Compute and display an optimal policy for the FrozenLake game, using Modified Policy Iteration, with $m=500$ and $\\gamma=0.9$.  \n",
    "Since you will use a fixed number of iterations for the resolution of $V=T^\\pi V$, you could reuse function `policy_eval_iter_mat` from the correction of exercise 5 for instance. Recall also that exercise 1 transformed a $V$ function into a $Q$ function, and that exercise 2 picked the greedy policy from a $Q$ function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercise9.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is the time complexity of Policy Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersComplexPI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersComplexPI\" class=\"collapse\">\n",
    "$O(|S|^2 |A|)$\n",
    "    \n",
    "Policy Iteration has the same time complexity as Value Iteration. In practice, for finite state and action spaces, Policy Iteration converges in a finite number of steps (contrarily to Value Iteration). But each of these steps requires the resolution of $V = T^\\pi V$ which is where the real computational cost is.  \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"adp\"></a>Approximate Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration and Policy Iteration are the two key Dynamic Programming methods that form the basis of model-based MDP resolution.\n",
    "\n",
    "But when the state space is just too large to be enumerated (too many discrete states or continuous state space for instance), lookup table representations for $V$ and $Q$ just become impractical and it becomes necessary to turn towards function approximators.\n",
    "\n",
    "In this section, we suppose we have a *function approximation* operator or a *supervised learning* algorithm $\\mathcal{A}$. This operator $\\mathcal{A}$ transforms a function into another function: all supervised learning methods that learn from samples fall into this category, but also piecewise polynomial approximations of more complex functions for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"avi\"></a>Approximate Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the Approximate Value Iteration algorithm, which defines the sequence:\n",
    "$$V_{n+1} = \\mathcal{A} T^* V_n.$$\n",
    "\n",
    "If nothing is known about $\\mathcal{A}$, then nothing much can be said about this sequence.\n",
    "\n",
    "However, let us suppose that $\\mathcal{A}$'s approximation error is uniformly bounded, that is, for all $f \\in \\mathcal{F}(S,\\mathbb{R}), \\ \\| f-Af \\|_\\infty \\leq \\epsilon$.\n",
    "\n",
    "In this case, the first important result is that Approximate Value Iteration does not converge. However, one can prove that $V_n$ reaches a neighborhood of $V^*$. Specifically:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\| V^* - V_n \\|_\\infty \\leq \\frac{\\epsilon}{1-\\gamma}.$$\n",
    "\n",
    "More importantly:  \n",
    "Let $\\pi_n$ be the greedy policy with respect to $V_n$. Then:\n",
    "$$\\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma}{1-\\gamma} \\|V^*-V_n\\|_\\infty.$$\n",
    "\n",
    "And consequently:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "So Approximate Value Iteration does not necessarily converge but reaches policies whose values are close to optimal. These results are proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider an MDP whose rewards are all in the $[0,1]$ interval and take $\\gamma = 0.9$.  \n",
    "What is the maximum range of values for any $V^\\pi(s)$?  \n",
    "Suppose we have a function approximator whose error is uniformly upper-bounded by $\\epsilon=0.1$.  \n",
    "Compare the neighborhood size from the equation above to the largest value $V^\\pi(s)$ can take and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#AVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"AVI\" class=\"collapse\">\n",
    "\n",
    "The smallest cumulated return we can get of obviously zero.  \n",
    "The largest cumulated return is reached if we receive a reward of 1 at each time step.  \n",
    "In this case, $V^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t = \\frac{1}{1-\\gamma}$.  \n",
    "With $\\gamma = 0.9$ this upper bound is $\\frac{1}{1-\\gamma}=10$.\n",
    "\n",
    "Since we suppose $\\epsilon=0.1$, that is roughly a 1% error in function approximation.\n",
    "    \n",
    "With $\\epsilon=0.1$ we have a neighborhood radius of $\\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}=18$.  \n",
    "If we had taken $\\gamma=0.99$, the upper bound on $V^\\pi$ would have been 100 and the neighborhood radious would have been 1980.\n",
    "\n",
    "So the bound above does not really provide strong guarantees. Fortunately, in practice, most Approximate Value Iteration application behave better than this worst-case example.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most supervised learning algorithms minimize a loss that is expressed as a weighted $L_2$ norm. Thus, they don't explicitly provide guarantees in $L_\\infty$ norm. R. Munos provided **[error bounds for approximate value iteration](https://www.aaai.org/Papers/AAAI/2005/AAAI05-159.pdf)** in the general case of weighted $L_p$ norms. Those bounds are similar to the one in $L_\\infty$ norm, thus justifying the use of supervised learning techniques (such as neural networks learning from samples for instance) in Approximate Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"api\"></a>Approximate Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Policy Iteration consists in solving the evaluation equation up to a certain precision $\\epsilon$ and then taking a greedy improvement step.\n",
    "\n",
    "We write $V_n$ the approximation of $V^{\\pi_n}$ at the end of an evaluation phase in Policy Iteration and suppose that the approximation error is uniformly bounded:\n",
    "$$\\| V^{\\pi_n} - V_n \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "Then it is known that, even though the sequence of greedy policies $\\pi_n$ does not converge, it oscillates among a set of policies such that:\n",
    "$$\\lim_{n\\rightarrow \\infty} \\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "This result is proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "So the neighborhood reached has the same size as that of Approximate Value Iteration.\n",
    "\n",
    "Similar bounds **[error bounds for approximate policy iteration](https://www.aaai.org/Papers/ICML/2003/ICML03-074.pdf)** in weighted $L_p$ norms were provided by R. Munos (2003), thus justifying the use of supervised learning techniques (such as neural networks learning from samples for instance) in Approximate Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"lp\"></a>Linear Programming\n",
    "\n",
    "An alternative way of finding $V^*$ is by casting the optimality equation as a linear optimization problem. This formulation is mainly given for your curiosity and we will not study it any further.<br>\n",
    "<br>\n",
    "Recall the optimality equation:\n",
    "$$\\forall s\\in S, V(s)=\\max\\limits_{a\\in A} \\left[r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s')\\right]$$\n",
    "\n",
    "The key remark to transform this into a linear program is to rephrase it as \"$V^*$ is the smallest value that dominates over all policy values\". This can be written as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall \\pi, \\ V \\geq T^\\pi V\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "\"For all $\\pi$\" means for all possible association $s\\leftrightarrow a$, so this can be expanded as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall (s,a)\\in S\\times A, \\quad V(s) - \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V(s') \\geq r(s,a)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Which, finally, is a linear program with $|S|$ variables and $|S||A|$ constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"asyncdp\"></a>Asynchronous Dynamic Programming\n",
    "\n",
    "We have seen that Value Iteration and Policy Iteration are Dynamic Programming algorithms. They follow a path, respectively in value function and in policy space that leads to $V^*$ and $\\pi^*$. But we can remark that they both perform *state-wise* operations such as:\n",
    "\n",
    "- $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $V(s) \\leftarrow \\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $\\pi(s) \\leftarrow \\arg\\max_a Q^{\\pi}(s,a)$\n",
    "- $V(s) \\leftarrow r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$\n",
    "\n",
    "These state-wise operations are called Bellman backups.\n",
    "\n",
    "Let's use $V$ functions to describe Value Iteration. We can define the Bellman backup operator:\n",
    "- `BBV(V,s): return` $\\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$  \n",
    "  `BBV` is the operation performed in every state, in one pass of Value Iteration\n",
    "\n",
    "Alternatively, we can operate only on $Q$ functions and define the corresponding Bellman backup operators:\n",
    "- `BBQ(Q,pi,s,a): return` $r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) Q(s',\\pi(s'))$  \n",
    "  `BBQ` serves in all *evaluation* steps,\n",
    "- `BBpi(Q,s): return` $\\max_a Q(s,a)$ and $\\arg\\max_a Q(s,a)$  \n",
    "  `BBpi` serves in all *improvement* steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `V`.\n",
    "Using `BBV`, rewrite Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersbackupsVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersbackupsVI\" class=\"collapse\">\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      W(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `Q` and a policy `pi`. \n",
    "Using `BBQ` and `BBpi`, rewrite Value Iteration and Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersbackups\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersbackups\" class=\"collapse\">\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      Qnew(s,a) = BBQ(Q,pi,s,a)\n",
    "    W(s), pi(s) = BBpi(Qnew,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "<br><br>\n",
    "Modified Policy Iteration:<br><code>\n",
    "while(pi not constant)\n",
    "  Q(s,a) = 0 for all s,a\n",
    "  while error>epsilon\n",
    "    for k in [1,m]\n",
    "      for s,a in SxA\n",
    "        Qnew(s,a) = BBQ(s,a)\n",
    "  for s in S\n",
    "    V, pi = BBpi(s)</code>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"asyncvi\"></a>Asynchronous Value Iteration\n",
    "\n",
    "Let's take the pseudo-code of Value Iteration using a $V$ function from the exercises above. Why don't we perform directly `V(s) = BBV(V,s)`, instead of relying on the intermediate `W` function? In other terms, if we have already performed a backup in $s$, why couldn't we reuse it in the next backup? Doing so is actually called **Gauss-Seidl Value Iteration** and it opens the door to a much wider class of algorithms called Asynchronous Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Using `BBV`, write Gauss-Seidl Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersbackupsGSVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersbackupsGSVI\" class=\"collapse\">\n",
    "    \n",
    "\n",
    "Value Iteration:<br><code>\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      V(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W</code>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to note that in Gauss-Seidl Value Iteration, the order in which the states are considered for backups greatly affects of rewards are propagated through the state space and how the sequence of value functions converges to $V^*$.  \n",
    "\n",
    "But still, in Gauss-Seidl Value Iteration, states are updated once per sweep over the state space.\n",
    "Why wouldn't we update the value of some states more often than others? Would the overall value function still converge to $V^*$? A very powerful theorem actually states what follows.\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Convergence of Asynchronous Value Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `V(s)` $\\leftarrow$ `BBV(V,s)` operation as time tends to $+\\infty$, the value function $V$ converges to $V^*$\n",
    "</div>\n",
    "\n",
    "Consequently, we could pick states totally randomly in order to perform Bellman backups on $V$, and $V$ would still converge to $V^*$. Although picking states randomly for that purpose seems like a bad idea, identifying a good ordering for the backups can lead to drastic improvements in convergence speed. This is the key idea of **Asynchronous Value Iteration** and has justified (among other things) the popular **Prioritized Sweeping** and **Real-Time Dynamic Programming** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"asyncpi\"></a>Asynchronous Policy Iteration\n",
    "\n",
    "Let's now take the pseudo-code of Modified Policy Iteration from the exercises above, using `BBQ` and `BBpi`. The evaluation step and the improvement step are clearly separated. But we know already that if we require the evaluation step to have infinite precision, $m$ needs to tend to $\\infty$. We also know that if we take an arbitrary value for $m$ and the algorithm still converges.\n",
    "\n",
    "Can we write introduce the idea of asynchronous Bellman backups in Policy Iteration? As in the value iteration case, can we update the value or policy of a given state in any ordering? Our most general theorem for Asynchronous Dynamic Programming in MDP states the following.\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "**Convergence of Asynchronous Policy Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `Q(s,a)` $\\leftarrow$ `BBQ(Q,pi,s,a)`  and the `pi(s)` $\\leftarrow$ `BBpi(Q,s)` operations as time tends to $+\\infty$, the value function $Q$ and the policy $\\pi$ converge respectively to $Q^*$ and $\\pi^*$\n",
    "</div>\n",
    "\n",
    "That is the most general framework one can give for **Asynchronous Dynamic Programming** in MDP resolution. It is often called **Asynchronous Policy Iteration**.<br>\n",
    "\n",
    "Overall, Asynchronous Policy Iteration can be written:\n",
    "`\n",
    "Do forever:\n",
    "  Pick a set SAset={(s,a)}\n",
    "  For s,a in SAset:\n",
    "    Q(s,a) = BBQ(Q,pi,s,a)\n",
    "  Pick a set Sset={s}:\n",
    "  For s in Sset:\n",
    "    pi(s) = BBpi(Q,s)\n",
    "`\n",
    "\n",
    "So Asynchronous Policy Iteration encompasses all the previous algorithms, both synchronous (VI, PI, MPI) and asynchronous (Asynchronous VI).\n",
    "\n",
    "Of course, just as for Asynchronous Value Iteration, the most important thing with Asynchronous Policy Iteration is the order in which we pick the states and actions for backups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
