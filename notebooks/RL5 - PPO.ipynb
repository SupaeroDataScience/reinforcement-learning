{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a383daae-85ab-460b-9d9d-868d9520f14d",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 5: Proximal Policy Optimization (PPO) and Vectorized Environments</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dff0d7-861f-4d1a-a5bb-8c45bd0d287d",
   "metadata": {},
   "source": [
    "1. <a href=\"#reinforce\">The REINFORCE Algorithm</a>\n",
    "2. <a href=\"#trpo\"> Trust Regions Policy Optimization (TRPO)</a>\n",
    "3. <a href=\"#ppo\">Proximal Policy Optimization (PPO)</a>\n",
    "4. <a href=\"#environments\">Environments</a>\n",
    "5. <a href=\"#your_turn\">Practice: your turn!</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58025c6d-606a-435b-b7b5-5342d90fa074",
   "metadata": {},
   "source": [
    "# <div id=\"reinforce\"></div> The REINFORCE Algorithm\n",
    "\n",
    "### Reminder on the PG Theorem\n",
    "\n",
    "In the previous notebook, we've seen methods to directly optimize a given policy $\\pi$ instead of deducing it through the optimal Q-value function $Q^*$.\n",
    "For a policy $\\pi$, we wrote the policy optimization's objective as\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim p_0} \\left[ V^{\\pi} (s) \\right] = \\mathbb{E}_{(s_i,a_i)_{i \\in [0,\\infty]}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi \\right].$$\n",
    "\n",
    "We then introduced the PG Theorem that allows us to write the gradient of this objective:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) \\nabla_\\theta \\log\\pi_\\theta(a|s)\\right]$$\n",
    "with $R(\\tau) = \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)$.\n",
    "\n",
    "Intuitively, we said that the $\\nabla_\\theta \\log\\pi(a | s)$ term \"selects\" the part of the network that is responsible for taking the action $a$ in the state $s$, and the sum of rewards $R(\\tau)$ indicates whether to encourage or discourage this action $a$.\n",
    "In other words, if a trajectory in the environment results in a good return, we encourage all the actions that were taken during the corresponding episode, i.e. we increase the probability that they're selected.\n",
    "\n",
    "This works, but it ignores that an action can only influence the future: if we received a high reward *before* we took an action $a_t$ in our trajectory, this alone should not impact the probability of taking $a_t$.\n",
    "To fix this, instead of the total return $R(\\tau)$, we can use the **reward-to-go** $\\hat{R}_t(\\tau) = \\sum_{k=t}^\\infty \\gamma^{t - k} r(s_k, a_k)$ which gives us:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\hat{R}_t(\\tau) \\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\\right]$$\n",
    "Using the reward-to-go decreases the noise on the gradient estimation, and thus leads empirically to better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b471a5a-f914-4342-9eed-03711cea7d96",
   "metadata": {},
   "source": [
    "## The REINFORCE algorithm\n",
    "\n",
    "Let's rewind and try to apply this theorem directly in its simplest form: the **REINFORCE** algorithm\n",
    "1. Initialize the policy parameters $\\theta$ randomly.\n",
    "2. Collect a batch of trajectories $D = \\{\\tau_1, \\tau_2, \\ldots, \\tau_N\\}$ by running the policy in the environment.\n",
    "3. Compute an estimate of the policy gradient, $\\hat{d}$:\n",
    "    * The policy gradient is given by:\n",
    "        $$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\hat{R}_t(\\tau_i) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right]$$\n",
    "    * We estimate it using Monte Carlo sampling:\n",
    "        $$\\hat{d} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\hat{R}_t(\\tau_i) \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})$$\n",
    "4. Update the policy using gradient ascent:\n",
    "    $$\\theta \\leftarrow \\theta + \\alpha \\hat{d}$$\n",
    "5. Repeat steps 2-4 until convergence.\n",
    "\n",
    "This algorithm introduced [in 1992 by Ronald Williams](https://link.springer.com/article/10.1007/BF00992696) is simple and easy to implement, but it presents two main flaws:\n",
    "1. The policy gradient estimation has high variance as we use Monte Carlo sampling\n",
    "2. The algorithm is on-policy, which means that we can only use a trajectory once to improve the policy and we need to collect new trajectories after each update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97036d60-e5e1-4ccd-9ff8-8aad3a6e6caa",
   "metadata": {},
   "source": [
    "## Reducing the gradient estimation variance\n",
    "\n",
    "Let's rewrite the policy gradient.\n",
    "We can show that by subtracting a baseline $b(s)$, we can significantly shrink the variance of our updates without changing the gradient.\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\nabla_\\theta \\log\\pi(a|s) ( Q^\\pi(s, a) - b(s) ) \\right]$$\n",
    "For this, we need to show that the baseline term does not modify the expectation computation:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) b(s) \\right] &= b(s) \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] && b \\text{ does not depend on the action} \\\\\n",
    "&= b(s) \\sum_{a} \\pi_\\theta(a|s) \\frac{\\nabla_\\theta \\pi_\\theta(a|s)}{\\pi_\\theta(a|s)}  && \\text{log-derivative trick} \\\\\n",
    "&= b(s) \\nabla_\\theta \\sum_{a} \\pi_\\theta(a|s) && \\text{gradient linearity} \\\\\n",
    "&= b(s) \\nabla_\\theta (1) && \\text{probabilities sum to 1} \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To see why adding a baseline can reduce the variance, imagine an environment in which rewards are always high (e.g. between 990 and 1010).\n",
    "In this case, $Q^\\pi(s, a)$ will be high and the gradient will be huge: a tiny difference in performance will lead to massive swings in the weight updates.\n",
    "By setting the baseline to 1000, we only modify the weights of the network depending on whether the outcome was better or worse than expected.\n",
    "\n",
    "In practice, we usually use the Value function $V^\\pi(s)$ as baseline.\n",
    "Let's define the **Advantage function** $$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$$\n",
    "Intuitively, the advantage $A^\\pi(s, a)$ represents how much better the action $a$ is compared to the average.\n",
    "We can then write:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\nabla_\\theta \\log\\pi(a|s) A^\\pi(s, a) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fac130-110f-45cf-b5e9-b34b4532f2ca",
   "metadata": {},
   "source": [
    "## Using efficiently collected trajectories\n",
    "\n",
    "As REINFORCE is **on-policy**, we can only use each trajectories once for computing the policy gradient.\n",
    "This is highly inefficient and greatly limit the capacity to learn in complex environments such as robotics systems.\n",
    "\n",
    "To mitigate this, a first approach would be to increase our algorithm's learning rate to take larger steps at each update.\n",
    "However, a greater learning rate would make the optimization process unstable.\n",
    "In Supervised Learning, taking gradient steps too large might result in the loss jumping up temporarily, but the data distribution remains fixed.\n",
    "In RL, the trajectory distribution is given by the policy $\\pi$ so there is a supplementary risk: if the update step is too large, $\\pi_{\\theta_{new}}$ might be a terrible policy (e.g. the robot falls over immediately).\n",
    "Because the policy leads to bad states, the next trajectory will also be uninformative (i.e. only containing states where the robot is lying on the ground).\n",
    "The agent then learns from this data, reinforcing failure and often never recovers: this is known as **performance collapse**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326230cf-40b6-4eeb-bb5f-647cb2f28688",
   "metadata": {},
   "source": [
    "# <div id=\"trpo\"></div> Trust Regions Policy Optimization (TRPO)\n",
    "\n",
    "To solve this problem, Schulman et al. introduced [**Trust Region Policy Optimization (TRPO)** in 2015](https://arxiv.org/abs/1502.05477).\n",
    "The problem is that the gradient steps operate in the **parameter space** (the weights of the neural network).\n",
    "But the relationship between the weights $\\theta$ and the resulting policy $\\pi_\\theta$ is highly non-linear.\n",
    "A very small change in the weights $\\theta$ can result in a massive change in the policy distribution $\\pi_\\theta$ (and thus the agent's behavior).\n",
    "\n",
    "Instead of limiting how much we change the *parameters* (which is hard to tune), TRPO suggests we limit how much we change the **policy distribution**.\n",
    "We define a **Trust Region** around the current policy where we believe the update is safe.\n",
    "But how do we define how much one policy's behavior differs from another?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc03596-a9ba-4e80-91c4-663bef09b253",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Kullback-Leibler divergence\n",
    "\n",
    "Recall that the output of a policy is a probability distribution over actions.\n",
    "For discrete action spaces, this is a categorical distribution, and for continuous action spaces, this is a multivariate Gaussian distribution.\n",
    "\n",
    "This means that a natural way to measure the difference between two policies is to measure the difference between their output distributions.\n",
    "This is where the **Kullback-Leibler divergence** comes in.\n",
    "The KL divergence is a measure of how different two probability distributions are.\n",
    "\n",
    "For two probability distributions $P$ and $Q$ over some set $X$, it is defined as:\n",
    "$$D_{KL}(P||Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "We won't go into too much depth here since the exact details of KL divergence are not relevant to our understanding, but some of the more important points to note are:\n",
    "1. The KL divergence is a measure of relative entropy: it measures how \"surprised\" we are if we use distribution $Q$ to model distribution $P$\n",
    "2. The KL divergence is not symmetric, i.e. $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127d61-1d43-4d8c-8fbd-1ba17700201b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TRPO Objective\n",
    "\n",
    "TRPO formulates the update as a constrained optimization problem:\n",
    "$$\\max_\\theta \\mathbb{E}_t \\left[ \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)} A^{\\pi_{old}}(s_t, a_t) \\right]$$\n",
    "$$\\text{subject to } \\mathbb{E}_t \\left[ D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s_t) || \\pi_\\theta(\\cdot|s_t)) \\right] \\le \\delta$$\n",
    "where $\\delta$ is a hard threshold (e.g., 0.01) defining the maximum allowed change in the policy.\n",
    "\n",
    "The probability ratio $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$ (not to be confused with the reward function) is called the **importance sampling ratio**: it is used to correct for the fact that we are using data collected under an old policy to optimize the new policy.\n",
    "\n",
    "This objective $\\max_\\theta \\mathbb{E}_t \\left[ r_t(\\theta) A^{\\pi_{old}}(s_t, a_t) \\right]$ is called the **Surrogate Objective** as it uses data collected by the old policy to *guess* how the new policy will perform.\n",
    "It essentially says: \"If the old policy found that action $a$ was good (high advantage), and the new policy is now $10\\%$ more likely to take action $a$, then the new policy's performance will probably be roughly $10\\%$ of that advantage better.\"\n",
    "Intuitively, this means that we want to increase the probability of actions that have high advantage and decrease the probability of actions with low (negative) advantage.\n",
    "\n",
    "However, if the new policy differs too much from the old policy, the importance sampling ratio becomes high-variance (we now take actions almost never seen before) and the states visited under the policy change, driving the surrogate estimation further from the true policy performance.\n",
    "That's why TRPO adds the KL constraint to force the update to not drastically modify the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937a1c9-c493-4fdd-9aa3-88c2f478765c",
   "metadata": {},
   "source": [
    "### TRPO in practice\n",
    "\n",
    "What we've described above is the *theoretical TRPO update*, but in practice we do not actually use this update rule directly as the true objective function is too expensive to compute exactly.\n",
    "Instead, we use a first-order approximation of the objective function involving the Hessian matrix of the KL divergence, and then solve the optimization problem using a [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) algorithm.\n",
    "\n",
    "We won't delve too much into the details of this, but if you want to know more, you can read the [OpenAI Spinning Up documentation](https://spinningup.openai.com/en/latest/algorithms/trpo.html)\n",
    "\n",
    "While theoretically grounded, TRPO is difficult to use in practice:\n",
    "1.  **Computationally Expensive:** Enforcing the KL constraint strictly requires calculating second-order derivatives (the Hessian matrix), which is very slow for large neural networks (for a network with $n$ parameters, the Hessian is of size $n \\times n$)\n",
    "2.  **Complex Implementation:** It requires using the Conjugate Gradient algorithm instead of standard optimizers like Adam or SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d8a6d-20f4-4608-9fe9-c1dfec348550",
   "metadata": {},
   "source": [
    "# <div id=\"ppo\"></div> Proximal Policy Optimization (PPO)\n",
    "\n",
    "Our goal is thus to prevent the new policy obtained after an update from being too different from the old policy.\n",
    "To achieve this, whereas TRPO adds a constraint to the optimization problem, we can take a different approach: changing the objective function so that there's no incentive to change the policy too much in one update.\n",
    "That is the approach used in the [**Proximal Policy Optimization** paper, also written by Schulman et al. in 2017](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b73128-53d2-4f60-baa8-355e2bc52d0a",
   "metadata": {},
   "source": [
    "### PPO policy objective\n",
    "\n",
    "The policy objective of PPO, called the **Clipped Surrogate Objective**, is:\n",
    "$$\\mathcal{L}^{CLIP}(\\theta, \\theta_{\\text{old}}) = \\mathbb{E}_{s \\sim \\rho_{\\theta_{\\text{old}}}, a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\min \\left( \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} A^{\\pi_{\\theta_{\\text{old}}}}(s, a), \\text{clip} \\left( \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) A^{\\pi_{\\theta_{\\text{old}}}}(s, a) \\right) \\right] = \\mathbb{E}_{s \\sim \\rho_{\\theta_{\\text{old}}}, a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ l(s, a) \\right] $$\n",
    "with $\\epsilon$ an hyperparameter, usually set to a small value like $0.2$.\n",
    "\n",
    "This raw expression can be scary, but let's break it down in small pieces to understand it better.\n",
    "The first term of the min is the same as in TRPO: it is the surrogate objective $\\left[ r_t(\\theta) A^{\\pi_{\\theta_{old}}}(s, a) \\right]$\n",
    "The second term can be broken down into two cases:\n",
    "- when $A^{\\pi_{\\theta_{old}}}(s, a) \\geq 0$, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "l(s, a) &= A^{\\pi_{\\theta_{old}}}(s, a) \\min \\left( r_t(\\theta), \\text{clip} (r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\right) \\\\\n",
    "&= A^{\\pi_{\\theta_{old}}}(s, a) \\min \\left( r_t(\\theta), 1 + \\epsilon \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "In this case, as the advantage is positive, we want to increase the probability to take the action $a$, but in the limit of $20\\%$ more than previously not to get too greedy.\n",
    "If the importance sampling ratio becomes greater than $1 + \\epsilon$, we then have $l(s, a) = (1 + \\epsilon) A^{\\pi_{\\theta_{old}}}(s, a)$ which does not depend on $\\theta$, so the gradient is null and the policy is not updated.\n",
    "- when $A^{\\pi_{\\theta_{old}}}(s, a) \\lt 0$, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "l(s, a) &= A^{\\pi_{\\theta_{old}}}(s, a) \\max \\left( r_t(\\theta), \\text{clip} (r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\right) \\\\\n",
    "&= A^{\\pi_{\\theta_{old}}}(s, a) \\max \\left( r_t(\\theta), 1 - \\epsilon \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "In this case, the advantage is negative so we want to decrease the probability to take the action $a$ still in the limit of $20\\%$ not to differ too much from the previous policy.\n",
    "If the importance sampling ratio becomes lower than $1 - \\epsilon$, we then have $l(s, a) = (1 - \\epsilon) A^{\\pi_{\\theta_{old}}}(s, a)$ which also leads to a null gradient.\n",
    "\n",
    "Let's visualize this behavior (directly from the PPO paper):\n",
    "<center><img src=\"img/PPO_clip_objective.png\"></center>\n",
    "\n",
    "An important remark is that this clipped objective **does not guarantee** that the KL divergence between the old and new policies will stay small, and it is still possible to end up with a new policy which is too far from the old one.\n",
    "In practice, the clipped objective is usually enough, but if needed it is still possible to control this using a smaller $\\epsilon$ or using simple method like early stopping: if the mean KL-divergence grows beyond a threshold, we stop taking gradient steps.\n",
    "\n",
    "Nevertheless, because the clipping prevents the policy from diverging too fast, the main benefit is that we can **perform multiple epochs of gradient descent on a same batch of data** and thus mitigate the fact that the algorithm is on-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff76ebb-2e78-405e-9a56-45c5a68505fe",
   "metadata": {},
   "source": [
    "### Generalized Advantage Estimation (GAE)\n",
    "\n",
    "In order to compute the clipped objective, we still need a way to compute the advantage.\n",
    "As evoked in the original paper, any method that approximates the advantage can be used here, but in practice PPO implementations usually use the **Generalized Advantage Estimation** equation.\n",
    "This formula established [in a paper from 2015 still by Schulman et al.](https://arxiv.org/abs/1506.02438) defines a family of advantage estimators that can be easily computed as a discounted sum of Bellman TD-errors:\n",
    "$$\\hat{A}^{GAE(\\gamma, \\lambda)}(s_t, a_t) = \\sum_{k=0}^\\infty (\\gamma \\lambda)^k \\delta_{t + k}$$\n",
    "with $\\delta_t$ the TD-error $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ and $\\lambda \\in (0, 1)$ an hyperparameter that controls the bias-variance tradeoff usually set at $0.95$.\n",
    "\n",
    "To compute the advantage in practice, we thus need an approximation of the value function $V$ which can be represented as usual using a deep neural network.\n",
    "To train this network, we can just learn through supervised learning on the return-to-go i.e. $L^{VF}(\\theta) = \\text{MSE} (V(s_t), \\hat{R}_t)$.\n",
    "\n",
    "However, as for REINFORCE, computing the return-to-go from the sampled trajectories has high variance, so we usually approximate it using the generalized advantage estimation:\n",
    "$$\\hat{R}_t \\approx V(s_t) + \\hat{A}^{GAE}(s_t, a_t)$$\n",
    "$$L^{VF}(\\theta) = \\text{MSE} \\left( V_\\theta(s_t), \\hat{R}_t \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380def86-7b04-4a93-87c9-a89a9c856ea9",
   "metadata": {},
   "source": [
    "### Total PPO loss\n",
    "\n",
    "Finally, in order to ensure sufficient exploration, we add an entropy bonus to the total PPO loss defined as in SAC $H(\\pi_\\theta) = -\\int p(x) \\log (p(x))dx$.\n",
    "\n",
    "This gives us the following final expression:\n",
    "$$L^{PPO}(\\theta) = -\\mathbb{E} \\left[ L^{CLIP}(\\theta) - c_1 L^{VF}(\\theta) + c_2 H(\\pi_\\theta(s_t)) \\right]$$\n",
    "with $c_1$ and $c_2$ coefficients to control the weights of each terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bd44d-033c-4801-9cc6-83310bdbc3c1",
   "metadata": {},
   "source": [
    "### PPO Pseudocode\n",
    "\n",
    "The final pseudocode for PPO is the following (from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html)):\n",
    "<center><img src=\"https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg\"></center>\n",
    "\n",
    "Here, <img src=\"https://spinningup.openai.com/en/latest/_images/math/39f524858866b80e627840ba77a54360e3bac55e.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c2e44-5e9c-4cf3-b38b-8391f685baa2",
   "metadata": {},
   "source": [
    "#  <div id=\"environments\"></div> Environments\n",
    "\n",
    "Your objective in the rest of the notebook will be to implement the PPO algorithm on various simplified robotics environment.\n",
    "\n",
    "### MuJoCo library\n",
    "\n",
    "From [the Gymnasium documentation](https://gymnasium.farama.org/environments/mujoco/):\n",
    "> MuJoCo stands for Multi-Joint dynamics with Contact. It is a physics engine for facilitating research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed. There is physical contact between the robots and their environment - and MuJoCo attempts at getting realistic physics simulations for the possible physical contact dynamics by aiming for physical accuracy and computational efficiency.\n",
    "\n",
    "Although we'll use it for simple robotics environments, this library is also used in research labs and universities to model real and complex hardware.\n",
    "\n",
    "Let's look at a classic example: the `Ant-v5` environment.\n",
    "<center><img src=\"https://gymnasium.farama.org/_images/ant.gif\" width=\"30%\"></center>\n",
    "\n",
    "Despite its name, it looks more like a four-legged robotic torso.\n",
    "The action space is continuous of dimension 8, representing the torque applied to each of the 8 hinge joints.\n",
    "The state space has 105 dimensions corresponding to the positions, velocities and external forces applied on each of the robot body parts.\n",
    "The agent is rewarded for moving forward as fast as possible, with a few penalties to prevent it from learning undesirable behaviors.\n",
    "\n",
    "You can read the exact environment definition and more information on [the environment documentation](https://gymnasium.farama.org/environments/mujoco/ant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a79517-d2c0-4a0a-a7e1-ee45f7cb926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Ant-v5\", render_mode=\"human\")\n",
    "\n",
    "_, _ = env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample() \n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb5230-84f3-477a-90b1-66ffdd3a9a33",
   "metadata": {},
   "source": [
    "### Vectorized environments\n",
    "\n",
    "In Reinforcement Learning, the biggest bottleneck in terms of training speed is generally interactions with the environment as it usually needs to calculate physics, collisions or game logic between each steps.\n",
    "\n",
    "As we've seen previously, for off-policy algorithms each collected sample can be stored in a Replay Buffer and reused continuously when computing the loss function, which improves sample efficiency and mitigates the need for very fast environments.\n",
    "However in the case of on-policy algorithms, each sample can be used only a few times at most, making the environment interaction speed the limiting factor.\n",
    "\n",
    "To fix this, two concurrent approaches exist:\n",
    "1. Accelerate the environment by running it on GPU (libraries like IsaacSim, Brax, Genesis...)\n",
    "2. Run multiple environments in parallel\n",
    "\n",
    "These two methods can be used simultaneously, and libraries that implement environments on GPU are almost always able to run multiple of them at the same time.\n",
    "This second approach is usually implemented using **vectorized environments**.\n",
    "\n",
    "A vectorized environment is simply a wrapper that runs multiple independent instances of the same environment simultaneously.\n",
    "Instead of sending one action and getting one state, we send it a batch of actions and receive a batch of states.\n",
    "This method presents several benefits:\n",
    "- **data diversity:** in one update, our algorithm sees experiences from diverse starting positions or random seeds. This prevents the agent from \"overfitting\" to a specific lucky trajectory\n",
    "- **less noisy estimations:** by averaging gradients over many parallel trajectories, the noise in the advantage estimation is naturally smoothed out\n",
    "- **hardware efficiency:** as the actor and critic networks are usually run on GPU, batches of data are processed in parallel making the forward pass almost as efficient as when processing only one environment\n",
    "\n",
    "The Gymnasium library makes vectorization very simple through the `AsyncVectorEvn` interface which runs environments in separate CPU processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697d3a7-fe04-421c-90e4-02f996849859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "num_envs = 16\n",
    "envs = gym.make_vec(\"Ant-v5\", num_envs=num_envs)\n",
    "\n",
    "obs, _ = envs.reset()\n",
    "print(f\"Observation batch shape: {obs.shape}\") \n",
    "\n",
    "actions = envs.action_space.sample() \n",
    "print(f\"Action batch shape: {actions.shape}\")\n",
    "\n",
    "_, rewards, terminations, _, _ = envs.step(actions)\n",
    "print(f\"Rewards received: {rewards}\")\n",
    "print(f\"Terminations received: {terminations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fa26a-2ec8-4269-84c5-7eddfd298401",
   "metadata": {},
   "source": [
    "One important feature of gymnasium vectorized environments is **auto-resetting**.\n",
    "In a single environment, when an agent dies it must be manually reset by calling `env.reset()`.\n",
    "In a vectorized environment, if an agent dies but the others are still alive, the wrapper automatically resets the dead agent and puts the new starting state into the next observation batch.\n",
    "This allows the training loop to run indefinitely without ever stopping to check if an episode finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da99ec-7c09-4768-a192-eaa46fb7e137",
   "metadata": {},
   "source": [
    "# <div id=\"your_turn\"></div> Your turn!\n",
    "\n",
    "Although PPO is the most used RL algorithm in the literature and in the industry today, it is far from simple and understanding all implementation details is quite tricky.\n",
    "Moreover, as often in RL the performance of any algorithm implementation is usually very dependent on many different tricks that are not always described in the original papers.\n",
    "\n",
    "Your task now is to implement the PPO algorithm for a vectorized MuJoCo robotics environment such as Ant, Half-Cheetah or Humanoid.\n",
    "The objective is that you apply everything we saw in previous notebooks on how to implement RL algorithms, combined with the theory of PPO developped in this notebook.\n",
    "The more you code on your own, the more you'll learn so try to implement the general RL training loop and PPO optimization steps with the minimum help possible!\n",
    "\n",
    "Here are some great resources to help you in your task:\n",
    "- [the original PPO paper](https://arxiv.org/abs/1707.06347)\n",
    "- [OpenAI SpinningUp PPO documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n",
    "- [CleanRL PPO implementation](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py): a single-file clean PPO implementation which focuses on understanding\n",
    "- [A great ICLR 2022 blogpost](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) about PPO important implementation details\n",
    "\n",
    "This last link in particular is a great resource that discusses every little choices and common errors found in different public implementations of the algorithm.\n",
    "You don't need to follow every one of their recommendations, but reading about them will help you understand PPO better.\n",
    "It also comes with video tutorials if you prefer.\n",
    "\n",
    "You will also find countless blog posts, Youtube videos, [academic papers](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf), Github repositories and other resources all focused on explaining PPO, but be aware that they're not always of the best quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd11562-a5fb-4f3f-8cf5-19d280b6c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the PPO algorithm on a vectorized MuJoCo environment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
