{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 1: Markov Decision Processes**\n",
    "\n",
    "Notation: MDP = Markov Decision Process\n",
    "1. [Everything you need to know](#everything)\n",
    "2. [A gentle introduction](#intro)\n",
    "    1. [A medical prescription example](#example)\n",
    "    2. [Patient variables](#vars)\n",
    "    3. [Prescription](#prescription)\n",
    "    4. [Patient evolution](#evolution)\n",
    "    5. [Physician's goal](#goal)\n",
    "    6. [Wrap-up](#wrapup)\n",
    "3. [Modeling sequential decision problems with MDPs](#modeling)\n",
    "    1. [Definition](#mdp)\n",
    "    1. [My first Markov Decision Processes](#frozenlake)\n",
    "    2. [Some Gym notions](#gym)\n",
    "    3. [Policies](#policies)\n",
    "    4. [Value Functions](#values)\n",
    "    5. [Optimal policies](#optimal)\n",
    "    6. [Evaluation equation](#eval)\n",
    "    7. [Optimality equation](#optimality)\n",
    "    8. [Summary](#summary)\n",
    "4. [Before you leave](#before)\n",
    "    1. [Limitations](#limits)\n",
    "    2. [A common misconception](#misconception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous class (RL0) provided the key intuitions about RL. RL is about **learning** to **control** dynamic systems. This class will present the family of systems that we build the RL theory upon. They are called Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"everything\"></a>Everything you need to know\n",
    "\n",
    "\n",
    "Everything you should remember after this session.\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> A Markov Decision Process (MDP) is a 4-tuple $\\langle S,A,p,r \\rangle$. $S$ is the state space, $A$ is the action space, $p(s'|s,a)$ the transition model and $r(s,a)$ the reward model.\n",
    "<li> A policy is a mapping $\\pi:S\\rightarrow A$ that specifies what to do in a given state.\n",
    "<li> Value function of a policy: $V^\\pi(s)=\\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H \\gamma^t r_t \\bigg| s_0 = s, \\pi \\right)$\n",
    "<li> State-action value function of a policy $Q^\\pi(s,a) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H \\gamma^t r_t \\bigg| s_0 = s, a_0=a, \\pi \\right)$\n",
    "<li> Optimal policy $\\pi^*$ dominates every other possible policy in every state: $\\forall s, \\ V^*(s) = V^{\\pi^*}(s) \\geq V^\\pi(s)$\n",
    "<li> Evaluation equation: $V^\\pi \\in \\mathcal{F}\\left(S,\\mathbb{R}\\right)$ is the only solution to $V\\left(s\\right) = (T^\\pi V)(s) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)$<br>\n",
    "Similarly, $Q^\\pi \\in \\mathcal{F}\\left(S\\times A,\\mathbb{R}\\right)$ is the only solution to $Q\\left(s,a\\right) = (T^\\pi Q)(s,a) = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q\\left(s', \\pi\\left(s'\\right)\\right)$\n",
    "<li> Properties of $T^\\pi$ 1) $V=T^\\pi V$ is a linear system, 2) $T^\\pi$ is a contraction mapping over $\\mathcal{F}(S,\\mathbb{R})$ or $\\mathcal{F}(S\\times A,\\mathbb{R})$.\n",
    "<li> Finding $V^\\pi$ or $Q^\\pi$: by matrix inversion or by repeatedly applying $T^\\pi$ to any initial function.\n",
    "<li> Optimality equation: $V^* \\in \\mathcal{F}\\left(S,\\mathbb{R}\\right)$ is the only solution to $V(s) = (T^* V) (s) = \\max\\limits_{a\\in A} \\left\\{ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s') \\right\\}$<br>\n",
    "Similarly, $Q^* \\in \\mathcal{F}\\left(S\\times A,\\mathbb{R}\\right)$ is the only solution to $Q(s,a) = (T^* Q) (s,a) = r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q(s',a')$\n",
    "<li> $T^*$ is a contraction mapping.\n",
    "<li> An OpenAI Gym environment has a standard API with 5 methods (step, reset, render, close, seed) and 3 attributes (action_space, observation_space, reward_range). Environment-specific features can be accessed via the \"unwrapped\" attribute.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Of course, all this seems very obscure right now and the block above will only serve as a reminder when you re-open the notebook later. We will introduce every concept intuitively and progessively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"intro\"></a>A gentle introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"example\"></a>A medical prescription example\n",
    "\n",
    "<img src=\"img/patient-doctor.png\" style=\"height: 200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A patient walks into a clinic with her medical file (medical history, x-rays, blood work, etc.). You, as her doctor, need to write a prescription. Let us use this example to formalize the process of deciding what to write on the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:** take a minute to formalize the problem of writing a prescription.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercice is open-ended: there is no single right answer. But before we move any further it is good to try to be a bit creative. Feel free to name variables, decisions, history of the patient to write the problem of writing this prescription. The goal here is to do it your way, not the RL way. If they match, it's all the better, but it's not necessary, so feel free to be creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"vars\"></a>Patient variables\n",
    "\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "<center>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states: $S_t$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medical file of the patient allows us to define a number of variables that characterize the patient now. We will write $S_0$ the vector of these variables. Future measurements will be noted $S_t$.\n",
    "\n",
    "$S_t$ is a random vector, taking different values in a *patient description space* $S$ at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Question:** what kind of variables are there in $S_t$? Scalars? Booleans? Images? Time series?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"prescription\"></a>Prescription\n",
    "\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "<center>\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prescription is a series of recommendation we give to the patient over the course of treatment. It is thus a sequence $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$ of variables $A_t$.\n",
    "\n",
    "These treatments $A_t$ are random variables too, taking their value in some space $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Question:** what kind of variables are there in $A_t$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"evolution\"></a>Patient evolution\n",
    "\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "<center>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patient evolves over time steps. Her evolution follows a certain probability distribution $\\mathbb{P}(S_t)$ over descriptive states.\n",
    "\n",
    "So $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ defines a *random process* that describes the patient's evolution under the influence of past $S_t$ and $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"goal\"></a>Physician's goal\n",
    "\n",
    "<img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physician's goal is to bring the patient from an unhealthy state $S_0$ to a healthy situation. This goal is not only defined by a final state of the patient but by the full trajectory followed by the variables $S_t$ and $A_t$. For example, prescribing a drug that damages the patient's liver, or letting the patient experience too much pain over the course of treatment is discouraged.\n",
    "\n",
    "We define a criterion $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ that allows to quantify how good a trajectory in the joint $S\\times A$ space is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Question:** how would you write this function explicitly?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"wrapup\"></a>Wrap-up\n",
    "\n",
    "- Patient state $S_t$  (random variable)\n",
    "- Physician instruction $A_t$ (random variable)\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$   \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$  \n",
    "- Patient's trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"modeling\"></a>Modeling sequential decision problems with Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"mdp\"></a>Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for our patient. We wish to control the trajectory of a system that we model as a Markov Decision Process.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 240px;\"></img>\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A set of states $S$\n",
    "<li> A set of actions $A$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<a href=\"#moreMDP\" data-toggle=\"collapse\"> More?</a><br>\n",
    "<div id=\"moreMDP\" class=\"collapse\">\n",
    "<ol>\n",
    "<li> Illustration of an MDP's components:\n",
    "<img src=\"img/transitions.png\">\n",
    "<li> And the resulting dynamics:\n",
    "<img src=\"img/mdp2.png\">\n",
    "<li> If $H\\rightarrow\\infty$ we have an infinite horizon planning problem.<br>\n",
    "<li> It is not obvious that the formulations $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, $r(s,a)$ and $r(s,a,s')$ are actually equivalent. We will admit that for now and will only use $r(s,a)$.\n",
    "<li> $S$ and $A$ may each be either:\n",
    "    <ul>\n",
    "    <li> arbitrary finite sets,\n",
    "    <li> arbitrary countable infinite sets,\n",
    "    <li> compact subsets of finite dimensional Euclidean space, or\n",
    "    <li> non-empty Borel subsets of complete, separable metric spaces.\n",
    "    </ul>\n",
    "<li> Markov transition model: $p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\\ldots) = p(s_{t+1}|s_t,a_t)$\n",
    "</ol>\n",
    "</div>\n",
    "Since we will only work with infinite horizon problems, we shall identify the MDP with the 4-tuple $\\langle S,A,p,r\\rangle$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Question:</b><br>What hypothesis has been introduced by comparison to the patient model in the introductory example?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answers0\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answers0\" class=\"collapse\">\n",
    "The probability $\\mathbb{P}(S_{t+1})$ is only conditioned by $S_t$ and $A_t$. It does not depend on the state and action history before $t$. In other words the dynamics of the model only depend on the current state and action: the transition model respects Markov's property.\n",
    "    \n",
    "If there were no actions, the random process of $S_t$ would be a *Markov process* (or Markov chain). Since transitions are controllable via the random variable $A_t$, this is called a Markov *decision* process.\n",
    "    \n",
    "Alternate name: Controlled Markov Chain.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"frozenlake\"></a>My first Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curing patients is a conceptually difficult task. We shall start with the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) game and work our way to more general concepts. It's also the occasion to familiarize with OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this problem's description (using for example `help(fl.FrozenLakeEnv)`). We read:\n",
    "\n",
    "`|  Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "|  when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "|  The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "|  If you step into one of those holes, you'll fall into the freezing water.\n",
    "|  At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "|  you navigate across the lake and retrieve the disc.\n",
    "|  However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "|  The surface is described using a grid like the following\n",
    "|  \n",
    "|      SFFF\n",
    "|      FHFH\n",
    "|      FFFH\n",
    "|      HFFG\n",
    "|  \n",
    "|  S : starting point, safe\n",
    "|  F : frozen surface, safe\n",
    "|  H : hole, fall to your doom\n",
    "|  G : goal, where the frisbee is located\n",
    "|  \n",
    "|  The episode ends when you reach the goal or fall in a hole.\n",
    "|  You receive a reward of 1 if you reach the goal, and zero otherwise.`\n",
    "\n",
    "So it's a game of navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Questions:</b><br>What are the possible states of an agent in this game?<br> What are its possible actions?<br>How would you describe the result of action $a$ in state $s$?<br> How would you formalize mathematically the goal of an agent?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answers1\" data-toggle=\"collapse\"><b>Answers:</b></a><br>\n",
    "<div id=\"answers1\" class=\"collapse\">\n",
    "States set: the 16 positions on the map.<br>\n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$<br>\n",
    "$s'$ resulting from $(s,a)$ follows a distribution $P(s'|s,a)$<br>\n",
    "Since each time step yields a reward, reaching the goal $\\Leftrightarrow$ $\\max \\sum r_t$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `print(help(gym.envs.toy_text.discrete))` one sees that all discrete state environments (such as FrozenLake) have the same inner attributes:\n",
    "\n",
    "`|  - nS: number of states\n",
    "|  - nA: number of actions\n",
    "|  - P: transitions (*)\n",
    "|  - isd: initial state distribution (**)\n",
    "|  \n",
    "|  (*) dictionary dict of dicts of lists, where\n",
    "|    P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "|  (**) list or array of length nS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "(0.3333333333333333, 1, 0.0, False)\n"
     ]
    }
   ],
   "source": [
    "print(env.unwrapped.P[1][0])\n",
    "print(env.unwrapped.P[1][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a few utilities for the FrozenLake environment (so that you don't spend time on useless things):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '←', 1: '↓', 2: '→', 3: '↑'}\n"
     ]
    }
   ],
   "source": [
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "print(actions)\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"gym\"></a>Some Gym notions\n",
    "\n",
    "Gym environments all are encapsulated so that they have the same API. This can be described by calling `help(gym.Env)` Namely, they all have 5 methods and 3 attributes.\n",
    "\n",
    "Methods:\n",
    "- `step` --- Runs one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling `reset()`  to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "- `reset` --- Resets the state of the environment and returns an initial observation.\n",
    "- `render` --- Renders the environment. The set of supported modes varies per environment. See `help(gym.Env.render)` for details and arguments.\n",
    "- `close` --- Cleans everything up, shuts the light and closes the door.\n",
    "- `seed` --- Sets the seed for this env's random number generator(s).\n",
    "\n",
    "Attributes:\n",
    "- `action_space` --- The Space(*) object corresponding to valid actions.\n",
    "- `observation_space` --- The Space(*) object corresponding to valid observations.\n",
    "- `reward_range` --- A tuple corresponding to the min and max possible rewards.\n",
    "\n",
    "(*) `Space` is a class (`help(gym.Space)`) that defines a set, be it continuous or discrete, with some useful methods. It is used specifically as the type of the action and observation spaces.\n",
    "\n",
    "Beyond those common API methods, some class-specific methods or attributes can be accessed by using the `unwrapped` attribute of each environment (if present) as in the example above.\n",
    "\n",
    "General references on OpenAI Gym:\n",
    "- [The short description paper](https://arxiv.org/pdf/1606.01540.pdf)\n",
    "- [The github page](https://github.com/openai/gym/blob/master/gym/envs/toy_text/discrete.py) (the most useful resource!)\n",
    "- [The general introduction to Gym](https://gym.openai.com/docs/)\n",
    "- [The deprecated list of environments](https://gym.openai.com/envs/) (refer to the github page for the latest version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"policies\"></a>Policies\n",
    "\n",
    "Now the question is: \"formally, how does one write the behaviour of an agent in the game?\".\n",
    "\n",
    "Using the notations introduced earlier, we want to decide on the distribution $\\mathbb{P}\\left(A_t\\right)$ at each time step.\n",
    "\n",
    "At time step $t$, action $a$ is picked with probability $\\delta_t(a|h)$, where $h$ is the history of state and actions seen between the initial time step and $t$.\n",
    "\n",
    "The collection of all $\\delta_t$ distributions defines a behaviour. Such a behaviour provides the agent with an action at each time step, given everything the agent has seen since the game started. We shall call such a behaviour a *policy*.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Policy $\\pi$</b><br>\n",
    "A policy $\\pi$ is a sequence of decision rules $\\delta_t$: $\\pi = \\{\\delta_t\\}_{t\\in T}$,<br>\n",
    "with $\\delta_t : \\left\\{\\begin{array}{ccc}\n",
    "S^{t+1}\\times A^{t} & \\rightarrow & \\mathcal{D}'(A)\\\\h & \\mapsto & \\delta_t(a|h)\n",
    "\\end{array}\\right.$\n",
    "</div>\n",
    "In other words:\n",
    "$\\delta_t(a|h)$ indicates<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the distribution over action $a$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; to undertake at time $t$, given<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the history of states/actions $h$.\n",
    "\n",
    "This is a *history-dependent, non-stationary, stochastic* policy.\n",
    "\n",
    "Ok, that's quite a convoluted definition but we said we wanted to be generic so we will keep that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "In the patient example, suppose the physician tells the patient to take drug A every day for 5 days, then drug B every two days for 9 days, then come back for a check-up. The physician adds to take drug C once a day if the patient feels pain over two consecutive days. Can you write the sequence of corresponding decision rules?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answers2\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answers2\" class=\"collapse\">\n",
    "This prescription is made over a finite horizon $H=14$ days.  \n",
    "    \n",
    "The prescription is deterministic: the distribution over actions is a Dirac. We will write it $a_t = \\delta_t(h)$.\n",
    "    \n",
    "The prescription depends on the two last states of the patient. Precisely, it depends on the boolean state variable \"is there pain?\". So we can write $\\delta_t(h) = \\delta_t(s_t,s_{t-1})$.  \n",
    "\n",
    "Consequently, the policy is:  \n",
    "For $t \\in [1, 5]$, $\\delta_t(s_t,s_{t-1}) = (A,C)$ if $pain(s_t,s_{t-1})=True$ and (A) otherwise.  \n",
    "For $t \\in [6, 14]$, $\\delta_t(s_t,s_{t-1}) = (B,C)$ if $pain(s_t,s_{t-1})=True$ and (B) otherwise.  \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"values\"></a>Value functions\n",
    "\n",
    "Now we want to find the best strategy for our game, ie. the best policy, so the question is \"how can one say that one policy is better than another?\". Consequently, we need to define criteria over policies.\n",
    "\n",
    "The baseline idea is to say that the criterion should reflect what the agent gains by applying the policy. Two problems arise:\n",
    "- this gain depends on the initial state\n",
    "- two applications of the policy can result in two different trajectories since the agent's environment is stochastic (it's an MDP, remember)\n",
    "\n",
    "Let us introduce three criteria as examples to give a general intuition of various possibilities. Bear in mind that many more (exotic) criteria are possible, they simply express what we consider an optimal behaviour. The three criteria below are actually functions, that map an **initial state** $s$ to the **expected** return the agent gets by applying the policy from $s$.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> Average reward </td>\n",
    "    <td width=\"300px\">$V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty}  \\frac{1}{H} \\sum\\limits_{t = 0}^H r_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Total reward </td>\n",
    "    <td>$V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H r_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Discounted reward </td>\n",
    "    <td>$V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H \\gamma^t r_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td width=\"150px\">with $0\\leq \\gamma<1$</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- The average reward criterion characterizes the average reward per time step the agent gets. This can be useful in some control applications. However, in the case of FrozenLake, we don't want to average our rewards, we want to get our frisbee as soon as possible.\n",
    "- The total reward criterion seems more adapted: it maximizes the cumulated rewards obtained during an episode. But it does not discriminate whether they were obtained at the beginning or late in the episode. Additionally, it suffers from a major flaw: for infinite horizon problems, even if the reward model is bounded, this sum might diverge. So we need a better formulation for the general case of infinite horizon problems.\n",
    "- The discounted reward criterion suits our needs. The gamma factor ($0\\leq \\gamma<1$) guarantees that with bounded reward models $r$, the sum always converges. Also it has the properties we desire: a reward of 1 obtained at the first time step weights 1 in the final criterion, while a reward of 1 obtained after $t$ time steps only weights $\\gamma^t$; it is *discounted* by $\\gamma^t$ (hence the criterion's name).\n",
    "\n",
    "From now on we shall concentrate on the discounted reward criterion. Given this criterion, we can introduce the notion of a value function.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "S & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H \\gamma^t r_t \\bigg| s_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Use the FrozenLake environment we've introduced earlier to obtain a Monte-Carlo estimate of $V^\\pi(s_0)$ over 100000 trials, with $s_0$ being the initial state and $\\pi$ being a simple policy that always goes right. Take $\\gamma = 0.9$.\n",
    "</div>\n",
    "Note that $\\gamma^{200} \\sim 10^{-9}$ so any reward obtained after 200 time steps will have a negligible contribution to $V^\\pi(s_0)$, thus rolling an episode out for 200 time steps should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value estimate: 0.01309986502660596\n",
      "value variance: 0.07565863789975193\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/RL1_exercice1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"optimal\"></a>Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state. We can now define what an optimal policy is.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-success\"><b>Optimal policy $\\pi^*$</b><br>\n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} V^\\pi$.<br>\n",
    "<br>\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get to our first fundamental result. Fortunately for us...<br>\n",
    "<br>\n",
    "<div class=\"alert alert-success\"><b>Optimal policy theorem</b><br>\n",
    "For $\\left\\{\\begin{array}{l}\n",
    "\\gamma\\textrm{-discounted criterion}\\\\\n",
    "\\textrm{infinite horizon}\n",
    "\\end{array}\\right.$, \n",
    "there always exists at least one optimal stationary, deterministic, Markovian policy.\n",
    "</div>\n",
    "\n",
    "Let's explain a little:\n",
    "- Markovian : $\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)\\in \\left(S\\times A\\right)^{t-1}\n",
    "\\end{array}\\right., \\delta_t\\left(a|s_0, a_0, \\ldots, s_t\\right) = \\delta_t\\left(a|s'_0, a'_0, \\ldots, s_t\\right)$.  \n",
    "One writes $\\delta_t(a|s)$.\n",
    "- Stationary : $\\forall (t,t')\\in \\mathbb{N}^2, \\delta_t = \\delta_t'$.  \n",
    "One writes $\\pi = \\delta_0$.\n",
    "- Deterministic : $\\delta_t(a|h) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal policies, at least one is a function $\\pi:S\\rightarrow A$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in that complex family of history-dependent, stochastic, non-stationary policies, can simply search for a function $\\pi(s)=a$ that maps states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#optPol\" data-toggle=\"collapse\"> Curious to know why?</a><br>\n",
    "<div id=\"optPol\" class=\"collapse\">\n",
    "    The proof (very simple but a little long) is in chapter 6 of <b>Markov Decision Processes</b> (book by Martin L. Puterman).<br>\n",
    "To give you the general flavour:\n",
    "<ul>\n",
    "<li> The infinite horizon leads to the existence of an optimal <b>stationary</b> policy: if the horizon is infinitely far, the optimal decision rule $n$ steps before the end if the same as the one $n+1$ steps before the end (watch out, this intuition can be very false in other contexts).\n",
    "<li> The <b>Markovian</b> property of $p(s'|s,a)$ allows to get optimal Markovian policies.\n",
    "<li> The <b>deterministic</b> part is somehow more tricky but just note that this result only holds for single-player MDPs. For a two-agents competitive game for example (like poker for instance), there is no deterministic optimal policy.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"eval\"></a>Evaluation equation\n",
    "\n",
    "Let's play a little bit (mathematically) with our new toys.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "What's the value of \"$a$ for the first step, then $\\pi$, starting from $s$\"?\n",
    "</div>\n",
    "\n",
    "Note: the answer uses the discounted criterion's definition and the value function of $\\pi$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersQsa\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersQsa\" class=\"collapse\">\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) & = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(s_t, a_t\\right) \\bigg| s_0 = s, a_0=a, \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\mathbb{E}\\left( \\sum\\limits_{t=1}^\\infty \\gamma^t r\\left(s_t, a_t\\right) \\bigg| s_0 = s, a_0=a, \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) \\mathbb{E}\\left( \\sum\\limits_{t=1}^\\infty \\gamma^{t-1} r\\left(s_t, a_t\\right) \\bigg| s_1 = s', \\pi \\right)\\\\\n",
    " & = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) V^\\pi\\left(s'\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have introduced the very important state-action value function $Q^\\pi$.\n",
    "<div class=\"alert alert-success\"><b>State-action value function</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(s_t, a_t\\right) \\bigg| s_0 = s, a_0=a, \\pi \\right)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Write a function that computes $Q^\\pi$ given $V^\\pi$ for FrozenLake.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercice2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting: given a policy $\\pi$, the best one-step lookahead action can be selected by maximizing $Q^\\pi$. To improve on a policy $\\pi$, it is more useful to know $Q^\\pi$ than $V^\\pi$ and pick the *greedy* action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Write a function that takes a $Q$ function for FrozenLake and returns the greedy policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercice3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remark that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. Let's replace that above and we get an important equation to characterize $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Evaluation equation</b><br>\n",
    "$V^\\pi$ is a solution to the linear system:\n",
    "\\begin{gather*}\n",
    "V^\\pi\\left(s\\right) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V^\\pi\\left(s'\\right)\\\\\n",
    "V^\\pi = r^\\pi + \\gamma P^\\pi V^\\pi = T^\\pi V^\\pi\n",
    "\\end{gather*}\n",
    "Similarly:\n",
    "\\begin{gather*}\n",
    "Q^\\pi\\left(s,a\\right) = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q^\\pi\\left(s', \\pi\\left(s'\\right)\\right)\\\\\n",
    "Q^\\pi = r + \\gamma P Q^\\pi = T^\\pi Q^\\pi\n",
    "\\end{gather*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#morePpi\" data-toggle=\"collapse\"> What are $P^\\pi$, $r^\\pi$ and $T^\\pi$ here?</a><br>\n",
    "<div id=\"morePpi\" class=\"collapse\">\n",
    "When the state space is discrete, $V^\\pi$ is vector of size $|S|$, $P^\\pi$ is a matrix containing the values $P^\\pi_{ij} = p\\left(s_j|s_i,\\pi(s_i)\\right)$ and, similarly, $r^\\pi$ is a vector containing the values $r^\\pi_i = r(s_i,\\pi(s_i))$. In better words, $P^\\pi$ is the <i>transition kernel</i> of the Markov chain describing the state dynamics under policy $\\pi$ and $r^\\pi$ is the associated reward model.<br>\n",
    "<br>\n",
    "This generalizes straightforwardly to the continuous states case: $V^\\pi$ is a function in the $\\mathcal{F}(S,\\mathbb{R})$ function space (the generalization of the vector in the previous sentence), $r^\\pi$ becomes the function $s\\mapsto r(s,\\pi(s))$ and  $P^\\pi$ becomes the operator over $\\mathcal{F}(S,\\mathbb{R})$ that maps function $V$ to function $s\\mapsto \\int\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)ds'$.<br>\n",
    "<br>\n",
    "In the same fashion, one can define the $T^\\pi$ operator. In the discrete state space case, $T^\\pi$ is the linear operator in $\\mathbb{R}^{|S|}$ that maps $V^\\pi$ to $r^\\pi + \\gamma P^\\pi V^\\pi$.<br>\n",
    "<br>\n",
    "In the continuous state space case, $T^\\pi$ is the linear operator in $\\mathcal{F}(S,\\mathbb{R})$ that maps $V^\\pi$ to the function $s\\mapsto r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V^\\pi\\left(s'\\right)$.\n",
    "</div>\n",
    "\n",
    "<a href=\"#moreEval\" data-toggle=\"collapse\"> A remark</a><br>\n",
    "<div id=\"moreEval\" class=\"collapse\">\n",
    "For stochastic policies:\n",
    "$\\forall s\\in S, \\quad V^\\pi(s) = \\sum\\limits_{a\\in A} \\pi(a|s) \\left(r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V^\\pi(s') \\right)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gone far from our original FrozenLake problem. Let's make all this very concrete:\n",
    "- A policy $\\pi$ is an agent's behaviour\n",
    "- In every state $s$, one can expect to gain $V^\\pi(s)$ in the long run by applying $\\pi$\n",
    "- The function $V^\\pi$ actually obeys the linear system of equations above that simply link the value of a state with the values of its successors in an episode.\n",
    "\n",
    "We can stop for a minute on the $T^\\pi$ evaluation operator (that maps a function $S\\rightarrow\\mathbb{R}$ to a function $S\\rightarrow\\mathbb{R}$) and the search for $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^\\pi$</b><br>\n",
    "<ol>\n",
    "<li> $T^\\pi$ is linear.<br>\n",
    "$\\Rightarrow$ Solving $V^\\pi = T^\\pi V^\\pi$ and $Q^\\pi = T^\\pi Q^\\pi$ by matrix inversion?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With $\\gamma<1$, $V^\\pi = \\left(I-\\gamma P^\\pi\\right)^{-1}r^\\pi$ and $Q^\\pi = \\left(I-\\gamma P\\right)^{-1}r^\\pi$\n",
    "<li> With $\\gamma<1$, $T^\\pi$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^\\pi$ (resp. $Q^\\pi$) is the unique solution to the (linear) fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^\\pi V$ (resp. $Q=T^\\pi Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Use the first property above to compute $V^\\pi$ for the policy that always moves right. To do this, you'll need to compute $r^\\pi$ and $P^\\pi$. Again, $\\gamma = 0.9$.\n",
    "Check if your result for $V^\\pi(s_0)$ is consistent with the Monte Carlo estimate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.308e-02  1.176e-02  2.744e-02  1.576e-16  1.875e-02  1.234e-16\n",
      "  6.402e-02  2.101e-16  4.944e-02  1.460e-01  1.860e-01 -1.179e-16\n",
      "  0.000e+00  3.008e-01  5.559e-01  0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/RL1_exercice4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Generalize the code above to a function that takes a policy as input. We'll suppose in this case that the policy is an array of actions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercice5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.308e-02  1.176e-02  2.744e-02  1.576e-16  1.875e-02  1.234e-16\n",
      "  6.402e-02  2.101e-16  4.944e-02  1.460e-01  1.860e-01 -1.179e-16\n",
      "  0.000e+00  3.008e-01  5.559e-01  0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "\n",
    "V_pi0 = policy_eval_lin(pi0)\n",
    "print(V_pi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Use the second property above to compute $V^\\pi$ for the policy that always moves right. To do this, you'll need to remember that since $T^\\pi$ is a contraction mapping, the sequence $V_{n+1}=T^\\pi V_n$ converges to $T^\\pi$'s fixed point (which happens to be $V^\\pi$ according to the property). Again, $\\gamma = 0.9$. For a start, apply $T^\\pi$ for a fixed number of steps `max_iter`.\n",
    "Check if your result is consistent with the previous estimate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013 0.012 0.027 0.    0.019 0.    0.064 0.    0.049 0.146 0.186 0.\n",
      " 0.    0.301 0.556 0.   ]\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/RL1_exercice6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between two iterations in the algorithm above, the distance between $V_{n+1}$ and $V_n$ decreases as $\\|V_{n+1}-V_n\\| = \\|r^\\pi + \\gamma P^\\pi V_n - V_n\\|$. Since $T^\\pi$ is a contraction mapping, we have $\\|V_{n+1}-V_n\\| \\leq \\|V_{n}-V_{n-1}\\|$. Let's call this distance at time step $n$ the **residual**. Then the successive residuals monotonically tend to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Now, can you use the property on the residuals to replace `max_iter` by a precision parameter `epsilon` that specifies the maximum error on $V^\\pi$? Advice: still keep `max_iter` to stop the computation in case you specify an `epsilon` that is too small. Return both $V^\\pi$ and the sequence of residuals. Plot the sequence of residuals and display the number of iterations necessary to reach the chose precision `epsilon`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013 0.012 0.027 0.    0.019 0.    0.064 0.    0.049 0.146 0.186 0.\n",
      " 0.    0.301 0.556 0.   ]\n",
      "number of iterations: 17\n",
      "last residual 7.988610483180246e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAf4UlEQVR4nO3deZRV5Znv8e9zzqnp1ABUUchQVBUoqDhrSaYOriRq0LbF7tYOJqbpG/sae8WMN51o0m3uMiuJnWR1zOqYG7lqxwyGa8zQ3FwMMYnRmDhQDqCAKCJDyWBBAYVAjee5f5xdxaEo5BQ17MPev89atc4e3n3qKYbfeevd797b3B0REYmuRNgFiIjI6FLQi4hEnIJeRCTiFPQiIhGnoBcRibhU2AUMNHHiRG9sbAy7DBGRE8ozzzyz091rB9tXcEHf2NhIc3Nz2GWIiJxQzGzT0fZp6EZEJOIU9CIiEaegFxGJOAW9iEjEKehFRCJOQS8iEnEKehGRiItM0O892M0dv32ZlVv2hF2KiEhBKbgLpo5XwuCO375CaVGSc6aPD7scEZGCEZkefWVpEdXlxWzadSDsUkRECkpkgh6gvjrN5rb9YZchIlJQIhX0DTVp9ehFRAaIVtBXp9m65yBdPZmwSxERKRjRCvqacjIOLbvVqxcR6ROxoE8DsKlNQS8i0idSQV8fBP1mjdOLiPSLVNDXVpSQLk7qhKyISI5IBb2ZaYqliMgAkQp6yI7Tb1SPXkSkXwSDvpzNbQfIZDzsUkRECkLkgr6+Ok1XT4Yd+zrCLkVEpCBELuj7p1hq+EZEBIhi0FeXA5piKSLSJ3JBP3V8KamEsUkzb0REgAgGfSqZoG5CmWbeiIgE8gp6M5tvZuvMbL2Z3TzI/hvN7AUze97MHjezOTn7bgmOW2dm7x/J4o+mvqZcQzciIoFjBr2ZJYE7gcuAOcC1uUEeuN/dz3L3c4GvA/8eHDsHWAicAcwHvhu836hqqE6zaZeGbkREIL8e/VxgvbtvcPcuYAmwILeBu7fnrJYDfZPYFwBL3L3T3V8D1gfvN6oaatK0d/Sw50DXaH8rEZGCl0/QTwO25Ky3BNsOY2YfM7NXyfboPzHEY28ws2Yza25tbc239qOqr9YUSxGRPvkEvQ2y7YjLTt39Tnc/Gfg88C9DPHaxuze5e1NtbW0eJb21xonZKZYbNXwjIpJX0LcA03PW64Ctb9F+CXDVcR47Ivp69DohKyKSX9CvAGaZ2QwzKyZ7cnVpbgMzm5Wz+pfAK8HyUmChmZWY2QxgFvD08Mt+a6VFSU6qKtEDSEREgNSxGrh7j5ndBCwHksC97r7azG4Dmt19KXCTmV0MdAO7gUXBsavN7AFgDdADfMzde0fpZzlMQ7WmWIqIQB5BD+Duy4BlA7bdmrP8ybc49ivAV463wONVX5Pmj68M/8SuiMiJLnJXxvZprEmzo72Tg11j8guEiEjBimzQ19cENzfTOL2IxFxkg76hfy69pliKSLxFN+iD+9KrRy8icRfZoB+fLqaqNKWrY0Uk9iIb9JB9fqzm0otI3EU86HUXSxGRyAf967sP0tObCbsUEZHQRDvoq8vpyThb93SEXYqISGgiHfT1wcwbPT9WROIs0kHfN8VSM29EJM4iHfQnVZZSkkrohKyIxFqkgz6RMOqr0+rRi0isRTroITt8o6tjRSTOIh/09dXlbG47gPsRTzAUEYmFyAd9Q02aA129tL7ZGXYpIiKhiHzQ12vmjYjEXOSDvjG4L72CXkTiKvJBP218GQmDzZpiKSIxFfmgL04lmDq+THexFJHYinzQQ99dLBX0IhJPeQW9mc03s3Vmtt7Mbh5k/2fMbI2ZrTKz35lZQ86+XjN7PvhaOpLF56tviqWISByljtXAzJLAncAlQAuwwsyWuvuanGbPAU3ufsDM/gn4OvCBYN9Bdz93hOseksaaNG37u2jv6KaqtCjMUkRExlw+Pfq5wHp33+DuXcASYEFuA3d/xN37usxPAnUjW+bw9D8/VsM3IhJD+QT9NGBLznpLsO1orgceylkvNbNmM3vSzK4a7AAzuyFo09za2ppHSUNTX60pliISX8ccugFskG2D3k/AzK4DmoCLcjbXu/tWM5sJ/N7MXnD3Vw97M/fFwGKApqamEb9Xge5LLyJxlk+PvgWYnrNeB2wd2MjMLga+CFzp7v33G3D3rcHrBuAPwHnDqPe4VJSkmFhRrKEbEYmlfIJ+BTDLzGaYWTGwEDhs9oyZnQfcRTbk38jZPsHMSoLlicC7gNyTuGOmoaacjbpoSkRi6JhB7+49wE3AcmAt8IC7rzaz28zsyqDZN4AK4KcDplGeDjSb2UrgEeD2AbN1xkxDdVo9ehGJpXzG6HH3ZcCyAdtuzVm++CjH/Rk4azgFjpT6mjS/eP51Ont6KUklwy5HRGTMxOLKWMhOsXSHLW0Hwy5FRGRMxSbo+6ZYbtbMGxGJmdgEfd9FUxt3apxeROIlNkFfU15MRUlK97wRkdiJTdCbGfXVaTZpiqWIxExsgh6C2xWrRy8iMROroK+vSdPSdpDezIjfZUFEpGDFKugbqsvp6s2wvb0j7FJERMZMrIK+se/mZjs1Ti8i8RGroD90F0uN04tIfMQq6KeMK6MoabovvYjESqyCPpkwpk9I6+pYEYmVWAU9ZIdv1KMXkTiJXdA31pSzadcB3DXFUkTiIXZBX1+d5s3OHtr2d4VdiojImIhd0Ddo5o2IxExsg15PmxKRuIhd0NdNSGOGTsiKSGzELuhLi5JMrirVXSxFJDZiF/Sgu1iKSLzkFfRmNt/M1pnZejO7eZD9nzGzNWa2ysx+Z2YNOfsWmdkrwdeikSz+eDVUl2voRkRi45hBb2ZJ4E7gMmAOcK2ZzRnQ7Dmgyd3PBh4Evh4cWw18CXgbMBf4kplNGLnyj099TZqdb3ayv7Mn7FJEREZdPj36ucB6d9/g7l3AEmBBbgN3f8Td+7rITwJ1wfL7gYfdvc3ddwMPA/NHpvTj1z/zRsM3IhID+QT9NGBLznpLsO1orgceGsqxZnaDmTWbWXNra2seJQ1PQ3U5oJk3IhIP+QS9DbJt0PsHmNl1QBPwjaEc6+6L3b3J3Ztqa2vzKGl4+m9XrJk3IhID+QR9CzA9Z70O2DqwkZldDHwRuNLdO4dy7FgbV1bEhHSRZt6ISCzkE/QrgFlmNsPMioGFwNLcBmZ2HnAX2ZB/I2fXcuBSM5sQnIS9NNgWuvqacl0dKyKxkDpWA3fvMbObyAZ0ErjX3Veb2W1As7svJTtUUwH81MwANrv7le7eZmZfJvthAXCbu7eNyk8yRA3VaZ7bsjvsMkRERt0xgx7A3ZcBywZsuzVn+eK3OPZe4N7jLXC0NNSk+dWqrXT1ZChOxfK6MRGJidgmXH11mozD63sOhl2KiMioim3QN07sm2KpmTciEm2xDfqGal00JSLxENugr60soawoqYumRCTyYhv0ZkZ9tR4ULiLRF9ugh+wVshqjF5Goi3XQN9ak2dx2gExm0Ds6iIhEQqyDvr6mnM6eDG/s6zx2YxGRE1Ssg75v5o2Gb0QkyuId9H13sdQUSxGJsFgH/dTxZSQTppubiUikxTroi5IJ6iaUsVFDNyISYbEOesje80ZXx4pIlMU+6BtqdNGUiESbgr66nL0Hu9l7oDvsUkRERkXsg77/+bFtGqcXkWiKfdD3TbHcqOEbEYmo2Ad9fd/tijXzRkQiKvZBny5OMamyRCdkRSSyYh/0EMy80RRLEYkoBT1QX12uq2NFJLLyCnozm29m68xsvZndPMj+eWb2rJn1mNnVA/b1mtnzwdfSkSp8JDXUpNne3kFHd2/YpYiIjLjUsRqYWRK4E7gEaAFWmNlSd1+T02wz8A/AZwd5i4Pufu4I1Dpq+mbebG47wOyTKkOuRkRkZOXTo58LrHf3De7eBSwBFuQ2cPeN7r4KyIxCjaOuoaYcQCdkRSSS8gn6acCWnPWWYFu+Ss2s2cyeNLOrBmtgZjcEbZpbW1uH8NYjQ/elF5EoyyfobZBtQ3n2Xr27NwEfBO4ws5OPeDP3xe7e5O5NtbW1Q3jrkTE+XURlaUo3NxORSMon6FuA6TnrdcDWfL+Bu28NXjcAfwDOG0J9Y8LMdHMzEYmsfIJ+BTDLzGaYWTGwEMhr9oyZTTCzkmB5IvAuYM1bHxWOhupyDd2ISCQdM+jdvQe4CVgOrAUecPfVZnabmV0JYGYXmlkLcA1wl5mtDg4/HWg2s5XAI8DtA2brFIyGmjQtuw/S03tCnk8WETmqY06vBHD3ZcCyAdtuzVleQXZIZ+BxfwbOGmaNY6KhJk1Pxtm2t4PpwclZEZEo0JWxgfpqTbEUkWhS0AcadF96EYkoBX1gclUpxamEevQiEjkK+kAiYdRXpzXzRkQiR0Gfo6Fac+lFJHoU9Dnqa9JsbjuA+1Au/BURKWwK+hwN1WkOdPWy882usEsRERkxCvoch+5iqXF6EYkOBX2O/imWGqcXkQhR0Oeom5AmYej5sSISKQr6HMWpBFPGlbFZQzciEiEK+gEaatLq0YtIpCjoB2ioSbNZY/QiEiEK+gHqq8vZtb+LfR3dYZciIjIiFPQDNGrmjYhEjIJ+gPog6PX8WBGJCgX9AIcumlLQi0g0KOgHqChJUVNezGbdl15EIkJBP4j6mjQbd6pHLyLRoKAfRGNNucboRSQy8gp6M5tvZuvMbL2Z3TzI/nlm9qyZ9ZjZ1QP2LTKzV4KvRSNV+Giqr06zde9BOnt6wy5FRGTYjhn0ZpYE7gQuA+YA15rZnAHNNgP/ANw/4Nhq4EvA24C5wJfMbMLwyx5dDTVp3KFl98GwSxERGbZ8evRzgfXuvsHdu4AlwILcBu6+0d1XAZkBx74feNjd29x9N/AwMH8E6h5VfXex1BWyIhIF+QT9NGBLznpLsC0fwzk2NI3BFMs/rHsj5EpERIYvn6C3Qbbl+6y9vI41sxvMrNnMmltbW/N869FTU1HCdW+v574nNvF/V24NuxwRkWHJJ+hbgOk563VAvumX17Huvtjdm9y9qba2Ns+3Hl23XnEGTQ0T+NyDq1iztT3sckREjls+Qb8CmGVmM8ysGFgILM3z/ZcDl5rZhOAk7KXBtoJXnErw3evOp6osxUd/1Mzu/XqOrIicmI4Z9O7eA9xENqDXAg+4+2ozu83MrgQwswvNrAW4BrjLzFYHx7YBXyb7YbECuC3YdkKYVFnK9667gB17O/n4T56jp3fguWYRkcJn7vkOt4+NpqYmb25uDruMwzywYguf+9kqbpg3ky9cfnrY5YiIHMHMnnH3psH2pca6mBPR3104nRde38vixzZwxtQqFpxb8BOHRET66RYIefrXK+ZwYeMEPv+zVazeujfsckRE8qagz1NxKsF3P3QB48uKueEHz9Cmk7MicoJQ0A9BbWUJd334Alrf7OSm+5/VyVkROSEo6IfonOnj+cpVZ/LnV3dx+0MvhV2OiMgx6WTscbimaTovvr6Xux9/jTOnjeOq83RyVkQKl3r0x+lfrpjD3BnVfP5nq3jxdZ2cFZHCpaA/TkXJBN/90PnUlBfz0R8+w643O8MuSURkUAr6YZhYUcL3+k/O6spZESlMCvphOrtuPF/767N4YsMuvrpMJ2dFpPDoZOwI+NsL6nhx617u/dNrnDmtir85vy7skkRE+qlHP0K+cPnpvH1mNbf8/AVeaNHJWREpHAr6EVKUTHDnB89nYkUJH/1hMzt1clZECoSCfgTVVGSvnN21v4uP/fhZunVyVkQKgIJ+hJ05bRy3/+1ZPPVaG1/5f2vDLkdERCdjR8Nfn1fHi6+3c8/jr3HG1CquaZp+7INEREaJevSj5JbLTuOdJ9fwxV++yMote8IuR0RiTEE/SlLJBN/54PnUVpRw44+e4c+v7mTvwe6wyxKRGNLQzSiqLi/mrg9fwN/d9QQf/N9PAVA3oYw5U6qYM7Wq/3Xa+DLMLORqRSSqFPSj7Mxp43j88+9lZcse1mxtZ822dtZua+fhtTvoe1xvVWkqCP5xnD6lkjlTq5g1qZLilH7hEpHhU9CPgeryYt5z6iTec+qk/m0Hunp4afu+/vBfs7Wd+5/eREd3dkpmUdI4ZVLl4b3/KVWMSxeF9WOIyAkqr6A3s/nAt4EkcLe73z5gfwnwA+ACYBfwAXffaGaNwFpgXdD0SXe/cWRKP7Gli1OcXz+B8+sn9G/rzTgbd+0/LPwfe6WVnz3b0t9m2vgy/v4dDfzju2eSTGi4R0SO7ZhBb2ZJ4E7gEqAFWGFmS919TU6z64Hd7n6KmS0E/g34QLDvVXc/d4TrjqRkwji5toKTayv4q3Om9m9/Y18Ha7ftY+22dv60fidfe+gllq/ezjevOYeZtRUhViwiJ4J8BoHnAuvdfYO7dwFLgAUD2iwA7guWHwTeZzq7OGImVZZy0exabrzoZH7wkbl8e+G5vNq6n8u+/Ufuefw1MhkPu0QRKWD5BP00YEvOekuwbdA27t4D7AVqgn0zzOw5M3vUzN492DcwsxvMrNnMmltbW4f0A8SNmbHg3Gk8/Ol5/MUpE/nyr9awcPGTbNq1P+zSRKRA5RP0g/XMB3Yhj9ZmG1Dv7ucBnwHuN7OqIxq6L3b3Jndvqq2tzaMkmVRVyt2LmvjmNeewdns78+/4Iz98YqN69yJyhHyCvgXIvYa/Dth6tDZmlgLGAW3u3unuuwDc/RngVWD2cIuWLDPj6gvq+M2n53HhjGr+9b9Wc909T7Gl7UDYpYlIAckn6FcAs8xshpkVAwuBpQPaLAUWBctXA793dzez2uBkLmY2E5gFbBiZ0qXPlHFl3PffLuRrf3MWK7fsYf4dj/GTpzfjrt69iOQR9MGY+03AcrJTJR9w99VmdpuZXRk0uweoMbP1ZIdobg62zwNWmdlKsidpb3T3tpH+ISTbu792bj3LPz2Pc6aP55afv8Df3/s0W/ccDLs0EQmZFVqvr6mpyZubm8Mu44SWyTg/fmoTX132EqmEcetfzeHqC+p0mwWRCDOzZ9y9abB9usY+ghIJ48PvaGT5p+Zx+tQq/vnBVfzjfc3saO8IuzQRCYGCPsLqa9Is+e9v59Yr5vCnV3dy6bce45fPva6xe5GYUdBHXCJhfOQvZrDsE+/m5NpyPvV/nufGHz1D6z4901YkLhT0MTGztoKf3vhOvnD5aTyyrpVLv/Uov1o1cJasiESRTsbG0Cs79vHZn65kZctezphaxeVnTeHys6YwY2J52KWJyHF6q5OxCvqY6unNcP/Tm/nFc6/z3Obsow5Pm1wZhP5kTplUGXKFIjIUCnp5S1v3HOTXL27noRe30bxpN+4wa1IFlwWhf+pJlZqaKVLgFPSStx3tHSxfvZ1lL2zj6dfayDjMnFjOZWdN5rIzp3DG1CqFvkgBUtDLcWnd18ny1dme/pMb2ujNOPXVaS47azKXnzmFs+vGKfRFCoSCXoZt15udPLxmB8te3M6f1++kJ+NMG1/GZWdO5vKzp3Bu3XgSeuKVSGgU9DKi9hzo4uE1O3joxe388ZVWunudKeNKee9pk7hodi3vPGUiFSV6HLHIWFLQy6jZe7Cb37+0g2UvbOdP63dyoKuXVMJoapzARbMnMW/2ROZM0bi+yGhT0MuY6OrJ0Lypjcde3smjL7eydls7ALWVJcybVctFp9by7lMmMqG8OORKRaJHQS+h2NHewWMvt/Loy608vn4new50YwZn143notm1XDS7lnPqxpFK6gJtkeFS0EvoejPOqpY9PBoE/8ote8g4jCsr4i9OmchFs2uZN7uWyeNKwy5V5ISkoJeCs+dAF4+v38mj67LB/0Zwk7XTJlcyb3Yt59SNZ2ZtOTMmllNalAy5WpHC91ZBr6kREorx6WKuOHsqV5w9FXdn3Y59/aH/n396je7ebAfEDKaOK2NmbTkzJ5Yzs7Yiu1xbwZSqUk3pFMmDevRScA529bJh55tsaN2f/epffpP9Xb397UqLEjTWlHNyf/iXM3NiBTNqy6kqLQrxJxAZe+rRywmlrDjJGVPHccbUcYdtd3da93Xyak74v7ZzP6u37uXXq7fTmznUaZlYUcLM2nJOri2nbkKaSZUlnFRVyuRxpZxUWUpVWUpTPiU2FPRywjAzJlWVMqmqlHecXHPYvq6eDJvb9vNqEP4bWrMfBMtX76Btf9cR71ValOCkqtL+r8lV2Q+CSVWlTK4q5aRgXecHJAoU9BIJxakEp0yqHPT2yh3dvbzR3sn29g525Hxtb+9kR3sHq1r28Ju9HXT2ZI44dlxZEZOrSpkUBH91eTGVJSkqS1NUlhZRUZpdriot6t9WWZqiSFNGpYDkFfRmNh/4NpAE7nb32wfsLwF+AFwA7AI+4O4bg323ANcDvcAn3H35iFUvkofSoiT1NWnqa9JHbePutB/sYce+4ENgbwdv7Otk+95DHwwv79jHngPdg34gHPk9E/2hX1lalPPhcOjDoLw4RWlRgpKiJGVFSUqLkpQWJQ5bLkklKSsO1lMJXXMgx+WYQW9mSeBO4BKgBVhhZkvdfU1Os+uB3e5+ipktBP4N+ICZzQEWAmcAU4Hfmtlsd+9FpICYGePSRYxLFzH7pLd+6EpXT4Y3O3vY19HNvo4e2oPXfR09vNm3HOxvD7bv6+hmR3tH/3LuSeWhKEoapalk9sOhOEFpKvshUJxKUJQ0ipIJUonsa1EyQSrZt9y3L2c5Z18qkaAolaAoYSQHfpmRSBipRPY1aYfvTwTrqZzlZAKSiQRJM8yys6cSZsFX9s87kbPNEvTvSwTHDGwvxy+fHv1cYL27bwAwsyXAAiA36BcA/zNYfhD4jmX/ZhYAS9y9E3jNzNYH7/fEyJQvMvaKUwmqU8VUD+NWDr0Z52B3Lx3dvRzs6qWzp5eO7kz/tsOXB9uW6d9+sLuX7t4M3T3Omz099PR6dr03Q0/G6e7J0J1xenozdAf7ejJ+2MnrE4EZGNnQt/717Ma+9UT/vuwruccEy30fJMG79i/3vUewdcD2Qx80/dv7vv9hx+XWm3PMYT/IkYt9bU+fUsV/XHte/n8oecon6KcBW3LWW4C3Ha2Nu/eY2V6gJtj+5IBjpw38BmZ2A3ADQH19fb61i5ywkgmjoiQV6l0+MxmnO5MN/9wPgd7gQ6DX/dByxsm405NxMoPsz7jTm4HeTCb76k5vJkMmAxl33MFxMp5dz3h2uCyTObTNc/Zl13PaZxyH/vfJvh5ap3+9770PtSPY3tc+E2zsWw9a5LQN3rN/+cjtHLb90PuR0/bQO+du9yO35zSYPqEs77+/ocjnX9lgvzMN7AocrU0+x+Lui4HFkJ1Hn0dNIjJMiYRRkkiiO0pHXz5ndlqA6TnrdcDWo7UxsxQwDmjL81gRERlF+QT9CmCWmc0ws2KyJ1eXDmizFFgULF8N/N6zv6MsBRaaWYmZzQBmAU+PTOkiIpKPY/7SFoy53wQsJzu98l53X21mtwHN7r4UuAf4YXCytY3shwFBuwfInrjtAT6mGTciImNL97oREYmAt7rXja6+EBGJOAW9iEjEKehFRCJOQS8iEnEFdzLWzFqBTcN4i4nAzhEqZySprqFRXUOjuoYminU1uHvtYDsKLuiHy8yaj3bmOUyqa2hU19CorqGJW10auhERiTgFvYhIxEUx6BeHXcBRqK6hUV1Do7qGJlZ1RW6MXkREDhfFHr2IiORQ0IuIRFxkgt7M5pvZOjNbb2Y3h10PgJlNN7NHzGytma02s0+GXVMuM0ua2XNm9quwa+ljZuPN7EEzeyn4c3tH2DUBmNmng7/DF83sJ2ZWGmIt95rZG2b2Ys62ajN72MxeCV4nFEhd3wj+LleZ2S/MbHwh1JWz77Nm5mY2sVDqMrOPB1m22sy+PhLfKxJBn/MA88uAOcC1wYPJw9YD/A93Px14O/CxAqmrzyeBtWEXMcC3gV+7+2nAORRAfWY2DfgE0OTuZ5K9XffCEEv6PjB/wLabgd+5+yzgd8H6WPs+R9b1MHCmu58NvAzcMtZFMXhdmNl04BJg81gXFPg+A+oys/eQfdb22e5+BvDNkfhGkQh6ch5g7u5dQN8DzEPl7tvc/dlgeR/Z0DrimblhMLM64C+Bu8OupY+ZVQHzyD7fAHfvcvc94VbVLwWUBU9QSxPik9Lc/TGyz33ItQC4L1i+D7hqTIti8Lrc/Tfu3hOsPkn2KXOh1xX4FvA5Bnm86Vg4Sl3/BNzu7p1BmzdG4ntFJegHe4B5QQRqHzNrBM4Dngq3kn53kP1Hngm7kBwzgVbgP4MhpbvNrDzsotz9dbI9q83ANmCvu/8m3KqOcJK7b4NsBwOYFHI9g/kI8FDYRQCY2ZXA6+6+MuxaBpgNvNvMnjKzR83swpF406gEfV4PIQ+LmVUAPwM+5e7tBVDPFcAb7v5M2LUMkALOB/6Xu58H7CecIYjDBOPdC4AZwFSg3MyuC7eqE4uZfZHsUOaPC6CWNPBF4NawaxlECphAdqj3n4EHzGywfBuSqAR9wT6E3MyKyIb8j93952HXE3gXcKWZbSQ7zPVeM/tRuCUB2b/HFnfv+63nQbLBH7aLgdfcvdXdu4GfA+8MuaaBdpjZFIDgdUR+5R8JZrYIuAL4kBfGhTsnk/3QXhn8H6gDnjWzyaFWldUC/Nyznib7G/ewTxRHJejzeYD5mAs+ie8B1rr7v4ddTx93v8Xd69y9keyf1e/dPfQeqrtvB7aY2anBpveRfd5w2DYDbzezdPB3+j4K4CTxAEuBRcHyIuC/Qqyln5nNBz4PXOnuB8KuB8DdX3D3Se7eGPwfaAHOD/79he2XwHsBzGw2UMwI3GUzEkEfnOzpe4D5WuABd18dblVAtuf8YbI95ueDr8vDLqrAfRz4sZmtAs4FvhpyPQS/YTwIPAu8QPb/TWiX0JvZT4AngFPNrMXMrgduBy4xs1fIziS5vUDq+g5QCTwc/Pv/XoHUFbqj1HUvMDOYcrkEWDQSvwXpFggiIhEXiR69iIgcnYJeRCTiFPQiIhGnoBcRiTgFvYhIxCnoRUQiTkEvIhJx/x87zbW0aSMUQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/RL1_exercice7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"optimality\"></a>Optimality equation\n",
    "\n",
    "Ok, so given a policy, we have a way (two actually) to compute its value function. Now we would like to find the optimal policy and we'd rather not enumerate all possible policies.\n",
    "\n",
    "Let's introduce some notations and start with two trivial remarks. We write:\n",
    "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
    "\n",
    "Then we can note that:\n",
    "1. $Q^*\\left(s,a\\right) = r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S}p\\left(s'|s,a\\right) V^*\\left(s'\\right)$\n",
    "2. If $\\pi^*$ is an optimal policy, then $V^*\\left(s\\right) = Q^*\\left(s,\\pi^*\\left(s\\right)\\right)$\n",
    "\n",
    "From the first remark, we note that the maximum expected return one can get from a trajectory initiated in $s$ starts with the action $a$ that maximizes $Q^*(s,a)$. So:\n",
    "<div class=\"alert alert-success\"><b>Optimal greedy policy</b><br>\n",
    "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, finding $\\pi^*$ is equivalent to finding $Q^*$.\n",
    "\n",
    "Here comes the key theorem of this class:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality equation</b><br>\n",
    "The optimal value function obeys:\n",
    "$$V^*(s) = \\max\\limits_{a\\in A} \\left\\{ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right\\}=(T^* V^*) (s)$$\n",
    "or in terms of $Q$-functions:\n",
    "$$Q^*(s,a) = r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')=(T^* Q^*) (s,a)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does that unfold intuitively from the previous remarks? Simply because of the Dynamic Programming principle: any subpath of an optimal path is itself optimal. That might still sound obscure. So let's say that if you start a trajectory in $s$ and if you know you will get the maximum value from any state $s'$ you reach, then the maximum value you can get from $s$ is precisely $\\max\\limits_{a\\in A} \\left\\{ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right\\}$. The actual proof is quite trickier but the key ideas are here.<br>\n",
    "<br>\n",
    "As for the policy evaluation operator $T^\\pi$, we define the **Bellman optimality operator** $T^*$ as above.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-success\"><b>Properties of $T^*$</b><br>\n",
    "<ol>\n",
    "<li> $T^*$ is non-linear.<br>\n",
    "<li> With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Use the property above to compute $V^*$. To do this, you'll need to remember that since $T^*$ is a contraction mapping, the sequence $V_{n+1}=T^* V_n$ converges to $T^*$'s fixed point (which happens to be $V^*$ according to the property). Again, $\\gamma = 0.9$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercice8.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"summary\"></a>Summary\n",
    "\n",
    "Let's wrap this whole section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$, $Q^\\pi(s,a)$.\n",
    "- Evaluation eq. $\\leftrightarrow$ $V^\\pi = T^\\pi V^\\pi$, $Q^\\pi = T^\\pi Q^\\pi$, linear equation, contraction mapping.\n",
    "- Bellman optimality eq. $\\leftrightarrow$ $V^* = T^* V^*$, $Q^* = T^* Q^*$, non-linear equation, contraction mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"before\"></a>Before you leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"limits\"></a>Limitations\n",
    "\n",
    "What if the system is an MDP but its state is not fully observable?  \n",
    "$\\rightarrow$ This is the (exciting) field of Partially Observable MDPs. Our key result of having a Markovian optimal policy does not hold anymore. There are ways to still obtain optimal policies (but it is often very computationaly costly) or approximate them with Markovian policies.\n",
    "\n",
    "What happens if there are multiple actions taken at the same time by different agents?  \n",
    "$\\rightarrow$ This falls into the category of multi-player stochastic games. Such games can be adversarial, cooperative, or a mix of the two. Of course they can also have partial observability.\n",
    "\n",
    "What if the transition model is not Markovian?  \n",
    "$\\rightarrow$ Beware, here be dragons! All the beautiful framework above crumbles down if its hypothesis are violated. So great care should be taken when choosing the state variables for a given problem. In a sense, an MDP is a discrete time version of a first-order differential equation. Writing a system as $\\dot{X} = f(X,U, noise)$ as is common in Control Theory is a good practice to ensure the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"misconception\"></a>Common misconception\n",
    "\n",
    "You will often see the following type of drawing, along with the sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<img src=\"img/misconception.png\" style=\"height: 300px;\"></img>\n",
    "\n",
    "Although this sentence is not false *per se*, it conveys an important misconception that may be grounded in too simple anthropomorphic analogies. One often talks about the *state of the agent* or the *state of the environment*. The distinction here is confusing at best: there is no separation between agent and environment. A better vocabulary is to talk about a *system to control*, that is described through its observed *state*. This system is controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with.\n",
    "\n",
    "Although less shiny, the drawing below may be less misleading.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 300px;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
